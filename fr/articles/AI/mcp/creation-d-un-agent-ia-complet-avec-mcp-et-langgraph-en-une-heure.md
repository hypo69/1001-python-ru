# Comment apprendre √† un r√©seau neuronal √† travailler avec ses mains : cr√©er un agent IA √† part enti√®re avec MCP et LangGraph en une heure

Amis, salutations ! J'esp√®re que je vous ai manqu√©.

Au cours des deux derniers mois, je me suis plong√© profond√©ment dans la recherche sur l'int√©gration des agents IA dans mes propres projets Python. Au cours de ce processus, j'ai accumul√© de nombreuses connaissances et observations pratiques qu'il serait dommage de ne pas partager. Je reviens donc aujourd'hui sur Habr ‚Äî avec un nouveau sujet, une nouvelle perspective et l'intention d'√©crire plus souvent.

√Ä l'ordre du jour, LangGraph et MCP : des outils avec lesquels vous pouvez cr√©er des agents IA vraiment utiles.

Si auparavant nous nous disputions pour savoir quel r√©seau neuronal r√©pondait le mieux en russe, aujourd'hui le champ de bataille s'est d√©plac√© vers des t√¢ches plus appliqu√©es : qui g√®re le mieux le r√¥le d'un agent IA ? Quels frameworks simplifient r√©ellement le d√©veloppement ? Et comment int√©grer tout cela dans un projet r√©el ?

Mais avant de plonger dans la pratique et le code, clarifions les concepts de base. Surtout deux points cl√©s : les **agents IA et le MCP**. Sans eux, la conversation sur LangGraph serait incompl√®te.

### Agents IA en termes simples

Les agents IA ne sont pas de simples chatbots ¬´am√©lior√©s¬ª. Ils repr√©sentent des entit√©s plus complexes et autonomes qui poss√®dent deux caract√©ristiques cruciales :

1.  **Capacit√© d'interagir et de se coordonner**

    Les agents modernes peuvent d√©composer les t√¢ches en sous-t√¢ches, appeler d'autres agents, demander des donn√©es externes, travailler en √©quipe. Ce n'est plus un assistant solitaire, mais un syst√®me distribu√© o√π chaque composant peut apporter sa contribution.

2.  **Acc√®s aux ressources externes**

    Un agent IA n'est plus limit√© par les fronti√®res d'un dialogue. Il peut acc√©der √† des bases de donn√©es, effectuer des appels API, interagir avec des fichiers locaux, des bases de connaissances vectorielles et m√™me ex√©cuter des commandes dans le terminal. Tout cela est devenu possible gr√¢ce √† l'√©mergence du MCP ‚Äî un nouveau niveau d'int√©gration entre le mod√®le et l'environnement.

---

En termes simples : **le MCP est un pont entre un r√©seau neuronal et son environnement**. Il permet au mod√®le de ¬´comprendre¬ª le contexte de la t√¢che, d'acc√©der aux donn√©es, d'effectuer des appels et de former des actions raisonn√©es, plut√¥t que de simplement produire des r√©ponses textuelles.

**Imaginons une analogie :**

*   Vous avez un **r√©seau neuronal** ‚Äî il peut raisonner et g√©n√©rer des textes.
*   Il y a des **donn√©es et des outils** ‚Äî documents, API, bases de connaissances, terminal, code.
*   Et il y a le **MCP** ‚Äî c'est une interface qui permet au mod√®le d'interagir avec ces sources externes comme si elles faisaient partie de son monde interne.

**Sans MCP :**

Le mod√®le ‚Äî est un moteur de dialogue isol√©. Vous lui donnez du texte ‚Äî il r√©pond. Et c'est tout.

**Avec MCP :**

Le mod√®le devient un **ex√©cuteur de t√¢ches** √† part enti√®re :

*   acc√®de aux structures de donn√©es et aux API ;
*   appelle des fonctions externes ;
*   navigue dans l'√©tat actuel du projet ou de l'application ;
*   peut m√©moriser, suivre et modifier le contexte au fur et √† mesure du dialogue ;
*   utilise des extensions telles que des outils de recherche, des ex√©cuteurs de code, des bases de donn√©es d'embeddings vectoriels, etc.

Au sens technique, **le MCP est un protocole d'interaction entre un LLM et son environnement**, o√π le contexte est fourni sous forme d'objets structur√©s (au lieu de texte ¬´brut¬ª), et les appels sont format√©s comme des op√©rations interactives (par exemple, appel de fonction, utilisation d'outil ou actions d'agent). C'est ce qui transforme un mod√®le ordinaire en un **v√©ritable agent IA**, capable de faire plus que simplement ¬´parler¬ª.

### Et maintenant ‚Äî au travail !

Maintenant que nous avons couvert les concepts de base, il est logique de se poser la question : ¬´Comment impl√©menter tout cela en pratique en Python ?¬ª

C'est l√† qu'intervient **LangGraph** ‚Äî un framework puissant pour construire des graphes d'√©tat, des comportements d'agent et des cha√Ænes de pens√©e. Il vous permet de ¬´coudre¬ª la logique d'interaction entre les agents, les outils et l'utilisateur, cr√©ant une architecture IA vivante qui s'adapte aux t√¢ches.

Dans les sections suivantes, nous verrons comment :

*   construire un agent √† partir de z√©ro ;
*   cr√©er des √©tats, des transitions et des √©v√©nements ;
*   int√©grer des fonctions et des outils ;
*   et comment tout cet √©cosyst√®me fonctionne dans un projet r√©el.

### Un peu de th√©orie : qu'est-ce que LangGraph

Avant de plonger dans la pratique, quelques mots sur le framework lui-m√™me.

**LangGraph** est un projet de l'√©quipe **LangChain**, ceux-l√† m√™mes qui ont les premiers propos√© le concept de ¬´cha√Ænes¬ª d'interaction avec les LLM. Si auparavant l'accent principal √©tait mis sur les pipelines lin√©aires ou √† embranchements conditionnels (langchain.chains), les d√©veloppeurs misent d√©sormais sur un **mod√®le de graphe**, et LangGraph est ce qu'ils recommandent comme le nouveau ¬´noyau¬ª pour construire des syst√®mes IA complexes.

**LangGraph** est un framework pour construire des machines √† √©tats finis et des graphes d'√©tat, o√π chaque **n≈ìud** repr√©sente une partie de la logique de l'agent : un appel de mod√®le, un outil externe, une condition, une entr√©e utilisateur, etc.

### Comment √ßa marche : graphes et n≈ìuds

Conceptuellement, LangGraph est construit sur les id√©es suivantes :

*   **Graphe** ‚Äî est une structure qui d√©crit les chemins d'ex√©cution possibles de la logique. On peut le consid√©rer comme une carte : d'un point, on peut passer √† un autre en fonction des conditions ou du r√©sultat de l'ex√©cution.
*   **N≈ìuds** ‚Äî sont des √©tapes sp√©cifiques au sein du graphe. Chaque n≈ìud ex√©cute une fonction : appelle un mod√®le, appelle une API externe, v√©rifie une condition ou met simplement √† jour l'√©tat interne.
*   **Transitions entre les n≈ìuds** ‚Äî est la logique de routage : si le r√©sultat de l'√©tape pr√©c√©dente est tel, alors allez l√†.
*   **√âtat** ‚Äî est transmis entre les n≈ìuds et accumule tout ce qui est n√©cessaire : historique, conclusions interm√©diaires, entr√©e utilisateur, r√©sultats des op√©rations d'outils, etc.

Ainsi, nous obtenons un **m√©canisme flexible pour contr√¥ler la logique de l'agent**, dans lequel des sc√©narios simples et tr√®s complexes peuvent √™tre d√©crits : boucles, conditions, actions parall√®les, appels imbriqu√©s, et bien plus encore.

### Pourquoi est-ce pratique ?

LangGraph vous permet de construire une **logique transparente, reproductible et extensible** :

*   facile √† d√©boguer ;
*   facile √† visualiser ;
*   facile √† adapter √† de nouvelles t√¢ches ;
*   facile √† int√©grer des outils externes et des protocoles MCP.

Essentiellement, LangGraph est le **¬´cerveau¬ª de l'agent**, o√π chaque √©tape est document√©e, contr√¥lable et peut √™tre modifi√©e sans chaos ni ¬´magie¬ª.

### Et maintenant ‚Äî assez de th√©orie !

On pourrait encore longtemps parler des graphes, des √©tats, de la composition logique et des avantages de LangGraph par rapport aux pipelines classiques. Mais, comme le montre la pratique, il vaut mieux le voir une fois dans le code.

**Il est temps de passer √† la pratique.** Ensuite ‚Äî un exemple en Python : nous allons cr√©er un agent IA simple mais utile bas√© sur LangGraph qui utilisera des outils externes, la m√©moire et prendra ses propres d√©cisions.

### Pr√©paration : r√©seaux neuronaux cloud et locaux

Pour commencer √† cr√©er des agents IA, nous avons d'abord besoin d'un **cerveau** ‚Äî un mod√®le linguistique. Il existe deux approches ici :

*   **utiliser des solutions cloud**, o√π tout est pr√™t ¬´cl√© en main¬ª ;
*   ou **d√©ployer le mod√®le localement** ‚Äî pour une autonomie et une confidentialit√© compl√®tes.

Examinons les deux options.

#### Services cloud : rapides et pratiques

Le moyen le plus simple est d'utiliser la puissance des grands fournisseurs : OpenAI, Anthropic, et utiliser...

### O√π obtenir les cl√©s et les jetons :

*   **OpenAI** ‚Äî ChatGPT et autres produits ;
*   **Anthropic** ‚Äî Claude ;
*   **OpenRouter.ai** ‚Äî des dizaines de mod√®les (un jeton ‚Äî de nombreux mod√®les via une API compatible OpenAI) ;
*   **Amvera Cloud** ‚Äî possibilit√© de connecter LLAMA avec paiement en roubles et proxy int√©gr√© √† OpenAI et Anthropic.

Cette voie est pratique, surtout si vous :

*   ne voulez pas configurer l'infrastructure ;
*   d√©veloppez en mettant l'accent sur la vitesse ;
*   travaillez avec des ressources limit√©es.

### Mod√®les locaux : contr√¥le total

Si la **confidentialit√©, le travail hors ligne** sont importants pour vous, ou si vous souhaitez construire des **agents enti√®rement autonomes**, il est judicieux de d√©ployer le r√©seau neuronal localement.

**Principaux avantages :**

*   **Confidentialit√©** ‚Äî les donn√©es restent chez vous ;
*   **Travail hors ligne** ‚Äî utile dans les r√©seaux isol√©s ;
*   **Pas d'abonnements ni de jetons** ‚Äî gratuit apr√®s la configuration.

**Les inconv√©nients sont √©vidents :**

*   Exigences en mati√®re de ressources (en particulier pour la m√©moire vid√©o) ;
*   La configuration peut prendre du temps ;
*   Certains mod√®les sont difficiles √† d√©ployer sans exp√©rience.

N√©anmoins, il existe des outils qui simplifient le lancement local. L'un des meilleurs aujourd'hui est **Ollama**.

### D√©ploiement d'un LLM local via Ollama + Docker

Nous allons pr√©parer un lancement local du mod√®le Qwen 2.5 (qwen2.5:32b) √† l'aide d'un conteneur Docker et du syst√®me Ollama. Cela permettra d'int√©grer le r√©seau neuronal avec le MCP et de l'utiliser dans vos propres agents bas√©s sur LangGraph.

Si les ressources informatiques de votre ordinateur ou de votre serveur sont insuffisantes pour travailler avec cette version du mod√®le, vous pouvez toujours choisir un r√©seau neuronal moins ¬´gourmand¬ª en ressources ‚Äî le processus d'installation et de lancement restera similaire.

**Installation rapide (r√©sum√© des √©tapes)**

1.  **Installez Docker + Docker Compose**
2.  **Cr√©ez la structure du projet :**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **Cr√©ez `docker-compose.yml`**
(option universelle, GPU d√©tect√© automatiquement)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **D√©marrez le conteneur :**
```bash
docker compose up -d
```

5.  **T√©l√©chargez le mod√®le :**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **V√©rifiez le fonctionnement de l'API :**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Bonjour !", "stream": false}'
```
*(Image avec le r√©sultat de l'ex√©cution de la commande curl)*

7.  **Int√©gration avec Python :**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Expliquez l'intrication quantique"))
```
Vous disposez maintenant d'un LLM local √† part enti√®re, pr√™t √† fonctionner avec MCP et LangGraph.

**Et ensuite ?**

Nous avons le choix entre les mod√®les cloud et locaux, et nous avons appris √† connecter les deux. Le plus int√©ressant est √† venir ‚Äî **la cr√©ation d'agents IA sur LangGraph**, qui utilisent le mod√®le s√©lectionn√©, la m√©moire, les outils et leur propre logique.

**Passons √† la partie la plus savoureuse ‚Äî le code et la pratique !**

---

Avant de passer √† la pratique, il est important de pr√©parer l'environnement de travail. Je suppose que vous √™tes d√©j√† familiaris√© avec les bases de Python, que vous savez ce que sont les biblioth√®ques et les d√©pendances, et que vous comprenez pourquoi utiliser un environnement virtuel.

Si tout cela est nouveau pour vous ‚Äî je vous recommande de suivre d'abord un cours court ou un guide sur les bases de Python, puis de revenir √† l'article.

#### √âtape 1 : Cr√©ation d'un environnement virtuel

Cr√©ez un nouvel environnement virtuel dans le dossier du projet :
```bash
python -m venv venv
source venv/bin/activate  # pour Linux/macOS
venv\Scripts\activate   # pour Windows
```

#### √âtape 2 : Installation des d√©pendances

Cr√©ez un fichier `requirements.txt` et ajoutez-y les lignes suivantes :
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ‚ö†Ô∏è **Les versions actuelles sont celles du 21 juillet 2025.** Elles ont pu changer depuis la publication ‚Äî **v√©rifiez la pertinence avant l'installation.**

Ensuite, installez les d√©pendances :
```bash
pip install -r requirements.txt```

#### √âtape 3 : Configuration des variables d'environnement

Cr√©ez un fichier `.env` √† la racine du projet et ajoutez-y les cl√©s API n√©cessaires :
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Objectif des variables :**

*   **OPENAI_API_KEY** ‚Äî cl√© pour acc√©der aux mod√®les GPT d'OpenAI ;
*   **DEEPSEEK_API_KEY** ‚Äî cl√© pour utiliser les mod√®les Deepseek ;
*   **OPENROUTER_API_KEY** ‚Äî cl√© unique pour acc√©der √† de nombreux mod√®les via OpenRouter

---
Certains outils MCP (par exemple, `brave-web-search`) n√©cessitent une cl√© pour fonctionner. Sans elle, ils ne s'activeront tout simplement pas.

**Et si vous n'avez pas de cl√©s API ?**

Pas de probl√®me. Vous pouvez commencer le d√©veloppement avec un mod√®le local (par exemple, via Ollama), sans connecter aucun service externe. Dans ce cas, vous n'avez pas du tout besoin de cr√©er le fichier `.env`.

C'est fait ! Nous avons maintenant tout ce dont nous avons besoin pour commencer ‚Äî un environnement isol√©, des d√©pendances et, si n√©cessaire, un acc√®s aux r√©seaux neuronaux cloud et aux int√©grations MCP.

Ensuite, nous allons ex√©cuter notre agent LLM de diff√©rentes mani√®res.

### Lancement simple des agents LLM via LangGraph : int√©gration de base

Commen√ßons par le plus simple : comment ¬´connecter le cerveau¬ª au futur agent. Nous analyserons les moyens de base de lancer des mod√®les linguistiques (LLM) √† l'aide de LangChain, afin de passer √† l'√©tape suivante √† l'int√©gration avec LangGraph et √† la construction d'un agent IA √† part enti√®re.

#### Importations
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` et `load_dotenv()` ‚Äî pour charger les variables du fichier `.env`.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` ‚Äî wrappers pour connecter les mod√®les linguistiques via LangChain.

> üí° Si vous utilisez des approches alternatives pour travailler avec les configurations (par exemple, Pydantic Settings), vous pouvez remplacer `load_dotenv()` par votre m√©thode habituelle.

#### Chargement des variables d'environnement
```python
load_dotenv()
```
Cela chargera toutes les variables de `.env`, y compris les cl√©s pour acc√©der aux API OpenAI, DeepSeek, OpenRouter, et autres.

#### Fonctions simples pour obtenir le LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
Si la variable `OPENAI_API_KEY` est correctement d√©finie, LangChain la substituera automatiquement ‚Äî l'indication explicite `api_key=...` est ici facultative.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
De m√™me, mais nous utilisons le wrapper `ChatDeepSeek`.

**OpenRouter (et autres API compatibles)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Caract√©ristiques :**

*   `ChatOpenAI` est utilis√©, m√™me si le mod√®le ne provient pas d'OpenAI ‚Äî car OpenRouter utilise le m√™me protocole.
*   `base_url` est obligatoire : il pointe vers l'API OpenRouter.
*   Le mod√®le `moonshotai/kimi-k2:free` a √©t√© choisi comme l'une des options les plus √©quilibr√©es en termes de qualit√© et de vitesse au moment de la r√©daction.
*   La cl√© API `OpenRouter` doit √™tre pass√©e explicitement ‚Äî la substitution automatique ne fonctionne pas ici.

#### Mini-test : v√©rification du fonctionnement du mod√®le
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Qui es-tu ?")
    print(response.content)
```
*(Image avec le r√©sultat de l'ex√©cution de la commande)*

Si tout est configur√© correctement, vous recevrez une r√©ponse significative du mod√®le. F√©licitations ‚Äî la premi√®re √©tape est franchie !

### Mais ce n'est pas encore un agent

√Ä l'√©tape actuelle, nous avons connect√© le LLM et effectu√© un simple appel. Cela ressemble plus √† un chatbot de console qu'√† un agent IA.

**Pourquoi ?**

*   Nous √©crivons du **code synchrone et lin√©aire** sans logique d'√©tat ni objectifs.
*   L'agent ne prend pas de d√©cisions, ne m√©morise pas le contexte et n'utilise pas d'outils.
*   Le MCP et LangGraph ne sont pas encore impliqu√©s.

**Et ensuite ?**

Ensuite, nous allons impl√©menter un **agent IA √† part enti√®re** en utilisant **LangGraph** ‚Äî d'abord sans MCP, pour nous concentrer sur l'architecture, les √©tats et la logique de l'agent lui-m√™me.

Plongeons dans la v√©ritable m√©canique des agents. C'est parti !

### Agent de classification des offres d'emploi : de la th√©orie √† la pratique

...concepts de LangGraph en pratique et cr√©er un outil utile pour les plateformes RH et les bourses de freelances.

#### T√¢che de l'agent

Notre agent prend en entr√©e une description textuelle d'une offre d'emploi ou d'un service et effectue une classification √† trois niveaux :

1.  **Type de travail** : travail de projet ou poste permanent
2.  **Cat√©gorie professionnelle** : parmi plus de 45 sp√©cialit√©s pr√©d√©finies
3.  **Type de recherche** : si une personne cherche un emploi ou cherche un prestataire

Le r√©sultat est renvoy√© au format JSON structur√© avec un score de confiance pour chaque classification.

#### üìà Architecture de l'agent sur LangGraph

En suivant les principes de LangGraph, nous cr√©ons un **graphe d'√©tats** de quatre n≈ìuds :

- Description d'entr√©e
- ‚Üì
- N≈ìud de classification du type de travail
- ‚Üì
- N≈ìud de classification de la cat√©gorie
- ‚Üì
- N≈ìud de d√©termination du type de recherche
- ‚Üì
- N≈ìud de calcul de la confiance
- ‚Üì
- R√©sultat JSON

Chaque n≈ìud est une **fonction sp√©cialis√©e** qui :

*   Re√ßoit l'√©tat actuel de l'agent
*   Effectue sa partie de l'analyse
*   Met √† jour l'√©tat et le transmet

#### Gestion de l'√©tat

D√©finir la **structure de la m√©moire de l'agent** via `TypedDict` :

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """√âtat de l'agent pour stocker les informations sur le processus de classification"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

C'est la **m√©moire de travail de l'agent** ‚Äî tout ce qu'il m√©morise et accumule pendant l'analyse. Semblable √† la fa√ßon dont un expert humain garde le contexte de la t√¢che √† l'esprit lors de l'analyse d'un document.

Examinons le code complet, puis concentrons-nous sur les points principaux.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Cat√©gories professionnelles
CATEGORIES = [
    "Animateur 2D", "Animateur 3D", "Modeleur 3D",
    "Analyste commercial", "D√©veloppeur Blockchain", ...
]

class JobType(Enum):
    PROJECT = "travail de projet"
    PERMANENT = "travail permanent"

class SearchType(Enum):
    LOOKING_FOR_WORK = "recherche d'emploi"
    LOOKING_FOR_PERFORMER = "recherche d'un prestataire"

class State(TypedDict):
    """√âtat de l'agent pour stocker les informations sur le processus de classification"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Agent asynchrone pour la classification des offres d'emploi et des services"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Initialisation de l'agent"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Cr√©e le flux de travail de l'agent bas√© sur LangGraph"""
        workflow = StateGraph(State)
        
        # Ajouter des n≈ìuds au graphe
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # D√©finir la s√©quence d'ex√©cution des n≈ìuds
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer le type de travail : projet ou permanent"""
        # ... (l'impl√©mentation suit)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer la cat√©gorie professionnelle"""
        # ... (l'impl√©mentation suit)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer le type de recherche"""
        # ... (l'impl√©mentation suit)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour calculer le niveau de confiance dans la classification"""
        # ... (l'impl√©mentation suit)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Trouve la cat√©gorie la plus proche dans la liste des cat√©gories disponibles"""
        # ... (l'impl√©mentation suit)

    async def classify(self, description: str) -> Dict[str, Any]:
        """M√©thode principale pour classer les offres d'emploi/services"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Lancer le flux de travail
        result = await self.workflow.ainvoke(initial_state)
        
        # Former la r√©ponse JSON finale
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """D√©monstration du fonctionnement de l'agent"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "D√©veloppeur Python requis pour cr√©er une application web sur Django. Poste permanent.",
        "Recherche de commandes pour cr√©er des logos et une identit√© d'entreprise. Je travaille avec Adobe Illustrator.",
        "Besoin d'un animateur 3D pour un projet √† court terme de cr√©ation d'une publicit√©.",
        "CV : marketeur exp√©riment√©, recherche un travail √† distance dans le marketing num√©rique",
        "Recherche un d√©veloppeur frontend React pour notre √©quipe √† temps plein"
    ]
    
    print("ü§ñ D√©monstration du fonctionnement de l'agent de classification des offres d'emploi\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Test {i} : ---")
        print(f"Description : {description}")
        try:
            result = await agent.classify(description)
            print("R√©sultat de la classification :")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå Erreur : {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...le reste du code avec l'impl√©mentation des m√©thodes a √©t√© pr√©sent√© dans l'article...)*

### Avantages cl√©s de l'architecture
1.  **Modularit√©** ‚Äî chaque n≈ìud r√©sout une t√¢che, facile √† tester et √† am√©liorer s√©par√©ment
2.  **Extensibilit√©** ‚Äî de nouveaux n≈ìuds d'analyse peuvent √™tre ajout√©s sans modifier les existants
3.  **Transparence** ‚Äî l'ensemble du processus de prise de d√©cision est document√© et tra√ßable
4.  **Performance** ‚Äî traitement asynchrone de plusieurs requ√™tes
5.  **Fiabilit√©** ‚Äî m√©canismes de secours et gestion des erreurs

### B√©n√©fices r√©els
Un tel agent peut √™tre utilis√© dans :
*   **Plateformes RH** pour la cat√©gorisation automatique des CV et des offres d'emploi
*   **Bourses de freelances** pour am√©liorer la recherche et les recommandations
*   **Syst√®mes internes** des entreprises pour le traitement des demandes et des projets
*   **Solutions analytiques** pour l'√©tude du march√© du travail

### MCP en action : cr√©ation d'un agent avec syst√®me de fichiers et recherche web
Apr√®s avoir compris les principes de base de LangGraph et cr√©√© un agent classificateur simple, √©tendons ses capacit√©s en le connectant au monde ext√©rieur via MCP.

Nous allons maintenant cr√©er un assistant IA √† part enti√®re qui pourra :
*   Travailler avec le syst√®me de fichiers (lire, cr√©er, modifier des fichiers)
*   Rechercher des informations pertinentes sur Internet
*   M√©moriser le contexte du dialogue
*   G√©rer les erreurs et r√©cup√©rer apr√®s des pannes

#### De la th√©orie aux outils r√©els
Vous vous souvenez comment, au d√©but de l'article, nous avons parl√© du fait que **le MCP est un pont entre un r√©seau neuronal et son environnement** ? Vous allez maintenant le voir en pratique. Notre agent aura acc√®s √† des **outils r√©els** :
```
# Outils du syst√®me de fichiers
- read_file ‚Äî lire des fichiers
- write_file ‚Äî √©crire et cr√©er des fichiers
- list_directory ‚Äî afficher le contenu des dossiers
- create_directory ‚Äî cr√©er des dossiers

# Outils de recherche web
- brave_web_search ‚Äî rechercher sur Internet
- get_web_content ‚Äî obtenir le contenu des pages
```
Ce n'est plus un agent ¬´jouet¬ª ‚Äî c'est un **outil de travail** qui peut r√©soudre des probl√®mes r√©els.

#### üìà Architecture : du simple au complexe

**1. Configuration comme base de la stabilit√©**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Configuration simplifi√©e de l'agent IA"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Validation de la configuration"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Le chemin n'existe pas : {self.filesystem_path}")
```
**Pourquoi est-ce important ?** Contrairement √† l'exemple de classification, ici l'agent interagit avec des syst√®mes externes. Une erreur dans le chemin du fichier ou une cl√© API manquante ‚Äî et tout l'agent cesse de fonctionner. La **validation au d√©marrage** permet d'√©conomiser des heures de d√©bogage.

**2. Fabrique de mod√®les : choix flexible**
```python
def create_model(config: AgentConfig):
    """Cr√©e un mod√®le selon la configuration"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... autres fournisseurs
```
Un code ‚Äî de nombreux mod√®les. Vous voulez un mod√®le local gratuit ? Utilisez Ollama. Besoin d'une pr√©cision maximale ? Passez √† GPT-4. La vitesse est importante ? Essayez DeepSeek. Le code reste le m√™me.

**3. Int√©gration MCP : connexion au monde r√©el**
```python
async def _init_mcp_client(self):
    """Initialisation du client MCP"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
Ici, le travail cl√© du MCP se produit : nous connectons des serveurs MCP externes √† l'agent, qui fournissent un ensemble d'outils et de fonctions. L'agent re√ßoit alors non seulement des fonctions individuelles, mais une compr√©hension contextuelle compl√®te de la fa√ßon de travailler avec le syst√®me de fichiers et Internet.

#### R√©silience aux erreurs
Dans le monde r√©el, tout tombe en panne : le r√©seau est indisponible, les fichiers sont bloqu√©s, les cl√©s API sont expir√©es. Notre agent est pr√™t pour cela :
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
Le d√©corateur `@retry_on_failure` relance automatiquement les op√©rations en cas de d√©faillances temporaires. L'utilisateur ne remarquera m√™me pas que quelque chose s'est mal pass√©.

### R√©sultats : de la th√©orie √† la pratique des agents IA

Aujourd'hui, nous avons parcouru le chemin des concepts de base √† la cr√©ation d'agents IA fonctionnels. R√©sumons ce que nous avons appris et r√©alis√©.

**Ce que nous avons ma√Ætris√©**

**1. Concepts fondamentaux**
*   Compris la diff√©rence entre les chatbots et les v√©ritables agents IA
*   Compris le r√¥le du **MCP (Model Context Protocol)** comme pont entre le mod√®le et le monde ext√©rieur
*   √âtudi√© l'architecture de **LangGraph** pour construire une logique d'agent complexe

**2. Comp√©tences pratiques**
*   Configur√© un environnement de travail avec prise en charge des mod√®les cloud et locaux
*   Cr√©√© un **agent classificateur** avec une architecture asynchrone et une gestion d'√©tat
*   Construit un **agent MCP** avec acc√®s au syst√®me de fichiers et √† la recherche web

**3. Mod√®les architecturaux**
*   Ma√Ætris√© la configuration modulaire et les fabriques de mod√®les
*   Impl√©ment√© la gestion des erreurs et les **m√©canismes de r√©essai** pour des solutions pr√™tes pour la production

### Avantages cl√©s de l'approche
**LangGraph + MCP** nous offrent :
*   **Transparence** ‚Äî chaque √©tape de l'agent est document√©e et tra√ßable
*   **Extensibilit√©** ‚Äî de nouvelles fonctionnalit√©s sont ajout√©es de mani√®re d√©clarative
*   **Fiabilit√©** ‚Äî gestion des erreurs et r√©cup√©ration int√©gr√©es
*   **Flexibilit√©** ‚Äî prise en charge de plusieurs mod√®les et fournisseurs pr√™ts √† l'emploi

### Conclusion

Les agents IA ne sont pas une fiction futuriste, mais une **technologie r√©elle d'aujourd'hui**. Avec LangGraph et MCP, nous pouvons cr√©er des syst√®mes qui r√©solvent des probl√®mes commerciaux sp√©cifiques, automatisent les routines et ouvrent de nouvelles possibilit√©s.

**L'essentiel est de commencer.** Prenez le code des exemples, adaptez-le √† vos t√¢ches, exp√©rimentez. Chaque projet est une nouvelle exp√©rience et un pas vers la ma√Ætrise dans le domaine du d√©veloppement IA.

Bonne chance dans vos projets !

---
*Tags: python, ia, mcp, langchain, assistant ia, ollama, agents ia, llm local, langgraph, mcp-server*
*Hubs: Blog de l'entreprise Amvera, Traitement du langage naturel, Intelligence artificielle, Python, Programmation*
