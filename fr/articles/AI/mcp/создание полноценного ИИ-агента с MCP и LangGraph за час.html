<h1>Comment apprendre √† un r√©seau neuronal √† travailler avec ses mains : cr√©er un agent IA √† part enti√®re avec MCP et LangGraph en une heure</h1>
<p>Salut les amis ! J'esp√®re que vous m'avez manqu√©.</p>
<p>Ces deux derniers mois, je me suis plong√© dans la recherche sur l'int√©gration des agents IA dans mes propres projets Python. Au cours de ce processus, j'ai accumul√© de nombreuses connaissances et observations pratiques qu'il serait dommage de ne pas partager. C'est pourquoi aujourd'hui, je reviens sur Habr ‚Äî avec un nouveau sujet, un regard neuf et l'intention d'√©crire plus souvent.</p>
<p>√Ä l'ordre du jour aujourd'hui : LangGraph et MCP, des outils avec lesquels vous pouvez cr√©er des agents IA vraiment utiles.</p>
<p>Si auparavant nous d√©battions de quel r√©seau neuronal r√©pondait le mieux en russe, aujourd'hui le champ de bataille s'est d√©plac√© vers des t√¢ches plus appliqu√©es : qui g√®re le mieux le r√¥le d'un agent IA ? Quels frameworks simplifient r√©ellement le d√©veloppement ? Et comment int√©grer tout cela dans un projet r√©el ?</p>
<p>Mais avant de plonger dans la pratique et le code, comprenons les concepts de base. Surtout les deux principaux : les <strong>agents IA et le MCP</strong>. Sans eux, la conversation sur LangGraph serait incompl√®te.</p>
<h3>Les agents IA en termes simples</h3>
<p>Les agents IA ne sont pas de simples chatbots ¬´am√©lior√©s¬ª. Ce sont des entit√©s plus complexes et autonomes qui poss√®dent deux caract√©ristiques cruciales :</p>
<ol>
<li><strong>Capacit√© d'interagir et de se coordonner</strong></li>
</ol>
<p>Les agents modernes sont capables de diviser les t√¢ches en sous-t√¢ches, d'appeler d'autres agents, de demander des donn√©es externes et de travailler en √©quipe. Ce n'est plus un assistant solitaire, mais un syst√®me distribu√© o√π chaque composant peut apporter sa contribution.</p>
<ol start="2">
<li><strong>Acc√®s aux ressources externes</strong></li>
</ol>
<p>Un agent IA n'est plus limit√© par les fronti√®res d'un dialogue. Il peut acc√©der √† des bases de donn√©es, effectuer des appels API, interagir avec des fichiers locaux, des bases de connaissances vectorielles et m√™me ex√©cuter des commandes dans le terminal. Tout cela est devenu possible gr√¢ce √† l'√©mergence du MCP ‚Äî un nouveau niveau d'int√©gration entre le mod√®le et l'environnement.</p>
<hr>
<p>Pour faire simple : <strong>le MCP est un pont entre un r√©seau neuronal et son environnement</strong>. Il permet au mod√®le de ¬´comprendre¬ª le contexte d'une t√¢che, d'acc√©der aux donn√©es, d'effectuer des appels et de former des actions raisonn√©es, plut√¥t que de simplement produire des r√©ponses textuelles.</p>
<p><strong>Imaginons une analogie :</strong></p>
<ul>
<li>Vous avez un <strong>r√©seau neuronal</strong> ‚Äî il peut raisonner et g√©n√©rer des textes.</li>
<li>Il y a des <strong>donn√©es et des outils</strong> ‚Äî des documents, des API, des bases de connaissances, un terminal, du code.</li>
<li>Et il y a le <strong>MCP</strong> ‚Äî c'est une interface qui permet au mod√®le d'interagir avec ces sources externes comme si elles faisaient partie de son monde interne.</li>
</ul>
<p><strong>Sans MCP :</strong></p>
<p>Le mod√®le est un moteur de dialogue isol√©. Vous lui donnez du texte, il r√©pond. Et c'est tout.</p>
<p><strong>Avec MCP :</strong></p>
<p>Le mod√®le devient un <strong>ex√©cuteur de t√¢ches</strong> √† part enti√®re :</p>
<ul>
<li>acc√®de aux structures de donn√©es et aux API ;</li>
<li>appelle des fonctions externes ;</li>
<li>s'oriente dans l'√©tat actuel du projet ou de l'application ;</li>
<li>peut m√©moriser, suivre et modifier le contexte au fur et √† mesure du dialogue ;</li>
<li>utilise des extensions telles que des outils de recherche, des ex√©cuteurs de code, des bases de donn√©es d'embeddings vectoriels, etc.</li>
</ul>
<p>Au sens technique, <strong>le MCP est un protocole d'interaction entre un LLM et son environnement</strong>, o√π le contexte est fourni sous forme d'objets structur√©s (au lieu de texte ¬´brut¬ª), et les appels sont format√©s comme des op√©rations interactives (par exemple, l'appel de fonctions, l'utilisation d'outils ou les actions d'agent). C'est ce qui transforme un mod√®le ordinaire en un <strong>v√©ritable agent IA</strong>, capable de faire plus que simplement ¬´parler¬ª.</p>
<h3>Et maintenant ‚Äî au travail !</h3>
<p>Maintenant que nous avons abord√© les concepts de base, il est logique de se poser la question : ¬´Comment mettre tout cela en pratique en Python ?¬ª</p>
<p>C'est l√† qu'intervient <strong>LangGraph</strong> ‚Äî un framework puissant pour construire des graphes d'√©tat, des comportoments d'agents et des cha√Ænes de pens√©e. Il permet de ¬´coudre¬ª la logique d'interaction entre les agents, les outils et l'utilisateur, cr√©ant une architecture IA vivante qui s'adapte aux t√¢ches.</p>
<p>Dans les sections suivantes, nous verrons comment :</p>
<ul>
<li>un agent est construit √† partir de z√©ro ;</li>
<li>les √©tats, les transitions et les √©v√©nements sont cr√©√©s ;</li>
<li>les fonctions et les outils sont int√©gr√©s ;</li>
<li>et comment tout cet √©cosyst√®me fonctionne dans un projet r√©el.</li>
</ul>
<h3>Un peu de th√©orie : qu'est-ce que LangGraph</h3>
<p>Avant de passer √† la pratique, quelques mots sur le framework lui-m√™me.</p>
<p><strong>LangGraph</strong> ‚Äî est un projet de l'√©quipe <strong>LangChain</strong>, ceux-l√† m√™mes qui ont les premiers propos√© le concept de ¬´cha√Ænes¬ª (chains) d'interaction avec les LLM. Si auparavant l'accent √©tait mis sur les pipelines lin√©aires ou √† ramification conditionnelle (langchain.chains), les d√©veloppeurs misent d√©sormais sur un <strong>mod√®le de graphe</strong>, et c'est LangGraph qu'ils recommandent comme le nouveau ¬´noyau¬ª pour la construction de syst√®mes IA complexes.</p>
<p><strong>LangGraph</strong> ‚Äî est un framework pour construire des automates finis et des graphes d'√©tat, o√π chaque <strong>n≈ìud</strong> repr√©sente une partie de la logique de l'agent : un appel de mod√®le, un outil externe, une condition, une entr√©e utilisateur, etc.</p>
<h3>Comment √ßa marche : graphes et n≈ìuds</h3>
<p>Conceptuellement, LangGraph est construit sur les id√©es suivantes :</p>
<ul>
<li><strong>Graphe</strong> ‚Äî est une structure qui d√©crit les chemins possibles d'ex√©cution de la logique. On peut le consid√©rer comme une carte : d'un point, on peut passer √† un autre en fonction des conditions ou du r√©sultat de l'ex√©cution.</li>
<li><strong>N≈ìuds</strong> ‚Äî sont des √©tapes sp√©cifiques au sein du graphe. Chaque n≈ìud ex√©cute une fonction : appelle un mod√®le, appelle une API externe, v√©rifie une condition ou met simplement √† jour l'√©tat interne.</li>
<li><strong>Transitions entre les n≈ìuds</strong> ‚Äî est la logique de routage : si le r√©sultat de l'√©tape pr√©c√©dente est tel, alors allez-y.</li>
<li><strong>√âtat</strong> ‚Äî est transmis entre les n≈ìuds et accumule tout ce qui est n√©cessaire : historique, conclusions interm√©diaires, entr√©e utilisateur, r√©sultat des op√©rations d'outils, etc.</li>
</ul>
<p>Ainsi, nous obtenons un <strong>m√©canisme flexible de gestion de la logique de l'agent</strong>, dans lequel des sc√©narios simples et tr√®s complexes peuvent √™tre d√©crits : boucles, conditions, actions parall√®les, appels imbriqu√©s et bien plus encore.</p>
<h3>Pourquoi est-ce pratique ?</h3>
<p>LangGraph vous permet de construire une <strong>logique transparente, reproductible et extensible</strong> :</p>
<ul>
<li>facile √† d√©boguer ;</li>
<li>facile √† visualiser ;</li>
<li>facile √† adapter √† de nouvelles t√¢ches ;</li>
<li>facile √† int√©grer des outils externes et des protocoles MCP.</li>
</ul>
<p>Essentiellement, LangGraph est le <strong>¬´cerveau¬ª de l'agent</strong>, o√π chaque √©tape est document√©e, contr√¥lable et peut √™tre modifi√©e sans chaos ni ¬´magie¬ª.</p>
<h3>Eh bien, assez de th√©orie !</h3>
<p>On pourrait encore longtemps parler des graphes, des √©tats, de la composition logique et des avantages de LangGraph par rapport aux pipelines classiques. Mais, comme le montre la pratique, il vaut mieux le voir une fois dans le code.</p>
<p><strong>Il est temps de passer √† la pratique.</strong> Ensuite ‚Äî un exemple en Python : nous allons cr√©er un agent IA simple mais utile bas√© sur LangGraph qui utilisera des outils externes, la m√©moire et prendra des d√©cisions par lui-m√™me.</p>
<h3>Pr√©paration : r√©seaux neuronaux cloud et locaux</h3>
<p>Pour commencer √† cr√©er des agents IA, nous avons d'abord besoin d'un <strong>cerveau</strong> ‚Äî un mod√®le linguistique. Il existe deux approches ici :</p>
<ul>
<li><strong>utiliser des solutions cloud</strong>, o√π tout est pr√™t ¬´cl√© en main¬ª ;</li>
<li>ou <strong>monter le mod√®le localement</strong> ‚Äî pour une autonomie et une confidentialit√© compl√®tes.</li>
</ul>
<p>Examinons les deux options.</p>
<h4>Services cloud : rapides et pratiques</h4>
<p>Le moyen le plus simple est d'utiliser la puissance des grands fournisseurs : OpenAI, Anthropic, et d'utiliser...</p>
<h3>O√π obtenir les cl√©s et les jetons :</h3>
<ul>
<li><strong>OpenAI</strong> ‚Äî ChatGPT et autres produits ;</li>
<li><strong>Anthropic</strong> ‚Äî Claude ;</li>
<li><strong>OpenRouter.ai</strong> ‚Äî des dizaines de mod√®les (un jeton ‚Äî de nombreux mod√®les via une API compatible OpenAI) ;</li>
<li><strong>Amvera Cloud</strong> ‚Äî possibilit√© de connecter LLAMA avec paiement en roubles et proxy int√©gr√© vers OpenAI et Anthropic.</li>
</ul>
<p>Cette voie est pratique, surtout si vous :</p>
<ul>
<li>ne voulez pas configurer l'infrastructure ;</li>
<li>d√©veloppez en mettant l'accent sur la vitesse ;</li>
<li>travaillez avec des ressources limit√©es.</li>
</ul>
<h3>Mod√®les locaux : contr√¥le total</h3>
<p>Si la <strong>confidentialit√©, le travail hors ligne</strong> sont importants pour vous, ou si vous souhaitez cr√©er des <strong>agents enti√®rement autonomes</strong>, il est judicieux de d√©ployer le r√©seau neuronal localement.</p>
<p><strong>Principaux avantages :</strong></p>
<ul>
<li><strong>Confidentialit√©</strong> ‚Äî les donn√©es restent avec vous ;</li>
<li><strong>Travail hors ligne</strong> ‚Äî utile dans les r√©seaux isol√©s ;</li>
<li><strong>Pas d'abonnements ni de jetons</strong> ‚Äî gratuit apr√®s la configuration.</li>
</ul>
<p><strong>Les inconv√©nients sont √©vidents :</strong></p>
<ul>
<li>Exigences en mati√®re de ressources (en particulier pour la m√©moire vid√©o) ;</li>
<li>La configuration peut prendre du temps ;</li>
<li>Certains mod√®les sont difficiles √† d√©ployer sans exp√©rience.</li>
</ul>
<p>N√©anmoins, il existe des outils qui facilitent le lancement local. L'un des meilleurs aujourd'hui est <strong>Ollama</strong>.</p>
<h3>D√©ploiement de LLM local via Ollama + Docker</h3>
<p>Nous allons pr√©parer un lancement local du mod√®le Qwen 2.5 (qwen2.5:32b) √† l'aide d'un conteneur Docker et du syst√®me Ollama. Cela permettra d'int√©grer le r√©seau neuronal avec le MCP et de l'utiliser dans vos propres agents bas√©s sur LangGraph.</p>
<p>Si les ressources de calcul de votre ordinateur ou de votre serveur sont insuffisantes pour travailler avec cette version du mod√®le, vous pouvez toujours choisir un r√©seau neuronal moins ¬´gourmand en ressources¬ª ‚Äî le processus d'installation et de lancement restera similaire.</p>
<p><strong>Installation rapide (r√©sum√© des √©tapes)</strong></p>
<ol>
<li><strong>Installez Docker + Docker Compose</strong></li>
<li><strong>Cr√©ez la structure du projet :</strong>
<pre class="line-numbers"><code class="language-bash">mkdir qwen-local && cd qwen-local
</code></pre>
</li>
<li><strong>Cr√©ez <code>docker-compose.yml</code></strong>
(option universelle, le GPU est d√©tect√© automatiquement)
<pre class="line-numbers"><code class="language-yaml">services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
</code></pre>
</li>
<li><strong>D√©marrez le conteneur :</strong>
<pre class="line-numbers"><code class="language-bash">docker compose up -d
</code></pre>
</li>
<li><strong>T√©l√©chargez le mod√®le :</strong>
<pre class="line-numbers"><code class="language-bash">docker exec -it ollama_qwen ollama pull qwen2.5:32b
</code></pre>
</li>
<li><strong>V√©rifiez le fonctionnement via l'API :</strong>
<pre class="line-numbers"><code class="language-bash">curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Bonjour !", "stream": false}'
</code></pre>
<p>*(Image avec le r√©sultat de l'ex√©cution de la commande curl)*</p>
</li>
<li><strong>Int√©gration avec Python :</strong>
<pre class="line-numbers"><code class="language-python">import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Expliquez l'intrication quantique"))
</code></pre>
<p>Vous disposez maintenant d'un LLM local √† part enti√®re, pr√™t √† fonctionner avec le MCP et LangGraph.</p>
</li>
</ol>
<p><strong>Et ensuite ?</strong></p>
<p>Nous avons le choix entre les mod√®les cloud et locaux, et nous avons appris √† connecter les deux. Le plus int√©ressant est √† venir : <strong>la cr√©ation d'agents IA sur LangGraph</strong>, qui utiliseront le mod√®le s√©lectionn√©, la m√©moire, les outils et leur propre logique.</p>
<p><strong>Passons √† la partie la plus excitante : le code et la pratique !</strong></p>
<hr>
<p>Avant de passer √† la pratique, il est important de pr√©parer l'environnement de travail. Je suppose que vous √™tes d√©j√† familiaris√© avec les bases de Python, que vous savez ce que sont les biblioth√®ques et les d√©pendances, et que vous comprenez pourquoi utiliser un environnement virtuel.</p>
<p>Si tout cela est nouveau pour vous, je vous recommande de suivre d'abord un cours court ou un guide sur les bases de Python, puis de revenir √† l'article.</p>
<h4>√âtape 1 : Cr√©ation d'un environnement virtuel</h4>
<p>Cr√©ez un nouvel environnement virtuel dans le dossier du projet :</p>
<pre class="line-numbers"><code class="language-bash">python -m venv venv
source venv/bin/activate  # pour Linux/macOS
venc\Scripts\activate   # pour Windows
</code></pre>
<h4>√âtape 2 : Installation des d√©pendances</h4>
<p>Cr√©ez un fichier <code>requirements.txt</code> et ajoutez-y les lignes suivantes :</p>
<pre class="line-numbers"><code class="language-text">langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
</code></pre>
<blockquote>
<p>‚ö†Ô∏è <strong>Les versions actuelles sont indiqu√©es au 21 juillet 2025.</strong> Depuis la publication, elles peuvent avoir chang√© ‚Äî <strong>v√©rifiez la pertinence avant l'installation.</strong></p>
</blockquote>
<p>Ensuite, installez les d√©pendances :</p>
<pre class="line-numbers"><code class="language-bash">pip install -r requirements.txt</code></pre>
<h4>√âtape 3 : Configuration des variables d'environnement</h4>
<p>Cr√©ez un fichier <code>.env</code> √† la racine du projet et ajoutez-y les cl√©s API n√©cessaires :</p>
<pre class="line-numbers"><code class="language-text">OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
</code></pre>
<p><strong>Objectif des variables :</strong></p>
<ul>
<li><strong>OPENAI_API_KEY</strong> ‚Äî cl√© pour acc√©der aux mod√®les GPT d'OpenAI ;</li>
<li><strong>DEEPSEEK_API_KEY</strong> ‚Äî cl√© pour utiliser les mod√®les Deepseek ;</li>
<li><strong>OPENROUTER_API_KEY</strong> ‚Äî cl√© unique pour acc√©der √† de nombreux mod√®les via OpenRouter</li>
</ul>
<hr>
<p>Certains outils MCP (par exemple, <code>brave-web-search</code>) n√©cessitent une cl√© pour fonctionner. Sans elle, ils ne s'activeront tout simplement pas.</p>
<p><strong>Et si vous n'avez pas de cl√©s API ?</strong></p>
<p>Pas de probl√®me. Vous pouvez commencer le d√©veloppement avec un mod√®le local (par exemple, via Ollama), sans connecter aucun service externe. Dans ce cas, le fichier <code>.env</code> peut √™tre omis enti√®rement.</p>
<p>C'est fait ! Nous avons maintenant tout ce dont nous avons besoin pour commencer ‚Äî un environnement isol√©, des d√©pendances et, si n√©cessaire, un acc√®s aux r√©seaux neuronaux cloud et aux int√©grations MCP.</p>
<p>Ensuite, nous lancerons notre agent LLM de diff√©rentes mani√®res.</p>
<h3>Lancement simple des agents LLM via LangGraph : int√©gration de base</h3>
<p>Commen√ßons par le plus simple : comment ¬´connecter le cerveau¬ª au futur agent. Nous analyserons les m√©thodes de base de lancement des mod√®les linguistiques (LLM) avec LangChain, afin de passer √† l'√©tape suivante √† l'int√©gration avec LangGraph et √† la construction d'un agent IA √† part enti√®re.</p>
<h4>Importations</h4>
<pre class="line-numbers"><code class="language-python">import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
</code></pre>
<ul>
<li><code>os</code> et <code>load_dotenv()</code> ‚Äî pour charger les variables du fichier <code>.env</code>.</li>
<li><code>ChatOpenAI</code>, <code>ChatOllama</code>, <code>ChatDeepSeek</code> ‚Äî wrappers pour connecter les mod√®les linguistiques via LangChain.</li>
</ul>
<blockquote>
<p>üí° Si vous utilisez des approches alternatives pour travailler avec les configurations (par exemple, Pydantic Settings), vous pouvez remplacer <code>load_dotenv()</code> par votre m√©thode habituelle.</p>
</blockquote>
<h4>Chargement des variables d'environnement</h4>
<pre class="line-numbers"><code class="language-python">load_dotenv()
</code></pre>
<p>Cela chargera toutes les variables de <code>.env</code>, y compris les cl√©s pour acc√©der aux API OpenAI, DeepSeek, OpenRouter et autres.</p>
<h4>Fonctions simples pour obtenir le LLM</h4>
<p><strong>OpenAI</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
</code></pre>
<p>Si la variable <code>OPENAI_API_KEY</code> est correctement d√©finie, LangChain la substituera automatiquement ‚Äî la sp√©cification explicite de <code>api_key=...</code> est facultative ici.</p>
<p><strong>DeepSeek</strong></p>
<pre class="line-numbers"><code class="language-python">def get_deepseek_llm():
    # ...
</code></pre>
<p>De m√™me, mais nous utilisons le wrapper <code>ChatDeepSeek</code>.</p>
<p><strong>OpenRouter (et autres API compatibles)</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
</code></pre>
<p><strong>Caract√©ristiques :</strong></p>
<ul>
<li><code>ChatOpenAI</code> est utilis√©, m√™me si le mod√®le ne provient pas d'OpenAI ‚Äî car OpenRouter utilise le m√™me protocole.</li>
<li><code>base_url</code> est obligatoire : il pointe vers l'API OpenRouter.</li>
<li>Le mod√®le <code>moonshotai/kimi-k2:free</code> a √©t√© choisi comme l'une des options les plus √©quilibr√©es en termes de qualit√© et de vitesse au moment de la r√©daction.</li>
<li>La cl√© API <code>OpenRouter</code> doit √™tre pass√©e explicitement ‚Äî la substitution automatique ne fonctionne pas ici.</li>
</ul>
<h4>Mini-test : v√©rification du fonctionnement du mod√®le</h4>
<pre class="line-numbers"><code class="language-python">if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Qui √™tes-vous ?")
    print(response.content)
</code></pre>
<p>*(Image avec le r√©sultat de l'ex√©cution de la commande curl : <code>Je suis un assistant IA cr√©√© par Moonshot AI...</code>)*</p>
<p>Si tout est configur√© correctement, vous recevrez une r√©ponse significative du mod√®le. F√©licitations ‚Äî la premi√®re √©tape est franchie !</p>
<h3>Mais ce n'est pas encore un agent</h3>
<p>√Ä ce stade, nous avons connect√© le LLM et effectu√© un simple appel. Cela ressemble plus √† un chatbot console qu'√† un agent IA.</p>
<p><strong>Pourquoi ?</strong></p>
<ul>
<li>Nous √©crivons du <strong>code synchrone et lin√©aire</strong> sans logique d'√©tat ni d'objectif.</li>
<li>L'agent ne prend pas de d√©cisions, ne m√©morise pas le contexte et n'utilise pas d'outils.</li>
<li>Le MCP et LangGraph ne sont pas encore impliqu√©s.</li>
</ul>
<p><strong>Et ensuite ?</strong></p>
<p>Ensuite, nous allons impl√©menter un <strong>agent IA √† part enti√®re</strong> en utilisant <strong>LangGraph</strong> ‚Äî d'abord sans MCP, pour nous concentrer sur l'architecture, les √©tats et la logique de l'agent lui-m√™me.</p>
<p>Plongeons dans la v√©ritable m√©canique des agents. Allons-y !</p>
<h3>Agent de classification des offres d'emploi : de la th√©orie √† la pratique</h3>
<p>...les concepts de LangGraph en pratique et cr√©er un outil utile pour les plateformes RH et les bourses de freelances.</p>
<h4>T√¢che de l'agent</h4>
<p>Notre agent prend en entr√©e une description textuelle d'une offre d'emploi ou d'un service et effectue une classification √† trois niveaux :</p>
<ol>
<li><strong>Type de travail</strong> : travail de projet ou poste permanent</li>
<li><strong>Cat√©gorie professionnelle</strong> : parmi plus de 45 sp√©cialit√©s pr√©d√©finies</li>
<li><strong>Type de recherche</strong> : si la personne cherche un emploi ou cherche un prestataire</li>
</ol>
<p>Le r√©sultat est renvoy√© au format JSON structur√© avec un score de confiance pour chaque classification.</p>
<h4>üìà Architecture de l'agent sur LangGraph</h4>
<p>En suivant les principes de LangGraph, nous cr√©ons un <strong>graphe d'√©tat</strong> de quatre n≈ìuds :</p>
<ul>
<li>Description d'entr√©e</li>
<li>‚Üì</li>
<li>N≈ìud de classification du type de travail</li>
<li>‚Üì</li>
<li>N≈ìud de classification de la cat√©gorie</li>
<li>‚Üì</li>
<li>N≈ìud de d√©termination du type de recherche</li>
<li>‚Üì</li>
<li>N≈ìud de calcul de la confiance</li>
<li>‚Üì</li>
<li>R√©sultat JSON</li>
</ul>
<p>Chaque n≈ìud est une <strong>fonction sp√©cialis√©e</strong> qui :</p>
<ul>
<li>Re√ßoit l'√©tat actuel de l'agent</li>
<li>Effectue sa partie de l'analyse</li>
<li>Met √† jour l'√©tat et le transmet</li>
</ul>
<h4>Gestion de l'√©tat</h4>
<p>Nous d√©finissons la <strong>structure de m√©moire de l'agent</strong> via <code>TypedDict</code> :</p>
<pre class="line-numbers"><code class="language-python">from typing import TypedDict, Dict

class State(TypedDict):
    """√âtat de l'agent pour stocker les informations sur le processus de classification"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
</code></pre>
<p>C'est la <strong>m√©moire de travail de l'agent</strong> ‚Äî tout ce qu'il m√©morise et accumule pendant le processus d'analyse. Semblable √† la fa√ßon dont un expert humain garde le contexte de la t√¢che √† l'esprit lors de l'analyse d'un document.</p>
<p>Examinons le code complet, puis concentrons-nous sur les points principaux.</p>
<pre class="line-numbers"><code class="language-python">import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Cat√©gories de professions
CATEGORIES = [
    "Animateur 2D", "Animateur 3D", "Mod√©lisateur 3D",
    "Analyste commercial", "D√©veloppeur Blockchain", ...
]

class JobType(Enum):
    PROJECT = "travail de projet"
    PERMANENT = "travail permanent"

class SearchType(Enum):
    LOOKING_FOR_WORK = "recherche d'emploi"
    LOOKING_FOR_PERFORMER = "recherche de prestataire"

class State(TypedDict):
    """√âtat de l'agent pour stocker les informations sur le processus de classification"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Agent asynchrone pour la classification des offres d'emploi et des services"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Initialisation de l'agent"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Cr√©e un flux de travail d'agent bas√© sur LangGraph"""
        workflow = StateGraph(State)

        # Ajouter des n≈ìuds au graphe
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)

        # D√©finir la s√©quence d'ex√©cution des n≈ìuds
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)

        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer le type de travail : projet ou permanent"""
        # ... (l'impl√©mentation suit)

    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer la cat√©gorie professionnelle"""
        # ... (l'impl√©mentation suit)

    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer le type de recherche"""
        # ... (l'impl√©mentation suit)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour calculer le niveau de confiance dans la classification"""
        # ... (l'impl√©mentation suit)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Trouve la cat√©gorie la plus proche dans la liste disponible"""
        # ... (l'impl√©mentation suit)

    async def classify(self, description: str) -> Dict[str, Any]:
        """M√©thode principale pour classer les offres d'emploi/services"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }

        # Ex√©cuter le flux de travail
        result = await self.workflow.ainvoke(initial_state)

        # Former la r√©ponse finale au format JSON
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """D√©monstration du fonctionnement de l'agent"""
    agent = VacancyClassificationAgent()

    test_cases = [
        "Recherche d√©veloppeur Python pour cr√©er une application web sur Django. Travail permanent.",
        "Cherche commandes pour cr√©er des logos et une identit√© visuelle. Je travaille avec Adobe Illustrator.",
        "Besoin d'un animateur 3D pour un projet √† court terme de cr√©ation d'une publicit√©.",
        "CV : marketeur exp√©riment√©, cherche travail √† distance dans le marketing digital",
        "Recherche d√©veloppeur frontend React pour notre √©quipe √† temps plein"
    ]

    print("ü§ñ D√©monstration du fonctionnement de l'agent de classification des offres d'emploi\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Test {i} : ---")
        print(f"Description : {description}")
        try:
            result = await agent.classify(description)
            print("R√©sultat de la classification :")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå Erreur : {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

</code></pre>
<p>*(...le reste du code avec l'impl√©mentation des m√©thodes a √©t√© pr√©sent√© dans l'article...)*</p>
<h3>Avantages cl√©s de l'architecture</h3>
<ol>
<li><strong>Modularit√©</strong> ‚Äî chaque n≈ìud r√©sout une t√¢che, facile √† tester et √† am√©liorer s√©par√©ment</li>
<li><strong>Extensibilit√©</strong> ‚Äî de nouvelles fonctionnalit√©s sont ajout√©es de mani√®re d√©clarative</li>
<li><strong>Transparence</strong> ‚Äî l'ensemble du processus de prise de d√©cision est document√© et tra√ßable</li>
<li><strong>Performance</strong> ‚Äî traitement asynchrone de plusieurs requ√™tes</li>
<li><strong>Fiabilit√©</strong> ‚Äî m√©canismes de secours int√©gr√©s et gestion des erreurs</li>
</ol>
<h3>B√©n√©fices r√©els</h3>
<p>Un tel agent peut √™tre utilis√© dans :</p>
<ul>
<li><strong>Les plateformes RH</strong> pour la cat√©gorisation automatique des CV et des offres d'emploi</li>
<li><strong>Les bourses de freelances</strong> pour am√©liorer la recherche et les recommandations</li>
<li><strong>Les syst√®mes internes</strong> des entreprises pour le traitement des demandes et des projets</li>
<li><strong>Les solutions analytiques</strong> pour l'√©tude du march√© du travail</li>
</ul>
<h3>MCP en action : cr√©ation d'un agent avec syst√®me de fichiers et recherche web</h3>
<p>Apr√®s avoir abord√© les principes de base de LangGraph et cr√©√© un agent classificateur simple, √©tendons ses capacit√©s en le connectant au monde ext√©rieur via le MCP.</p>
<p>Nous allons maintenant cr√©er un assistant IA √† part enti√®re qui pourra :</p>
<ul>
<li>Travailler avec le syst√®me de fichiers (lire, cr√©er, modifier des fichiers)</li>
<li>Rechercher des informations pertinentes sur Internet</li>
<li>M√©moriser le contexte du dialogue</li>
<li>G√©rer les erreurs et r√©cup√©rer apr√®s des pannes</li>
</ul>
<h4>De la th√©orie aux outils r√©els</h4>
<p>Vous vous souvenez comment, au d√©but de l'article, nous avons parl√© du fait que <strong>le MCP est un pont entre un r√©seau neuronal et son environnement</strong> ? Vous allez maintenant le voir en pratique. Notre agent aura acc√®s √† des <strong>outils r√©els</strong> :</p>
<pre class="line-numbers"><code class="language-text"># Outils du syst√®me de fichiers
- read_file ‚Äî lecture de fichiers
- write_file ‚Äî √©criture et cr√©ation de fichiers
- list_directory ‚Äî affichage du contenu des dossiers
- create_directory ‚Äî cr√©ation de dossiers

# Outils de recherche web
- brave_web_search ‚Äî recherche sur Internet
- get_web_content ‚Äî obtention du contenu des pages
</code></pre>
<p>Ce n'est plus un agent ¬´jouet¬ª ‚Äî c'est un <strong>outil de travail</strong> qui peut r√©soudre des probl√®mes r√©els.</p>
<h4>üìà Architecture : du simple au complexe</h4>
<p><strong>1. La configuration comme base de la stabilit√©</strong></p>
<pre class="line-numbers"><code class="language-python">from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Configuration simplifi√©e de l'agent IA"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Validation de la configuration"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Le chemin n'existe pas : {self.filesystem_path}")
</code></pre>
<p><strong>Pourquoi est-ce important ?</strong> Contrairement √† l'exemple de classification, ici l'agent interagit avec des syst√®mes externes. Une erreur dans le chemin du fichier ou une cl√© API manquante ‚Äî et tout l'agent cesse de fonctionner. La <strong>validation au d√©marrage</strong> permet d'√©conomiser des heures de d√©bogage.</p>
<p><strong>2. Fabrique de mod√®les : flexibilit√© du choix</strong></p>
<pre class="line-numbers"><code class="language-python">def create_model(config: AgentConfig):
    """Cr√©e un mod√®le selon la configuration"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... autres fournisseurs
</code></pre>
<p>Un seul code ‚Äî de nombreux mod√®les. Vous voulez un mod√®le local gratuit ? Utilisez Ollama. Vous avez besoin d'une pr√©cision maximale ? Passez √† GPT-4. Vous avez besoin de vitesse ? Essayez DeepSeek. Le code reste le m√™me.</p>
<p><strong>3. Int√©gration MCP : connexion au monde r√©el</strong></p>
<pre class="line-numbers"><code class="language-python">async def _init_mcp_client(self):
    """Initialisation du client MCP"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
</code></pre>
<p>Ici, le travail cl√© du MCP a lieu : nous connectons des serveurs MCP externes √† l'agent, qui fournissent un ensemble d'outils et de fonctions. L'agent, √† son tour, re√ßoit non seulement des fonctions individuelles, mais une compr√©hension contextuelle compl√®te de la fa√ßon de travailler avec le syst√®me de fichiers et Internet.</p>
<h4>R√©silience aux erreurs</h4>
<p>Dans le monde r√©el, tout tombe en panne : le r√©seau est indisponible, les fichiers sont bloqu√©s, les cl√©s API sont expir√©es. Notre agent est pr√™t pour cela :</p>
<pre class="line-numbers"><code class="language-python">@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
</code></pre>
<p>Le d√©corateur <code>@retry_on_failure</code> r√©essaie automatiquement les op√©rations en cas de pannes temporaires. L'utilisateur ne remarquera m√™me pas que quelque chose s'est mal pass√©.</p>
<h3>Bilan : de la th√©orie √† la pratique des agents IA</h3>
<p>Aujourd'hui, nous avons parcouru un long chemin, des concepts de base √† la cr√©ation d'agents IA fonctionnels. R√©sumons ce que nous avons appris et r√©alis√©.</p>
<p><strong>Ce que nous avons ma√Ætris√©</strong></p>
<p><strong>1. Concepts fondamentaux</strong></p>
<ul>
<li>Compris la diff√©rence entre les chatbots et les v√©ritables agents IA</li>
<li>Compris le r√¥le du <strong>MCP (Model Context Protocol)</strong> comme pont entre le mod√®le et le monde ext√©rieur</li>
<li>√âtudi√© l'architecture de <strong>LangGraph</strong> pour la construction d'une logique d'agent complexe</li>
</ul>
<p><strong>2. Comp√©tences pratiques</strong></p>
<ul>
<li>Mis en place un environnement de travail avec prise en charge des mod√®les cloud et locaux</li>
<li>Cr√©√© un <strong>agent classificateur</strong> avec une architecture asynchrone et une gestion d'√©tat</li>
<li>Construit un <strong>agent MCP</strong> avec acc√®s au syst√®me de fichiers et √† la recherche web</li>
</ul>
<p><strong>3. Mod√®les architecturaux</strong></p>
<ul>
<li>Ma√Ætris√© la configuration modulaire et les fabriques de mod√®les</li>
<li>Impl√©ment√© la gestion des erreurs et les <strong>m√©canismes de r√©essai</strong> pour des solutions pr√™tes pour la production</li>
</ul>
<h3>Avantages cl√©s de l'approche</h3>
<p><strong>LangGraph + MCP</strong> nous offrent :</p>
<ul>
<li><strong>Transparence</strong> ‚Äî chaque √©tape de l'agent est document√©e et tra√ßable</li>
<li><strong>Extensibilit√©</strong> ‚Äî de nouvelles fonctionnalit√©s sont ajout√©es de mani√®re d√©clarative</li>
<li><strong>Fiabilit√©</strong> ‚Äî gestion des erreurs et r√©cup√©ration int√©gr√©es</li>
<li><strong>Flexibilit√©</strong> ‚Äî prise en charge de plusieurs mod√®les et fournisseurs pr√™ts √† l'emploi</li>
</ul>
<h3>Conclusion</h3>
<p>Les agents IA ne sont pas une fantaisie futuriste, mais une <strong>technologie r√©elle d'aujourd'hui</strong>. Avec LangGraph et MCP, nous pouvons cr√©er des syst√®mes qui r√©solvent des probl√®mes commerciaux sp√©cifiques, automatisent les routines et ouvrent de nouvelles possibilit√©s.</p>
<p><strong>L'essentiel est de commencer.</strong> Prenez le code des exemples, adaptez-le √† vos t√¢ches, exp√©rimentez. Chaque projet est une nouvelle exp√©rience et un pas vers la ma√Ætrise dans le domaine du d√©veloppement IA.</p>
<p>Bonne chance avec vos projets !</p>
<hr>
<p><em>Tags : python, ia, mcp, langchain, assistant ia, ollama, agents ia, llm local, langgraph, mcp-server</em><br>
<em>Hubs : Blog de la soci√©t√© Amvera, Traitement du langage naturel, Intelligence artificielle, Python, Programmation</em><br>
<img src="https://habr.com/ru/companies/amvera/articles/929568/" alt="habr"></p>
