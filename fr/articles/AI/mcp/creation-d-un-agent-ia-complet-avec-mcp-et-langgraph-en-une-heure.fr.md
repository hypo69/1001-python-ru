# Comment apprendre √† un r√©seau neuronal √† travailler de ses mains : cr√©ation d'un agent IA complet avec MCP et LangGraph en une heure


Amis, salutations ! J'esp√®re que vous m'avez manqu√©.

Ces deux derniers mois, je me suis plong√© dans la recherche sur l'int√©gration des agents IA dans mes propres projets Python. Au cours de ce processus, j'ai accumul√© de nombreuses connaissances pratiques et observations qu'il serait dommage de ne pas partager. C'est pourquoi je reviens aujourd'hui sur Habr, avec un nouveau sujet, un regard neuf et l'intention d'√©crire plus souvent.

√Ä l'ordre du jour : LangGraph et MCP : des outils avec lesquels vous pouvez cr√©er des agents IA vraiment utiles.

Si auparavant nous discutions de la meilleure fa√ßon pour un r√©seau neuronal de r√©pondre en russe, aujourd'hui le champ de bataille s'est d√©plac√© vers des t√¢ches plus appliqu√©es : qui g√®re le mieux le r√¥le d'un agent IA ? Quels frameworks simplifient r√©ellement le d√©veloppement ? Et comment int√©grer tout cela dans un projet r√©el ?

Mais avant de plonger dans la pratique et le code, comprenons les concepts de base. Surtout deux cl√©s : **les agents IA et le MCP**. Sans eux, la conversation sur LangGraph serait incompl√®te.

### Les agents IA en termes simples

Les agents IA ne sont pas de simples chatbots "am√©lior√©s". Ce sont des entit√©s plus complexes et autonomes qui poss√®dent deux caract√©ristiques cruciales :

1.  **Capacit√© d'interagir et de se coordonner**

    Les agents modernes peuvent d√©composer les t√¢ches en sous-t√¢ches, appeler d'autres agents, demander des donn√©es externes et travailler en √©quipe. Ce n'est plus un assistant solitaire, mais un syst√®me distribu√© o√π chaque composant peut apporter sa contribution.

2.  **Acc√®s aux ressources externes**

    Un agent IA n'est plus limit√© par les contraintes d'un dialogue. Il peut acc√©der √† des bases de donn√©es, effectuer des appels API, interagir avec des fichiers locaux, des bases de connaissances vectorielles et m√™me ex√©cuter des commandes dans le terminal. Tout cela est devenu possible gr√¢ce √† l'√©mergence du MCP, un nouveau niveau d'int√©gration entre le mod√®le et l'environnement.

---

Pour le dire simplement : **le MCP est un pont entre un r√©seau neuronal et son environnement**. Il permet au mod√®le de ¬´ comprendre ¬ª le contexte de la t√¢che, d'acc√©der aux donn√©es, d'effectuer des appels et de former des actions raisonn√©es, plut√¥t que de simplement √©mettre des r√©ponses textuelles.

**Imaginons une analogie :**

*   Vous avez un **r√©seau neuronal** : il sait raisonner et g√©n√©rer des textes.
*   Il y a des **donn√©es et des outils** : documents, API, bases de connaissances, terminal, code.
*   Et il y a le **MCP** : c'est une interface qui permet au mod√®le d'interagir avec ces sources externes comme si elles faisaient partie de son monde interne.

**Sans MCP :**

Le mod√®le est un moteur de dialogue isol√©. Vous lui donnez du texte, et il r√©pond. Et c'est tout.

**Avec MCP :**

Le mod√®le devient un **ex√©cuteur de t√¢ches** √† part enti√®re :

*   obtient l'acc√®s aux structures de donn√©es et aux API ;
*   appelle des fonctions externes ;
*   s'oriente dans l'√©tat actuel du projet ou de l'application ;
*   peut m√©moriser, suivre et modifier le contexte au fur et √† mesure du dialogue ;
*   utilise des extensions telles que des outils de recherche, des ex√©cuteurs de code, des bases de donn√©es d'embeddings vectoriels, etc.

Au sens technique, **le MCP est un protocole d'interaction entre le LLM et son environnement**, o√π le contexte est fourni sous forme d'objets structur√©s (au lieu de texte ¬´ brut ¬ª), et les appels sont formalis√©s comme des op√©rations interactives (par exemple, l'appel de fonctions, l'utilisation d'outils ou les actions d'agent). C'est ce qui transforme un mod√®le ordinaire en un **v√©ritable agent IA**, capable de faire plus que simplement ¬´ parler ¬ª.

### Et maintenant, au travail !

Maintenant que nous avons abord√© les concepts de base, il est logique de se poser la question : ¬´ Comment mettre tout cela en pratique en Python ? ¬ª

C'est l√† qu'intervient **LangGraph** ‚Äì un framework puissant pour la construction de graphes d'√©tats, de comportements d'agents et de cha√Ænes de pens√©e. Il permet de ¬´ coudre ¬ª la logique d'interaction entre les agents, les outils et l'utilisateur, cr√©ant une architecture IA vivante qui s'adapte aux t√¢ches.

Dans les sections suivantes, nous verrons comment :

*   construire un agent √† partir de z√©ro ;
*   cr√©er des √©tats, des transitions et des √©v√©nements ;
*   int√©grer des fonctions et des outils ;
*   et comment tout cet √©cosyst√®me fonctionne dans un projet r√©el.

### Un peu de th√©orie : qu'est-ce que LangGraph ?

Avant de passer √† la pratique, il faut dire quelques mots sur le framework lui-m√™me.

**LangGraph** est un projet de l'√©quipe **LangChain**, ceux-l√† m√™mes qui ont les premiers propos√© le concept de ¬´ cha√Ænes ¬ª (chains) d'interaction avec les LLM. Si auparavant l'accent √©tait mis sur les pipelines lin√©aires ou √† embranchements conditionnels (langchain.chains), les d√©veloppeurs misent d√©sormais sur un **mod√®le de graphe**, et LangGraph est ce qu'ils recommandent comme le nouveau ¬´ c≈ìur ¬ª pour la construction de syst√®mes IA complexes.

**LangGraph** est un framework pour la construction de machines √† √©tats finis et de graphes d'√©tats, o√π chaque **n≈ìud** repr√©sente une partie de la logique de l'agent : un appel de mod√®le, un outil externe, une condition, une entr√©e utilisateur, etc.

### Comment √ßa marche : graphes et n≈ìuds

Conceptuellement, LangGraph est construit sur les id√©es suivantes :

*   **Graphe** : c'est une structure qui d√©crit les chemins d'ex√©cution possibles de la logique. On peut le consid√©rer comme une carte : d'un point, on peut passer √† un autre en fonction des conditions ou du r√©sultat de l'ex√©cution.
*   **N≈ìuds** : ce sont des √©tapes sp√©cifiques au sein du graphe. Chaque n≈ìud ex√©cute une fonction : appelle un mod√®le, appelle une API externe, v√©rifie une condition ou met simplement √† jour l'√©tat interne.
*   **Transitions entre les n≈ìuds** : c'est la logique de routage : si le r√©sultat de l'√©tape pr√©c√©dente est tel, alors on va l√†.
*   **√âtat** : il est transmis entre les n≈ìuds et accumule tout ce qui est n√©cessaire : historique, conclusions interm√©diaires, entr√©e utilisateur, r√©sultat du travail des outils, etc.

Ainsi, nous obtenons un **m√©canisme flexible de contr√¥le de la logique de l'agent**, dans lequel des sc√©narios simples et tr√®s complexes peuvent √™tre d√©crits : boucles, conditions, actions parall√®les, appels imbriqu√©s et bien plus encore.

### Pourquoi est-ce pratique ?

LangGraph permet de construire une **logique transparente, reproductible et extensible** :

*   facile √† d√©boguer ;
*   facile √† visualiser ;
*   facile √† adapter √† de nouvelles t√¢ches ;
*   facile √† int√©grer des outils externes et des protocoles MCP.

En substance, LangGraph est le **¬´ cerveau ¬ª de l'agent**, o√π chaque √©tape est document√©e, contr√¥lable et peut √™tre modifi√©e sans chaos ni ¬´ magie ¬ª.

### Bon, √ßa suffit la th√©orie !

On pourrait encore longtemps parler des graphes, des √©tats, de la composition logique et des avantages de LangGraph par rapport aux pipelines classiques. Mais, comme le montre la pratique, il vaut mieux le voir une fois dans le code.

**Il est temps de passer √† la pratique.** Ensuite, un exemple en Python : nous allons cr√©er un agent IA simple mais utile bas√© sur LangGraph qui utilisera des outils externes, la m√©moire et prendra des d√©cisions par lui-m√™me.

### Pr√©paration : r√©seaux neuronaux cloud et locaux

Pour commencer √† cr√©er des agents IA, nous avons d'abord besoin d'un **cerveau** ‚Äì un mod√®le linguistique. Il existe deux approches ici :

*   **utiliser des solutions cloud**, o√π tout est pr√™t ¬´ cl√© en main ¬ª ;
*   ou **monter le mod√®le localement** ‚Äì pour une autonomie et une confidentialit√© compl√®tes.

Examinons les deux options.

#### Services cloud : rapides et pratiques

Le moyen le plus simple est d'utiliser la puissance des grands fournisseurs : OpenAI, Anthropic, et d'utiliser...

### O√π obtenir les cl√©s et les jetons :

*   **OpenAI** ‚Äì ChatGPT et autres produits ;
*   **Anthropic** ‚Äì Claude ;
*   **OpenRouter.ai** ‚Äì des dizaines de mod√®les (un jeton ‚Äì de nombreux mod√®les via une API compatible OpenAI) ;
*   **Amvera Cloud** ‚Äì la possibilit√© de connecter LLAMA avec un paiement en roubles et un proxy int√©gr√© vers OpenAI et Anthropic.

Ce chemin est pratique, surtout si vous :

*   ne voulez pas configurer l'infrastructure ;
*   d√©veloppez en mettant l'accent sur la vitesse ;
*   travaillez avec des ressources limit√©es.

### Mod√®les locaux : contr√¥le total

Si la **confidentialit√©, le travail hors ligne** sont importants pour vous, ou si vous souhaitez cr√©er des **agents enti√®rement autonomes**, il est judicieux de d√©ployer le r√©seau neuronal localement.

**Principaux avantages :**

*   **Confidentialit√©** ‚Äì les donn√©es restent chez vous ;
*   **Travail hors ligne** ‚Äì utile dans les r√©seaux isol√©s ;
*   **Pas d'abonnements ni de jetons** ‚Äì gratuit apr√®s la configuration.

**Les inconv√©nients sont √©vidents :**

*   Exigences en mati√®re de ressources (en particulier pour la m√©moire vid√©o) ;
*   La configuration peut prendre du temps ;
*   Certains mod√®les sont difficiles √† d√©ployer sans exp√©rience.

N√©anmoins, il existe des outils qui facilitent le lancement local. L'un des meilleurs aujourd'hui est **Ollama**.

### D√©ploiement de LLM local via Ollama + Docker

Nous allons pr√©parer un lancement local du mod√®le Qwen 2.5 (qwen2.5:32b) √† l'aide d'un conteneur Docker et du syst√®me Ollama. Cela permettra d'int√©grer le r√©seau neuronal avec le MCP et de l'utiliser dans vos propres agents bas√©s sur LangGraph.

Si les ressources informatiques de votre ordinateur ou de votre serveur sont insuffisantes pour travailler avec cette version du mod√®le, vous pouvez toujours choisir un r√©seau neuronal moins ¬´ gourmand en ressources ¬ª ‚Äì le processus d'installation et de lancement restera similaire.

**Installation rapide (r√©sum√© des √©tapes)**

1.  **Installer Docker + Docker Compose**
2.  **Cr√©er la structure du projet :**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **Cr√©er `docker-compose.yml`**
(option universelle, le GPU est d√©tect√© automatiquement)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **D√©marrer le conteneur :**
```bash
docker compose up -d
```

5.  **T√©l√©charger le mod√®le :**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **V√©rifier le fonctionnement via l'API :**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Bonjour !", "stream": false}'
```
*(Image avec le r√©sultat de l'ex√©cution de la commande curl)*

7.  **Int√©gration avec Python :**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Expliquez l'intrication quantique"))
```
Vous disposez maintenant d'un LLM local complet, pr√™t √† fonctionner avec MCP et LangGraph.

**Et ensuite ?**

Nous avons le choix entre les mod√®les cloud et locaux, et nous avons appris √† connecter les deux. Le plus int√©ressant est √† venir : **cr√©er des agents IA sur LangGraph**, qui utilisent le mod√®le s√©lectionn√©, la m√©moire, les outils et leur propre logique.

**Passons √† la partie la plus savoureuse : le code et la pratique !**

---

Avant de passer √† la pratique, il est important de pr√©parer l'environnement de travail. Je suppose que vous √™tes d√©j√† familiaris√© avec les bases de Python, que vous savez ce que sont les biblioth√®ques et les d√©pendances, et que vous comprenez pourquoi utiliser un environnement virtuel.

Si tout cela est nouveau pour vous, je vous recommande de suivre d'abord un cours court ou un guide sur les bases de Python, puis de revenir √† l'article.

#### √âtape 1 : Cr√©ation d'un environnement virtuel

Cr√©ez un nouvel environnement virtuel dans le dossier du projet :
```bash
python -m venv venv
source venv/bin/activate  # pour Linux/macOS
virtualenv\Scripts\activate   # pour Windows
```

#### √âtape 2 : Installation des d√©pendances

Cr√©ez un fichier `requirements.txt` et ajoutez-y les lignes suivantes :
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ‚ö†Ô∏è **Les versions actuelles sont indiqu√©es au 21 juillet 2025.** Elles peuvent avoir chang√© depuis la publication ‚Äì **v√©rifiez la pertinence avant l'installation.**

Ensuite, installez les d√©pendances :
```bash
pip install -r requirements.txt```

#### √âtape 3 : Configuration des variables d'environnement

Cr√©ez un fichier `.env` √† la racine du projet et ajoutez-y les cl√©s API n√©cessaires :
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Objectif des variables :**

*   **OPENAI_API_KEY** ‚Äì cl√© pour acc√©der aux mod√®les GPT d'OpenAI ;
*   **DEEPSEEK_API_KEY** ‚Äì cl√© pour utiliser les mod√®les Deepseek ;
*   **OPENROUTER_API_KEY** ‚Äì cl√© unique pour acc√©der √† de nombreux mod√®les via OpenRouter

---

Certains outils MCP (par exemple, `brave-web-search`) n√©cessitent une cl√© pour fonctionner. Sans elle, ils ne s'activeront tout simplement pas.

**Et si vous n'avez pas de cl√©s API ?**

Pas de probl√®me. Vous pouvez commencer le d√©veloppement avec un mod√®le local (par exemple, via Ollama), sans connecter aucun service externe. Dans ce cas, vous n'avez pas du tout besoin de cr√©er le fichier `.env`.

C'est fait ! Nous avons maintenant tout ce dont nous avons besoin pour commencer ‚Äì un environnement isol√©, des d√©pendances et, si n√©cessaire, l'acc√®s aux r√©seaux neuronaux cloud et aux int√©grations MCP.

Ensuite, nous lancerons notre agent LLM de diff√©rentes mani√®res.

### Lancement simple des agents LLM via LangGraph : int√©gration de base

Commen√ßons par le plus simple : comment ¬´ connecter le cerveau ¬ª √† un futur agent. Nous analyserons les m√©thodes de base pour lancer des mod√®les de langage (LLM) √† l'aide de LangChain, afin qu'√† l'√©tape suivante, nous puissions passer √† l'int√©gration avec LangGraph et √† la construction d'un agent IA complet.

#### Importations
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` et `load_dotenv()` ‚Äì pour charger les variables du fichier `.env`.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` ‚Äì des wrappers pour connecter les mod√®les de langage via LangChain.

> üí° Si vous utilisez des approches alternatives pour travailler avec les configurations (par exemple, Pydantic Settings), vous pouvez remplacer `load_dotenv()` par votre m√©thode habituelle.

#### Chargement des variables d'environnement
```python
load_dotenv()
```
Cela chargera toutes les variables de `.env`, y compris les cl√©s pour acc√©der aux API OpenAI, DeepSeek, OpenRouter et autres.

#### Fonctions simples pour obtenir LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
Si la variable `OPENAI_API_KEY` est correctement d√©finie, LangChain la substituera automatiquement ‚Äì l'indication explicite `api_key=...` est ici facultative.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
De m√™me, mais en utilisant le wrapper `ChatDeepSeek`.

**OpenRouter (et autres API compatibles)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Caract√©ristiques :**

*   `ChatOpenAI` est utilis√©, bien que le mod√®le ne provienne pas d'OpenAI ‚Äì car OpenRouter utilise le m√™me protocole.
*   `base_url` est obligatoire : il pointe vers l'API OpenRouter.
*   Le mod√®le `moonshotai/kimi-k2:free` a √©t√© choisi comme l'une des options les plus √©quilibr√©es en termes de qualit√© et de vitesse au moment de la r√©daction.
*   La cl√© API `OpenRouter` doit √™tre pass√©e explicitement ‚Äì la substitution automatique ne fonctionne pas ici.

#### Mini-test : v√©rification du fonctionnement du mod√®le
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Qui es-tu ?")
    print(response.content)
```
*(Image avec le r√©sultat de l'ex√©cution : `Je suis un assistant IA cr√©√© par Moonshot AI...`)*

Si tout est configur√© correctement, vous recevrez une r√©ponse significative du mod√®le. F√©licitations ‚Äì la premi√®re √©tape est franchie !

### Mais ce n'est pas encore un agent

√Ä l'√©tape actuelle, nous avons connect√© le LLM et effectu√© un simple appel. Cela ressemble plus √† un chatbot de console qu'√† un agent IA.

**Pourquoi ?**

*   Nous √©crivons du **code synchrone et lin√©aire** sans logique d'√©tat ni objectifs.
*   L'agent ne prend pas de d√©cisions, ne m√©morise pas le contexte et n'utilise pas d'outils.
*   Le MCP et LangGraph ne sont pas encore impliqu√©s.

**Et ensuite ?**

Ensuite, nous allons impl√©menter un **agent IA complet** en utilisant **LangGraph** ‚Äì d'abord sans MCP, pour nous concentrer sur l'architecture, les √©tats et la logique de l'agent.

Plongeons dans la v√©ritable m√©canique des agents. C'est parti !

### Agent de classification des emplois : de la th√©orie √† la pratique

...concepts de LangGraph en pratique et cr√©er un outil utile pour les plateformes RH et les bourses de freelances.

#### T√¢che de l'agent

Notre agent prend en entr√©e une description textuelle d'une offre d'emploi ou d'un service et effectue une classification √† trois niveaux :

1.  **Type de travail** : travail de projet ou poste permanent
2.  **Cat√©gorie de profession** : parmi plus de 45 sp√©cialit√©s pr√©d√©finies
3.  **Type de recherche** : si une personne cherche un emploi ou cherche un prestataire

Le r√©sultat est renvoy√© dans un format JSON structur√© avec un score de confiance pour chaque classification.

#### üìà Architecture de l'agent sur LangGraph

Suivant les principes de LangGraph, nous cr√©ons un **graphe d'√©tats** de quatre n≈ìuds :

- Description d'entr√©e
- ‚Üì
- N≈ìud de classification du type de travail
- ‚Üì
- N≈ìud de classification de la cat√©gorie
- ‚Üì
- N≈ìud de d√©termination du type de recherche
- ‚Üì
- N≈ìud de calcul de la confiance
- ‚Üì
- R√©sultat JSON

Chaque n≈ìud est une **fonction sp√©cialis√©e** qui :

*   Re√ßoit l'√©tat actuel de l'agent
*   Effectue sa partie de l'analyse
*   Met √† jour l'√©tat et le transmet

#### Gestion de l'√©tat

Nous d√©finissons la **structure de la m√©moire de l'agent** via `TypedDict` :

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """√âtat de l'agent pour stocker les informations sur le processus de classification"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

C'est la **m√©moire de travail de l'agent** ‚Äì tout ce qu'il m√©morise et accumule pendant l'analyse. Similaire √† la fa√ßon dont un expert humain garde le contexte de la t√¢che √† l'esprit lors de l'analyse d'un document.

Examinons le code complet, puis concentrons-nous sur les points principaux.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Cat√©gories de professions
CATEGORIES = [
    "Animateur 2D", "Animateur 3D", "Modeleur 3D",
    "Analyste commercial", "D√©veloppeur Blockchain", ...
]

class JobType(Enum):
    PROJECT = "travail de projet"
    PERMANENT = "travail permanent"

class SearchType(Enum):
    LOOKING_FOR_WORK = "recherche d'emploi"
    LOOKING_FOR_PERFORMER = "recherche de prestataire"

class State(TypedDict):
    """√âtat de l'agent pour stocker les informations sur le processus de classification"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Agent asynchrone pour la classification des offres d'emploi et des services"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Initialisation de l'agent"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Cr√©e le flux de travail de l'agent bas√© sur LangGraph"""
        workflow = StateGraph(State)
        
        # Ajouter des n≈ìuds au graphe
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # D√©finir la s√©quence d'ex√©cution des n≈ìuds
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer le type de travail : projet ou permanent"""
        # ... (l'impl√©mentation suit)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer la cat√©gorie de profession"""
        # ... (l'impl√©mentation suit)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour d√©terminer le type de recherche"""
        # ... (l'impl√©mentation suit)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """N≈ìud pour calculer le niveau de confiance dans la classification"""
        # ... (l'impl√©mentation suit)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Trouve la cat√©gorie la plus proche dans la liste des cat√©gories disponibles"""
        # ... (l'impl√©mentation suit)

    async def classify(self, description: str) -> Dict[str, Any]:
        """M√©thode principale pour classer les offres d'emploi/services"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Ex√©cuter le flux de travail
        result = await self.workflow.ainvoke(initial_state)
        
        # Former la r√©ponse JSON finale
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """D√©monstration du fonctionnement de l'agent"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "Recherche d√©veloppeur Python pour cr√©er une application web en Django. Poste permanent.",
        "Recherche commandes pour cr√©er des logos et une identit√© d'entreprise. Je travaille avec Adobe Illustrator.",
        "Besoin d'un animateur 3D pour un projet √† court terme de cr√©ation d'une publicit√©.",
        "CV : marketeur exp√©riment√©, recherche travail √† distance dans le domaine du marketing digital",
        "Recherche d√©veloppeur frontend React pour notre √©quipe √† temps plein"
    ]
    
    print("ü§ñ D√©monstration du fonctionnement de l'agent de classification des offres d'emploi\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Test {i} : ---")
        print(f"Description : {description}")
        try:
            result = await agent.classify(description)
            print("R√©sultat de la classification :")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå Erreur : {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...le reste du code avec les impl√©mentations des m√©thodes a √©t√© pr√©sent√© dans l'article...)*

### Avantages cl√©s de l'architecture
1.  **Modularit√©** - chaque n≈ìud r√©sout une t√¢che, facile √† tester et √† am√©liorer s√©par√©ment
2.  **Extensibilit√©** - de nouvelles fonctionnalit√©s sont ajout√©es de mani√®re d√©clarative
3.  **Transparence** - l'ensemble du processus de prise de d√©cision est document√© et tra√ßable
4.  **Performance** - traitement asynchrone de multiples requ√™tes
5.  **Fiabilit√©** - gestion des erreurs et r√©cup√©ration int√©gr√©es

### Avantages r√©els
Un tel agent peut √™tre utilis√© dans :
*   **Les plateformes RH** pour la cat√©gorisation automatique des CV et des offres d'emploi
*   **Les bourses de freelances** pour am√©liorer la recherche et les recommandations
*   **Les syst√®mes internes** des entreprises pour le traitement des demandes et des projets
*   **Les solutions analytiques** pour l'√©tude du march√© du travail

### MCP en action : cr√©ation d'un agent avec un syst√®me de fichiers et une recherche web
Apr√®s avoir abord√© les principes de base de LangGraph et cr√©√© un agent classificateur simple, √©tendons ses capacit√©s en le connectant au monde ext√©rieur via le MCP.

Nous allons maintenant cr√©er un assistant IA complet qui pourra :
*   Travailler avec le syst√®me de fichiers (lire, cr√©er, modifier des fichiers)
*   Rechercher des informations pertinentes sur Internet
*   M√©moriser le contexte du dialogue
*   G√©rer les erreurs et r√©cup√©rer apr√®s des pannes

#### De la th√©orie aux outils r√©els
Vous vous souvenez comment, au d√©but de l'article, nous avons parl√© du fait que **le MCP est un pont entre un r√©seau neuronal et son environnement** ? Vous allez maintenant le voir en pratique. Notre agent aura acc√®s √† des **outils r√©els** :
```
# Outils du syst√®me de fichiers
- read_file ‚Äì lecture de fichiers
- write_file ‚Äì √©criture et cr√©ation de fichiers
- list_directory ‚Äì affichage du contenu des dossiers
- create_directory ‚Äì cr√©ation de dossiers

# Outils de recherche web
- brave_web_search ‚Äì recherche sur Internet
- get_web_content ‚Äì obtention du contenu des pages
```
Ce n'est plus un agent ¬´ jouet ¬ª ‚Äì c'est un **outil de travail** qui peut r√©soudre des probl√®mes r√©els.

#### üìà Architecture : du simple au complexe

**1. Configuration comme base de la stabilit√©**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Configuration simplifi√©e de l'agent IA"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Validation de la configuration"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Le chemin n'existe pas : {self.filesystem_path}")
```
**Pourquoi est-ce important ?** Contrairement √† l'exemple de classification, ici l'agent interagit avec des syst√®mes externes. Une erreur dans le chemin du fichier ou une cl√© API manquante ‚Äì et tout l'agent cesse de fonctionner. La **validation au d√©marrage** permet d'√©conomiser des heures de d√©bogage.

**2. Fabrique de mod√®les : flexibilit√© du choix**
```python
def create_model(config: AgentConfig):
    """Cr√©e un mod√®le selon la configuration"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... autres fournisseurs
```
Un seul code ‚Äì de nombreux mod√®les. Vous voulez un mod√®le local gratuit ? Utilisez Ollama. Besoin d'une pr√©cision maximale ? Passez √† GPT-4. La vitesse est importante ? Essayez DeepSeek. Le code reste le m√™me.

**3. Int√©gration MCP : connexion au monde r√©el**
```python
async def _init_mcp_client(self):
    """Initialisation du client MCP"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
Ici, le travail cl√© du MCP a lieu : nous connectons des serveurs MCP externes √† l'agent, qui fournissent un ensemble d'outils et de fonctions. L'agent, √† son tour, ne re√ßoit pas seulement des fonctions individuelles, mais une compr√©hension contextuelle compl√®te de la fa√ßon de travailler avec le syst√®me de fichiers et Internet.

#### R√©silience aux erreurs
Dans le monde r√©el, tout tombe en panne : le r√©seau est indisponible, les fichiers sont verrouill√©s, les cl√©s API sont expir√©es. Notre agent est pr√™t pour cela :
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
Le d√©corateur `@retry_on_failure` r√©essaie automatiquement les op√©rations en cas de d√©faillances temporaires. L'utilisateur ne remarquera m√™me pas que quelque chose s'est mal pass√©.

### R√©sum√© : de la th√©orie √† la pratique des agents IA

Aujourd'hui, nous avons parcouru le chemin des concepts de base √† la cr√©ation d'agents IA fonctionnels. R√©sumons ce que nous avons appris et r√©alis√©.

**Ce que nous avons ma√Ætris√©**

**1. Concepts fondamentaux**
*   Compris la diff√©rence entre les chatbots et les v√©ritables agents IA
*   Compris le r√¥le du **MCP (Model Context Protocol)** comme pont entre le mod√®le et le monde ext√©rieur
*   √âtudi√© l'architecture de **LangGraph** pour la construction d'une logique d'agent complexe

**2. Comp√©tences pratiques**
*   Configur√© un environnement de travail avec prise en charge des mod√®les cloud et locaux
*   Cr√©√© un **agent classificateur** avec une architecture asynchrone et une gestion d'√©tat
*   Construit un **agent MCP** avec acc√®s au syst√®me de fichiers et √† la recherche web

**3. Mod√®les architecturaux**
*   Ma√Ætris√© la configuration modulaire et les fabriques de mod√®les
*   Impl√©ment√© la gestion des erreurs et les **m√©canismes de r√©essai** pour les solutions pr√™tes pour la production

### Avantages cl√©s de l'approche
**LangGraph + MCP** nous offrent :
*   **Transparence** ‚Äì chaque √©tape de l'agent est document√©e et tra√ßable
*   **Extensibilit√©** ‚Äì de nouvelles fonctionnalit√©s sont ajout√©es de mani√®re d√©clarative
*   **Fiabilit√©** ‚Äì gestion des erreurs et r√©cup√©ration int√©gr√©es
*   **Flexibilit√©** ‚Äì prise en charge de plusieurs mod√®les et fournisseurs pr√™ts √† l'emploi

### Conclusion

Les agents IA ne sont pas une fiction futuriste, mais une **technologie r√©elle d'aujourd'hui**. Avec LangGraph et MCP, nous pouvons cr√©er des syst√®mes qui r√©solvent des probl√®mes commerciaux sp√©cifiques, automatisent les routines et ouvrent de nouvelles possibilit√©s.

**L'essentiel est de commencer.** Prenez le code des exemples, adaptez-le √† vos t√¢ches, exp√©rimentez. Chaque projet est une nouvelle exp√©rience et un pas vers la ma√Ætrise dans le domaine du d√©veloppement IA.

Bonne chance dans vos projets !

---
*Tags : python, ia, mcp, langchain, assistant ia, ollama, agents ia, llm local, langgraph, mcp-server*
*Hubs : Blog de la soci√©t√© Amvera, Traitement du langage naturel, Intelligence artificielle, Python, Programmation*
