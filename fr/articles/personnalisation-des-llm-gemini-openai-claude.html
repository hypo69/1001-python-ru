<h2>Cheat Sheet. Customizing LLMs: Prompts, Model Fine-tuning, Code Examples.</h2>
<p>In this article:</p>
<ol>
<li>How the "memory effect" is created in LLMs (a brief overview).</li>
<li>Why and when model fine-tuning is necessary.</li>
<li>When fine-tuning is not the best solution.</li>
<li>Data preparation.</li>
<li>Fine-tuning examples for <strong>OpenAI (GPT)</strong>, <strong>Google (Gemini)</strong>, and <strong>Anthropic (Claude)</strong> (differs).</li>
</ol>
<h3>1. How LLMs "remember" and "adapt": The illusion of context</h3>
<p>Before talking about fine-tuning, it is important to understand how LLMs manage to create a sense of personalization.
This is important so as not to rush into expensive fine-tuning if the task can be solved more simply:</p>
<ul>
<li>By the <strong>context window (short-term memory):</strong> Within the same dialogue, you send the model not only a new question, but also <strong>all or part of the previous correspondence</strong>. The model processes all this text as a single "context". It is thanks to this that it "remembers" previous remarks and continues the thought. The limitation here is the length of the context window (the number of tokens).</li>
<li>Creation of <strong>system prompts:</strong> You can define the role, tone, and rules of conduct of the model at the beginning of each dialogue. For example: "You are a Python expert, answer briefly".</li>
<li>The inclusion in the query of several examples of the desired behavior <strong>Few-Shot Learning:</strong> (input/output pairs) allows the model to "learn" this pattern directly within the current query.</li>
<li><strong>Application-side state management:</strong> The most powerful way. The application (which accesses the API) can store information about the user (preferences, history, profile data) and dynamically add it to the prompt before sending it to the model.</li>
</ul>
<h3>2.</h3>
<p>Fine-tuning is the process of continuously training an already prepared base LLM on your own specific dataset. This allows the model to:</p>
<ul>
<li><strong>Adapt style and tone:</strong> The model will speak "your language" - whether it's a strict scientific style, friendly marketing, or the jargon of a specific community.</li>
<li><strong>Follow specific instructions and formats:</strong> If you need responses in a strictly defined JSON structure, or always with a specific set of fields.</li>
<li><strong>Understand domain-specific language:</strong> Training on your internal documentation or industry texts will help the model better handle the terminology of your niche.</li>
<li><strong>Improve performance on narrow tasks:</strong> For certain types of queries (e.g., review classification, code generation in a specific framework), fine-tuning can provide more accurate and relevant responses than the base model.</li>
<li><strong>Reduce prompt length:</strong> If the model already "knows" the desired behavior through fine-tuning, you don't need to remind it every time in the prompt, which saves tokens and reduces latency.</li>
</ul>
<h3>3.</h3>
<p>Fine-tuning is a powerful tool, but not universal. You should not use it if:</p>
<ul>
<li><strong>The model needs to access new knowledge:</strong> Fine-tuning modifies the model's weights, but does not "load" new facts in real time. If your task is to answer questions based on an ever-evolving knowledge base (company documents, latest news), it is better to use <strong>Retrieval Augmented Generation (RAG)</strong>. Here, the base model receives the context from your database <em>at the time of the query</em>.</li>
<li><strong>A simple task is solved by prompt engineering:</strong> Always start with the most effective prompt engineering possible. If the task is solved with simple instructions and few-shot examples, fine-tuning is redundant and more expensive.</li>
<li><strong>You do not have enough high-quality data:</strong> Bad data = bad tuned model.</li>
</ul>
<h3>4. Data preparation.</h3>
<p>The quality and quantity of your data are essential. The model learns from your examples, so they must be accurate, varied, and consistent.</p>
<ul>
<li><strong>Format:</strong> Most often JSON Lines (<code>.jsonl</code>) or CSV (<code>.csv</code>).</li>
<li><strong>Data structure:</strong> Depends on the task.
<ul>
<li><strong>Instruction tuning (Instruction-Response):</strong> Suitable for generalized tasks such as "question-answer", paraphrase, summary.
<pre class="line-numbers"><code class="language-json">
        {"input_text": "Paraphrase the sentence: 'AI technology is developing rapidly.'", "output_text": "Artificial intelligence is making rapid progress."}
        {"input_text": "Name the capital of France.", "output_text": "The capital of France is Paris."}
        </code></pre>
</li>
<li><strong>Chat Tuning:</strong> Ideal for training the model to conduct a dialogue in a specific role or style.
<pre class="line-numbers"><code class="language-json">
        {"messages": [{"author": "user", "content": "Hi! What do you recommend for dinner?"}, {"author": "model", "content": "Good evening! Today is a great day for pasta carbonara, or, if you prefer something light, a Caesar salad."}]}
        {"messages": [{"author": "user", "content": "Tell me about the new features of Python 3.12."}, {"author": "model", "content": "Python 3.12 introduced indented f-strings, a new syntax for universal generics, and better Unicode error handling."}]}
        </code></pre>
</li>
</ul>
</li>
<li><strong>Quantity:</strong> A minimum of 100 to 200 high-quality examples, but the more the better (thousands for serious tasks).</li>
<li><strong>Quality:</strong> Accuracy, consistency of style, tone, format throughout the entire dataset.</li>
</ul>
<h3>5. Models and platforms</h3>
<h4>5.1. OpenAI</h4>
<p>OpenAI provides a fairly simple and intuitive API for fine-tuning.</p>
<p><strong>Process:</strong></p>
<ol>
<li>Prepare the data in JSONL format, where each line is an object with a <code>messages</code> array (as in the Chat Completions API).</li>
<li>Upload the data file via the API.</li>
<li>Create a fine-tuning job, specifying the uploaded file and the base model.</li>
<li>Track progress.</li>
<li>Use the new fine-tuned model by its ID.</li>
</ol>
<p><strong>Example data (<code>training_data.jsonl</code> file):</strong></p>
<pre class="line-numbers"><code class="language-json">
{"messages": [{"role": "system", "content": "You are a friendly customer support assistant from an IT company."}, {"role": "user", "content": "My internet is not working."}, {"role": "assistant", "content": "Oh, I'm sorry for the inconvenience! Let's try to understand. How are you connected to the Internet â€“ by Wi-Fi or by cable?"}]}
{"messages": [{"role": "system", "content": "You are a friendly customer support assistant from an IT company."}, {"role": "user", "content": "How can I reset my password?"}, {"role": "assistant", "content": "Of course! To reset your password, please go to the login page and click on 'Forgot password?' below the input field. Then follow the on-screen instructions."}]
</code></pre>
<p><strong>Python code example:</strong></p>
<p>First, install: <code>pip install openai</code></p>
<pre class="line-numbers"><code class="language-python">
import openai
from openai import OpenAI
import os

# Set your OpenAI API key. It is recommended to use an environment variable.
# os.environ["OPENAI_API_KEY"] = "sk-..."
client = OpenAI()

# 1. Uploading the data file
try:
    file_response = client.files.create(
        file=open("training_data.jsonl", "rb"),
        purpose="fine-tune"
    )
    file_id = file_response.id
    print(f"File uploaded successfully. File ID: {file_id}")
except openai.APIStatusError as e:
    print(f"Error uploading file: {e.status_code} - {e.response}")
    exit()

# 2. Creating a fine-tuning job
try:
    ft_job_response = client.fine_tuning.jobs.create(
        training_file=file_id,
        model="gpt-3.5-turbo" # You can specify a specific version, for example, "gpt-3.5-turbo-0125"
    )
    job_id = ft_job_response.id
    print(f"Fine-tuning job created. Job ID: {job_id}")
    print("Track job status via the API or in OpenAI Playground.")
except openai.APIStatusError as e:
    print(f"Error creating job: {e.status_code} - {e.response}")
    exit()

# Example of tracking status and getting the model name (to be run after job creation):
# # job_id = "ftjob-..." # Replace with your job ID
# # job_status = client.fine_tuning.jobs.retrieve(job_id)
# # print(f"Current job status: {job_status.status}")
# # if job_status.status == "succeeded":
# #     fine_tuned_model_name = job_status.fine_tuned_model
# #     print(f"Fine-tuned model name: {fine_tuned_model_name}")

# 3. Using the fine-tuned model (after it's ready)
# # Replace with the actual name of your model, obtained after successful fine-tuning
# # fine_tuned_model_name = "ft:gpt-3.5-turbo-0125:my-org::abcd123"

# # if 'fine_tuned_model_name' in locals() and fine_tuned_model_name:
# #     try:
# #         response = client.chat.completions.create(
# #             model=fine_tuned_model_name,
# #             messages=[
# #                 {"role": "user", "content": "I have a problem with my connection."}
# #             ]
# #         )
# #         print("\nFine-tuned model response:")
# #         print(response.choices[0].message.content)
# #     except openai.APIStatusError as e:
# #         print(f"Error using the model: {e.status_code} - {e.response}")
</code></pre>
<h4>5.2. Anthropic</h4>
<p>Anthropic <strong>does not provide a public API for fine-tuning its Claude 3 models (Opus, Sonnet, Haiku) in the same way as OpenAI or Google.</strong></p>
<p>Anthropic focuses on creating very powerful base models that, they believe, work very well with advanced prompt engineering and RAG models, minimizing the need for fine-tuning in most cases.
For large enterprise clients or partners, there may be programs to create "custom" models or specialized integrations, but this is not a publicly accessible fine-tuning feature via the API.</p>
<p>If you are working with Claude 3, your main focus should be:</p>
<ul>
<li><strong>Quality prompt engineering:</strong> Experiment with system instructions, few-shot examples, and clear query formatting. Claude is known for its ability to strictly follow instructions, especially in XML tags.</li>
<li><strong>RAG systems:</strong> Use external knowledge bases to provide the model with up-to-date context.</li>
</ul>
<h4>5.3. Google (Gemini)</h4>
<p>Google is actively developing fine-tuning capabilities through its <strong>Google Cloud Vertex AI</strong> platform.
This is a comprehensive ML platform that provides tools for data preparation, training job execution, and model deployment.
Fine-tuning is available for the Gemini model family.</p>
<p><strong>Process:</strong></p>
<ol>
<li>Prepare the data (JSONL or CSV) in <code>input_text</code>/<code>output_text</code> format (for instruction tuning) or <code>messages</code> (for chat tuning).</li>
<li>Upload the data to Google Cloud Storage (GCS).</li>
<li>Create and run a fine-tuning job via the Vertex AI console or SDK.</li>
<li>Deploy the fine-tuned model to an endpoint.</li>
<li>Use the fine-tuned model via this endpoint.</li>
</ol>
<p><strong>Example data (<code>gemini_tuning_data.jsonl</code> file):</strong></p>
<pre class="line-numbers"><code class="language-json">
{"input_text": "Summarize the main ideas of this book: 'The book is about a hero's journey, overcoming obstacles and finding himself.'", "output_text": "The main character of the book embarks on a transformative journey, facing difficulties and gaining self-knowledge."}
{"input_text": "Explain the principle of a thermonuclear reactor in simple terms.", "output_text": "A thermonuclear reactor attempts to reproduce the process that occurs on the Sun: the fusion of light atomic nuclei at very high temperatures, releasing an enormous amount of energy."}
</code></pre>
<p><strong>Python code example (requires <code>google-cloud-aiplatform</code>):</strong></p>
<p>First, install: <code>pip install google-cloud-aiplatform</code> and <code>pip install google-cloud-storage</code></p>
<pre class="line-numbers"><code class="language-python">
import os
from google.cloud import aiplatform
from google.cloud import storage

# --- Parameters ---
# REPLACE with your values:
PROJECT_ID = "your-gcp-project-id"
REGION = "us-central1"               # Choose a region that supports Gemini and Vertex AI
BUCKET_NAME = "your-gcs-bucket-for-tuning" # The name of your GCS bucket (must be created beforehand)
DATA_FILE_LOCAL_PATH = "gemini_tuning_data.jsonl"
GCS_DATA_URI = f"gs://{BUCKET_NAME}/{DATA_FILE_LOCAL_PATH}"
TUNED_MODEL_DISPLAY_NAME = "my-tuned-gemini-model"
# --- End of parameters ---

# Initialize Vertex AI
aiplatform.init(project=PROJECT_ID, location=REGION)

# 1. Create a data file (if it doesn't exist)
with open(DATA_FILE_LOCAL_PATH, "w", encoding="utf-8") as f:
    f.write('{"input_text": "Summarize the main ideas of this book: \'The book is about a hero\'s journey, overcoming obstacles and finding himself.\'", "output_text": "The main character of the book embarks on a transformative journey, facing difficulties and gaining self-knowledge."}\n')
    f.write('{"input_text": "Explain the principle of a thermonuclear reactor in simple terms.", "output_text": "A thermonuclear reactor attempts to reproduce the process that occurs on the Sun: the fusion of light atomic nuclei at very high temperatures, releasing an enormous amount of energy."}\n')
print(f"Data file '{DATA_FILE_LOCAL_PATH}' created.")


# 2. Upload data to Google Cloud Storage
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """Uploads a file to the GCS bucket."""
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"File '{source_file_name}' uploaded to 'gs://{bucket_name}/{destination_blob_name}'.")

try:
    upload_blob(BUCKET_NAME, DATA_FILE_LOCAL_PATH, DATA_FILE_LOCAL_PATH)
except Exception as e:
    print(f"Error uploading file to GCS. Make sure the bucket exists and you have permissions: {e}")
    exit()

# 3. Create and run a fine-tuning job
print(f"\nStarting fine-tuning of model '{TUNED_MODEL_DISPLAY_NAME}'...")
try:
    # `tune_model` starts the job and returns the fine-tuned model after completion
    tuned_model = aiplatform.Model.tune_model(
        model_display_name=TUNED_MODEL_DISPLAY_NAME,
        source_model_name="gemini-1.0-pro-001", # Base Gemini Pro model
        training_data_path=GCS_DATA_URI,
        tuning_method="SUPERVISED_TUNING",
        train_steps=100, # Number of training steps. Optimal value depends on data size.
        # batch_size=16, # You can specify
        # learning_rate_multiplier=1.0 # You can specify
    )
    print(f"Model '{TUNED_MODEL_DISPLAY_NAME}' fine-tuned successfully. Model ID: {tuned_model.name}")
    print("The fine-tuning process can take a considerable amount of time.")
except Exception as e:
    print(f"Fine-tuning error. Check logs in Vertex AI console: {e}")
    exit()

# 4. Deploy the fine-tuned model (for use)
print(f"\nDeploying fine-tuned model '{TUNED_MODEL_DISPLAY_NAME}' to an endpoint...")
try:
    endpoint = tuned_model.deploy(
        machine_type="n1-standard-4", # Machine type for the endpoint. Choose an appropriate one.
        min_replica_count=1,
        max_replica_count=1
    )
    print(f"Model deployed to endpoint: {endpoint.name}")
    print("Deployment can also take several minutes.")
except Exception as e:
    print(f"Error deploying model: {e}")
    exit()

# 5. Use the fine-tuned model
print("\nTesting the fine-tuned model...")
prompt = "Tell me about your capabilities after training."
instances = [{"prompt": prompt}] # For instruction tuning. If chat tuning, then {"messages": [...]} 

try:
    response = endpoint.predict(instances=instances)
    print("\nFine-tuned model response:")
    print(response.predictions[0])
except Exception as e:
    print(f"Error using the fine-tuned model: {e}")

# After finishing work, don't forget to delete the endpoint and model to avoid unnecessary costs:
# endpoint.delete()
# tuned_model.delete()
</code></pre>
<h3>6. General recommendations</h3>
<ul>
<li><strong>Start small:</strong> Don't try to immediately train a model on thousands of examples. Start with a small but high-quality dataset.</li>
<li><strong>Iterate:</strong> Fine-tuning is an iterative process. Train, evaluate, adjust data or hyperparameters, repeat.</li>
<li><strong>Monitoring:</strong> Carefully monitor training metrics (loss) and use a validation dataset to avoid overfitting.</li>
<li><strong>Evaluation:</strong> Always test the fine-tuned model on data it has <em>never seen</em> during training to assess its generalization capability.</li>
<li><strong>Cost:</strong> Remember that fine-tuning and deploying endpoints are paid services. Take this into account in your budget.</li>
<li><strong>Documentation:</strong> Always consult the official LLM provider documentation. APIs and features are constantly evolving.</li>
</ul>
