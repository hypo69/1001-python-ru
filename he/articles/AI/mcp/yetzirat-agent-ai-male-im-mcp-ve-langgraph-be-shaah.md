# כיצד ללמד רשת עצבית לעבוד עם ידיה: יצירת סוכן AI מלא עם MCP ו-LangGraph בשעה

חברים, שלום! אני מקווה שהתגעגעתם.

במהלך החודשיים האחרונים, צללתי עמוק לחקר שילוב סוכני AI בפרויקטי הפייתון שלי. בתהליך, צברתי ידע מעשי ותצפיות רבות שפשוט חבל לא לשתף. אז היום אני חוזר ל-Habr – עם נושא חדש, נקודת מבט רעננה ובכוונה לכתוב לעיתים קרובות יותר.

על הפרק – LangGraph ו-MCP: כלים שבאמצעותם ניתן ליצור סוכני AI שימושיים באמת.

אם בעבר התווכחנו איזו רשת עצבית עונה טוב יותר ברוסית, הרי שהיום שדה הקרב עבר למשימות יישומיות יותר: מי מתמודד טוב יותר עם תפקיד סוכן AI? אילו פריימוורקים באמת מפשטים את הפיתוח? וכיצד לשלב את כל הטוב הזה בפרויקט אמיתי?

אבל לפני שנצלול לפרקטיקה ולקוד, בואו נבין את המושגים הבסיסיים. במיוחד שניים מרכזיים: **סוכני AI ו-MCP**. בלעדיהם, השיחה על LangGraph תהיה חסרה.

### סוכני AI במילים פשוטות

סוכני AI הם לא רק צ'אטבוטים "משודרגים". הם מייצגים ישויות מורכבות ואוטונומיות יותר, בעלות שתי תכונות חשובות ביותר:

1.  **יכולת אינטראקציה ותיאום**

    סוכנים מודרניים מסוגלים לפרק משימות לתת-משימות, לקרוא לסוכנים אחרים, לבקש נתונים חיצוניים, לעבוד בצוות. זו כבר לא עוזרת בודדת, אלא מערכת מבוזרת שבה כל רכיב יכול לתרום.

2.  **גישה למשאבים חיצוניים**

    סוכן AI אינו מוגבל עוד לגבולות דיאלוג. הוא יכול לגשת למסדי נתונים, לבצע קריאות API, ליצור אינטראקציה עם קבצים מקומיים, מאגרי ידע וקטוריים ואף להריץ פקודות בטרמינל. כל זה התאפשר הודות להופעת MCP – רמה חדשה של אינטגרציה בין המודל לסביבה.

---

במילים פשוטות: **MCP הוא גשר בין רשת עצבית לסביבתה**. הוא מאפשר למודל "להבין" את הקשר המשימה, לגשת לנתונים, לבצע קריאות ולגבש פעולות מנומקות, במקום פשוט להוציא תגובות טקסטואליות.

**בואו נדמיין אנלוגיה:**

*   יש לך **רשת עצבית** — היא יודעת להסיק מסקנות ולייצר טקסטים.
*   יש **נתונים וכלים** — מסמכים, API, מאגרי ידע, טרמינל, קוד.
*   ויש **MCP** — זהו ממשק המאפשר למודל ליצור אינטראקציה עם מקורות חיצוניים אלה כאילו היו חלק מעולמו הפנימי.

**ללא MCP:**

מודל — הוא מנוע דיאלוג מבודד. אתה מזין לו טקסט — הוא מגיב. וזהו.

**עם MCP:**

המודל הופך ל**מבצע משימות** מלא:

*   מקבל גישה למבני נתונים ו-API;
*   קורא לפונקציות חיצוניות;
*   מתמצא במצב הנוכחי של הפרויקט או היישום;
*   יכול לזכור, לעקוב ולשנות הקשר ככל שהדיאלוג מתקדם;
*   משתמש בהרחבות כגון כלי חיפוש, מריצי קוד, מסדי נתונים של הטמעות וקטוריות וכו'.

במובן הטכני, **MCP הוא פרוטוקול אינטראקציה בין LLM לסביבתה**, כאשר ההקשר מסופק כאובייקטים מובנים (במקום טקסט "גולמי"), והקריאות מעוצבות כפעולות אינטראקטיביות (לדוגמה, קריאת פונקציה, שימוש בכלי או פעולות סוכן). זה מה שהופך מודל רגיל ל**סוכן AI אמיתי**, המסוגל לעשות יותר מסתם "לדבר".

### ועכשיו — לעבודה!

כעת, לאחר שהבנו את המושגים הבסיסיים, הגיוני לשאול: "כיצד ליישם את כל זה בפועל בפייתון?"

כאן נכנס לתמונה **LangGraph** — פריימוורק רב עוצמה לבניית גרפי מצבים, התנהגויות סוכנים ושרשרות חשיבה. הוא מאפשר "לתפור" את לוגיקת האינטראקציה בין סוכנים, כלים ומשתמש, וליצור ארכיטקטורת AI חיה המותאמת למשימות.

בסעיפים הבאים נראה כיצד:

*   בונים סוכן מאפס;
*   יוצרים מצבים, מעברים ואירועים;
*   משלבים פונקציות וכלים;
*   וכיצד כל המערכת האקולוגית הזו פועלת בפרויקט אמיתי.

### קצת תיאוריה: מה זה LangGraph

לפני שנעבור לפרקטיקה, כמה מילים על הפריימוורק עצמו.

**LangGraph** הוא פרויקט של צוות **LangChain**, אותם אלה שהציעו לראשונה את הרעיון של "שרשרות" (chains) אינטראקציה עם LLM. אם בעבר הדגש העיקרי היה על צינורות ליניאריים או מסועפים מותנים (langchain.chains), הרי שכעת המפתחים שמים דגש על **מודל גרפי**, ו-LangGraph הוא מה שהם ממליצים כ"ליבה" החדשה לבניית מערכות AI מורכבות.

**LangGraph** הוא פריימוורק לבניית מכונות מצבים סופיות וגרפי מצבים, שבהם כל **צומת** מייצג חלק מלוגיקת הסוכן: קריאה למודל, כלי חיצוני, תנאי, קלט משתמש וכו'.

### איך זה עובד: גרפים וצמתים

מבחינה קונספטואלית, LangGraph בנוי על הרעיונות הבאים:

*   **גרף** — הוא מבנה המתאר את נתיבי הביצוע האפשריים של הלוגיקה. ניתן לחשוב עליו כמפה: מנקודה אחת ניתן לעבור לאחרת בהתאם לתנאים או לתוצאת הביצוע.
*   **צמתים** — הם שלבים ספציפיים בתוך הגרף. כל צומת מבצע פונקציה כלשהי: קורא למודל, קורא ל-API חיצוני, בודק תנאי או פשוט מעדכן את המצב הפנימי.
*   **מעברים בין צמתים** — זוהי לוגיקת הניתוב: אם תוצאת השלב הקודם היא כזו וכזו, אז לך לשם.
*   **מצב** — מועבר בין צמתים ואוגר את כל מה שצריך: היסטוריה, מסקנות ביניים, קלט משתמש, תוצאות פעולות כלים וכו'.

לפיכך, אנו מקבלים **מנגנון גמיש לשליטה בלוגיקת הסוכן**, שבו ניתן לתאר תרחישים פשוטים ומורכבים מאוד: לולאות, תנאים, פעולות מקבילות, קריאות מקוננות ועוד.

### למה זה נוח?

LangGraph מאפשר לבנות **לוגיקה שקופה, ניתנת לשחזור וניתנת להרחבה**:

*   קל לניפוי באגים;
*   קל להדמיה;
*   קל להתאמה למשימות חדשות;
*   קל לשלב כלים חיצוניים ופרוטוקולי MCP.

בעצם, LangGraph הוא **"המוח" של הסוכן**, שבו כל שלב מתועד, ניתן לשליטה וניתן לשינוי ללא כאוס ו"קסם".

### ועכשיו — מספיק תיאוריה!

אפשר עוד הרבה לדבר על גרפים, מצבים, הרכבת לוגיקה ויתרונות LangGraph על פני צינורות קלאסיים. אבל, כפי שמראה הניסיון, עדיף לראות פעם אחת בקוד.

**הגיע הזמן לעבור לפרקטיקה.** בהמשך — דוגמה בפייתון: ניצור סוכן AI פשוט אך שימושי המבוסס על LangGraph שישתמש בכלים חיצוניים, זיכרון ויקבל החלטות בעצמו.

### הכנה: רשתות עצביות בענן ומקומיות

כדי להתחיל ליצור סוכני AI, אנו זקוקים קודם כל ל**מוח** — מודל שפה. ישנן שתי גישות כאן:

*   **להשתמש בפתרונות ענן**, שבהם הכל מוכן "מהקופסה";
*   או **להעלות את המודל באופן מקומי** — לאוטונומיה וסודיות מלאה.

בואו נבחן את שתי האפשרויות.

#### שירותי ענן: מהירים ונוחים

הדרך הקלה ביותר היא להשתמש בכוחם של ספקים גדולים: OpenAI, Anthropic, ולהשתמש...

### היכן להשיג מפתחות ואסימונים:

*   **OpenAI** — ChatGPT ומוצרים אחרים;
*   **Anthropic** — Claude;
*   **OpenRouter.ai** — עשרות מודלים (אסימון אחד — מודלים רבים באמצעות API תואם OpenAI);
*   **Amvera Cloud** — אפשרות לחבר LLAMA עם תשלום ברובלים ופרוקסי מובנה ל-OpenAI ו-Anthropic.

נתיב זה נוח, במיוחד אם אתה:

*   לא רוצה להגדיר תשתית;
*   מפתח עם דגש על מהירות;
*   עובד עם משאבים מוגבלים.

### מודלים מקומיים: שליטה מלאה

אם **פרטיות, עבודה ללא אינטרנט** חשובים לך, או שאתה רוצה לבנות **סוכנים אוטונומיים לחלוטין**, אז הגיוני לפרוס את הרשת העצבית באופן מקומי.

**יתרונות עיקריים:**

*   **סודיות** — הנתונים נשארים אצלך;
*   **עבודה ללא אינטרנט** — שימושי ברשתות מבודדות;
*   **ללא מנויים ואסימונים** — חינם לאחר ההגדרה.

**החסרונות ברורים:**

*   דרישות משאבים (במיוחד לזיכרון וידאו);
*   ההגדרה עשויה לקחת זמן;
*   חלק מהמודלים קשים לפריסה ללא ניסיון.

עם זאת, ישנם כלים המפשטים את ההפעלה המקומית. אחד הטובים ביותר כיום הוא **Ollama**.

### פריסת LLM מקומי באמצעות Ollama + Docker

אנו נכין הפעלה מקומית של מודל Qwen 2.5 (qwen2.5:32b) באמצעות קונטיינר Docker ומערכת Ollama. זה יאפשר לשלב את הרשת העצבית עם MCP ולהשתמש בה בסוכנים שלך המבוססים על LangGraph.

אם משאבי המחשוב של המחשב או השרת שלך אינם מספיקים לעבודה עם גרסה זו של המודל, תוכל תמיד לבחור רשת עצבית פחות "רעבה" למשאבים — תהליך ההתקנה וההפעלה יישאר דומה.

**התקנה מהירה (סיכום שלבים)**

1.  **התקן Docker + Docker Compose**
2.  **צור מבנה פרויקט:**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **צור `docker-compose.yml`**
(אפשרות אוניברסלית, GPU מזוהה אוטומטית)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **הפעל את הקונטיינר:**
```bash
docker compose up -d
```

5.  **הורד את המודל:**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **בדוק את פעולת ה-API:**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "שלום!", "stream": false}'
```
*(תמונה עם תוצאת ביצוע הפקודה curl)*

7.  **שילוב עם Python:**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("הסבר סבך קוונטי"))
```
כעת יש לך LLM מקומי מלא, מוכן לעבוד עם MCP ו-LangGraph.

**מה הלאה?**

יש לנו בחירה בין מודלים בענן למודלים מקומיים, ולמדנו כיצד לחבר את שניהם. החלק המעניין ביותר לפנינו — **יצירת סוכני AI ב-LangGraph**, המשתמשים במודל הנבחר, בזיכרון, בכלים ובלוגיקה משלהם.

**בואו נעבור לחלק הטעים ביותר — קוד ופרקטיקה!**

---

לפני שנעבור לפרקטיקה, חשוב להכין את סביבת העבודה. אני מניח שאתה כבר מכיר את יסודות הפייתון, יודע מהן ספריות ותלויות, ומבין מדוע להשתמש בסביבה וירטואלית.

אם כל זה חדש לך — אני ממליץ קודם לעבור קורס קצר או מדריך על בסיס פייתון, ולאחר מכן לחזור למאמר.

#### שלב 1: יצירת סביבה וירטואלית

צור סביבה וירטואלית חדשה בתיקיית הפרויקט:
```bash
python -m venv venv
source venv/bin/activate  # עבור לינוקס/macOS
venv\Scripts\activate   # עבור Windows
```

#### שלב 2: התקנת תלויות

צור קובץ `requirements.txt` והוסף לו את השורות הבאות:
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ⚠️ **הגרסאות הנוכחיות הן נכון ל-21 ביולי 2025.** ייתכן שהן השתנו מאז הפרסום — **בדוק רלוונטיות לפני ההתקנה.**

לאחר מכן התקן את התלויות:
```bash
pip install -r requirements.txt```

#### שלב 3: הגדרת משתני סביבה

צור קובץ `.env` בשורש הפרויקט והוסף לו את מפתחות ה-API הדרושים:
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**מטרת המשתנים:**

*   **OPENAI_API_KEY** — מפתח לגישה למודלי GPT מ-OpenAI;
*   **DEEPSEEK_API_KEY** — מפתח לשימוש במודלי Deepseek;
*   **OPENROUTER_API_KEY** — מפתח יחיד לגישה למודלים רבים באמצעות OpenRouter

---
חלק מכלי MCP (לדוגמה, `brave-web-search`) דורשים מפתח כדי לעבוד. בלעדיו הם פשוט לא יופעלו.

**ומה אם אין לך מפתחות API?**

אין בעיה. אתה יכול להתחיל פיתוח עם מודל מקומי (לדוגמה, באמצעות Ollama), מבלי לחבר שום שירות חיצוני. במקרה זה, אין צורך ליצור את קובץ `.env` כלל.

מוכן! עכשיו יש לנו את כל מה שצריך כדי להתחיל — סביבה מבודדת, תלויות, ובמידת הצורך, גישה לרשתות עצביות בענן ושילובי MCP.

הבא - נריץ את סוכן ה-LLM שלנו בדרכים שונות.

### הפעלה פשוטה של סוכני LLM באמצעות LangGraph: אינטגרציה בסיסית

נתחיל מהפשוט ביותר: כיצד "לחבר את המוח" לסוכן העתידי. ננתח את הדרכים הבסיסיות להפעלת מודלי שפה (LLM) באמצעות LangChain, כך שבשלב הבא נוכל לעבור לשילוב עם LangGraph ובניית סוכן AI מלא.

#### ייבוא
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` ו-`load_dotenv()` — לטעינת משתנים מקובץ `.env`.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` — עטיפות לחיבור מודלי שפה באמצעות LangChain.

> 💡 אם אתה משתמש בגישות חלופיות לעבודה עם תצורות (לדוגמה, Pydantic Settings), תוכל להחליף את `load_dotenv()` בשיטה הרגילה שלך.

#### טעינת משתני סביבה
```python
load_dotenv()
```
זה יטען את כל המשתנים מ-`.env`, כולל מפתחות לגישה ל-API של OpenAI, DeepSeek, OpenRouter ואחרים.

#### פונקציות פשוטות לקבלת LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
אם משתנה `OPENAI_API_KEY` מוגדר כהלכה, LangChain יחליף אותו אוטומטית — ציון מפורש של `api_key=...` כאן הוא אופציונלי.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
באופן דומה, אך אנו משתמשים בעטיפה `ChatDeepSeek`.

**OpenRouter (ו-API תואמים אחרים)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**תכונות:**

*   `ChatOpenAI` משמש, למרות שהמודל אינו מ-OpenAI — מכיוון ש-OpenRouter משתמש באותו פרוטוקול.
*   `base_url` נדרש: הוא מצביע על ה-API של OpenRouter.
*   מודל `moonshotai/kimi-k2:free` נבחר כאחת האפשרויות המאוזנות ביותר מבחינת איכות ומהירות בזמן כתיבת המאמר.
*   מפתח ה-API של `OpenRouter` חייב להיות מועבר במפורש — החלפה אוטומטית אינה פועלת כאן.

#### מיני-בדיקה: בדיקת פעולת המודל
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("מי אתה?")
    print(response.content)
```
*(תמונה עם תוצאת ביצוע: `אני עוזר AI שנוצר על ידי Moonshot AI...`)*

אם הכל מוגדר כהלכה, תקבל תגובה משמעותית מהמודל. מזל טוב — הצעד הראשון נעשה!

### אבל זה עדיין לא סוכן

בשלב הנוכחי, חיברנו LLM וביצענו קריאה פשוטה. זה דומה יותר לצ'אטבוט קונסולה מאשר לסוכן AI.

**למה?**

*   אנו כותבים **קוד סינכרוני, ליניארי** ללא לוגיקת מצב או מטרות.
*   הסוכן אינו מקבל החלטות, אינו זוכר הקשר ואינו משתמש בכלים.
*   MCP ו-LangGraph עדיין לא מעורבים.

**מה הלאה?**

בהמשך, ניישם **סוכן AI מלא** באמצעות **LangGraph** — תחילה ללא MCP, כדי להתמקד בארכיטקטורה, במצבים ובלוגיקה של הסוכן עצמו.

בואו נצלול למכניקת סוכנים אמיתית. קדימה!

### סוכן סיווג משרות: מתיאוריה לפרקטיקה

...מושגי LangGraph בפועל וליצור כלי שימושי עבור פלטפורמות משאבי אנוש ובורסות פרילנסרים.

#### משימת הסוכן

הסוכן שלנו מקבל כקלט תיאור טקסטואלי של משרה פנויה או שירות ומבצע סיווג תלת-שכבתי:

1.  **סוג עבודה**: עבודת פרויקט או משרה קבועה
2.  **קטגוריית מקצוע**: מתוך 45+ התמחויות מוגדרות מראש
3.  **סוג חיפוש**: האם אדם מחפש עבודה או מחפש מבצע

התוצאה מוחזרת בפורמט JSON מובנה עם ציון ביטחון עבור כל סיווג.

#### 📈 ארכיטקטורת סוכן ב-LangGraph

בהתאם לעקרונות LangGraph, אנו יוצרים **גרף מצבים** של ארבעה צמתים:

- תיאור קלט
- ↓
- צומת סיווג סוג עבודה
- ↓
- צומת סיווג קטגוריה
- ↓
- צומת קביעת סוג חיפוש
- ↓
- צומת חישוב ביטחון
- ↓
- תוצאת JSON

כל צומת הוא **פונקציה מיוחדת** אשר:

*   מקבלת את המצב הנוכחי של הסוכן
*   מבצעת את חלקה בניתוח
*   מעדכנת את המצב ומעבירה אותו הלאה

#### ניהול מצב

הגדר את **מבנה הזיכרון של הסוכן** באמצעות `TypedDict`:

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """מצב הסוכן לאחסון מידע על תהליך הסיווג"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

זהו **זיכרון העבודה של הסוכן** — כל מה שהוא זוכר ואוגר במהלך הניתוח. בדומה לאופן שבו מומחה אנושי שומר בראשו את הקשר המשימה בעת ניתוח מסמך.

בואו נסתכל על הקוד המלא, ולאחר מכן נתמקד בנקודות העיקריות.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# קטגוריות מקצועיות
CATEGORIES = [
    "אנימטור דו-ממדי", "אנימטור תלת-ממדי", "מודל תלת-ממדי",
    "אנליסט עסקי", "מפתח בלוקצ'יין", ...
]

class JobType(Enum):
    PROJECT = "עבודת פרויקט"
    PERMANENT = "עבודה קבועה"

class SearchType(Enum):
    LOOKING_FOR_WORK = "מחפש עבודה"
    LOOKING_FOR_PERFORMER = "מחפש מבצע"

class State(TypedDict):
    """מצב הסוכן לאחסון מידע על תהליך הסיווג"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """סוכן אסינכרוני לסיווג משרות ושירותים"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """אתחול הסוכן"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """יוצר את זרימת העבודה של הסוכן המבוססת על LangGraph"""
        workflow = StateGraph(State)
        
        # הוספת צמתים לגרף
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # הגדרת רצף ביצוע הצמתים
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """צומת לקביעת סוג העבודה: פרויקט או קבוע"""
        # ... (היישום ממשיך)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """צומת לקביעת קטגוריית מקצוע"""
        # ... (היישום ממשיך)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """צומת לקביעת סוג חיפוש"""
        # ... (היישום ממשיך)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """צומת לחישוב רמת הביטחון בסיווג"""
        # ... (היישום ממשיך)

    def _find_closest_category(self, predicted_category: str) -> str:
        """מוצא את הקטגוריה הקרובה ביותר מתוך רשימת הקטגוריות הזמינות"""
        # ... (היישום ממשיך)

    async def classify(self, description: str) -> Dict[str, Any]:
        """שיטה ראשית לסיווג משרות/שירותים"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # הפעלת זרימת העבודה
        result = await self.workflow.ainvoke(initial_state)
        
        # יצירת תגובת JSON סופית
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """הדגמת פעולת הסוכן"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "דרוש מפתח פייתון ליצירת יישום אינטרנט ב-Django. עבודה קבועה.",
        "מחפש הזמנות ליצירת לוגואים וזהות תאגידית. אני עובד ב-Adobe Illustrator.",
        "דרוש אנימטור תלת-ממדי לפרויקט קצר טווח ליצירת פרסומת.",
        "קורות חיים: משווק מנוסה, מחפש עבודה מרחוק בתחום השיווק הדיגיטלי",
        "מחפשים מפתח פרונטאנד React לצוות שלנו על בסיס קבוע"
    ]
    
    print("🤖 הדגמת פעולת סוכן סיווג משרות\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- בדיקה {i}: ---")
        print(f"תיאור: {description}")
        try:
            result = await agent.classify(description)
            print("תוצאת סיווג:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"❌ שגיאה: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...שאר הקוד עם יישום השיטות הוצג במאמר...)*

### יתרונות מפתח של הארכיטקטורה
1.  **מודולריות** — כל צומת פותר משימה אחת, קל לבדוק ולשפר בנפרד
2.  **הרחבה** — ניתן להוסיף צמתי ניתוח חדשים מבלי לשנות קיימים
3.  **שקיפות** — כל תהליך קבלת ההחלטות מתועד וניתן למעקב
4.  **ביצועים** — עיבוד אסינכרוני של בקשות מרובות
5.  **אמינות** — מנגנוני גיבוי וטיפול בשגיאות מובנים

### תועלת אמיתית
סוכן כזה יכול לשמש ב:
*   **פלטפורמות משאבי אנוש** לסיווג אוטומטי של קורות חיים ומשרות פנויות
*   **בורסות פרילנסרים** לשיפור חיפוש והמלצות
*   **מערכות פנימיות** של חברות לטיפול בבקשות ופרויקטים
*   **פתרונות אנליטיים** לחקר שוק העבודה

### MCP בפעולה: יצירת סוכן עם מערכת קבצים וחיפוש אינטרנט
לאחר שהבנו את העקרונות הבסיסיים של LangGraph ויצרנו סוכן מסווג פשוט, בואו נרחיב את יכולותיו על ידי חיבורו לעולם החיצוני באמצעות MCP.

כעת ניצור עוזר AI מלא שיוכל:
*   לעבוד עם מערכת הקבצים (לקרוא, ליצור, לשנות קבצים)
*   לחפש מידע רלוונטי באינטרנט
*   לזכור את הקשר הדיאלוג
*   לטפל בשגיאות ולהתאושש מכשלים

#### מתיאוריה לכלים אמיתיים
זוכרים איך בתחילת המאמר דיברנו על כך ש**MCP הוא גשר בין רשת עצבית לסביבתה**? עכשיו תראו זאת בפועל. הסוכן שלנו יקבל גישה ל**כלים אמיתיים**:
```
# כלי מערכת קבצים
- read_file — קריאת קבצים
- write_file — כתיבה ויצירת קבצים
- list_directory — הצגת תוכן תיקיות
- create_directory — יצירת תיקיות

# כלי חיפוש אינטרנט
- brave_web_search — חיפוש באינטרנט
- get_web_content — קבלת תוכן דפים
```
זה כבר לא סוכן "צעצוע" — זהו **כלי עבודה** שיכול לפתור בעיות אמיתיות.

#### 📈 ארכיטקטורה: מפשוט למורכב

**1. תצורה כבסיס ליציבות**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """תצורת סוכן AI פשוטה"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """אימות תצורה"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"הנתיב אינו קיים: {self.filesystem_path}")
```
**למה זה חשוב?** בניגוד לדוגמת הסיווג, כאן הסוכן יוצר אינטראקציה עם מערכות חיצוניות. שגיאה אחת בנתיב הקובץ או מפתח API חסר — וכל הסוכן מפסיק לעבוד. **אימות בהפעלה** חוסך שעות של ניפוי באגים.

**2. מפעל מודלים: בחירה גמישה**
```python
def create_model(config: AgentConfig):
    """יוצר מודל בהתאם לתצורה"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... ספקים אחרים
```
קוד אחד — מודלים רבים. רוצה מודל מקומי חינם? השתמש ב-Ollama. צריך דיוק מרבי? עבור ל-GPT-4. מהירות חשובה? נסה את DeepSeek. הקוד נשאר זהה.

**3. שילוב MCP: חיבור לעולם האמיתי**
```python
async def _init_mcp_client(self):
    """אתחול לקוח MCP"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
כאן מתרחשת עבודת ה-MCP המרכזית: אנו מחברים שרתי MCP חיצוניים לסוכן, המספקים סט כלים ופונקציות. הסוכן מקבל לא רק פונקציות בודדות, אלא הבנה הקשרית מלאה של אופן העבודה עם מערכת הקבצים והאינטרנט.

#### עמידות בפני שגיאות
בעולם האמיתי, הכל מתקלקל: הרשת אינה זמינה, קבצים חסומים, מפתחות API פג תוקף. הסוכן שלנו מוכן לכך:
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
הדקורטור `@retry_on_failure` מנסה שוב אוטומטית פעולות במקרה של כשלים זמניים. המשתמש אפילו לא ישים לב שמשהו השתבש.

### תוצאות: מתיאוריה לפרקטיקה של סוכני AI

היום עברנו דרך מהמושגים הבסיסיים עד ליצירת סוכני AI עובדים. בואו נסכם את מה שלמדנו והשגנו.

**מה שלמדנו**

**1. מושגי יסוד**
*   הבנו את ההבדל בין צ'אטבוטים לסוכני AI אמיתיים
*   הבנו את תפקיד **MCP (Model Context Protocol)** כגשר בין המודל לעולם החיצון
*   למדנו את ארכיטקטורת **LangGraph** לבניית לוגיקת סוכן מורכבת

**2. מיומנויות מעשיות**
*   הגדרנו סביבת עבודה עם תמיכה במודלים בענן ומקומיים
*   יצרנו **סוכן מסווג** עם ארכיטקטורה אסינכרונית וניהול מצבים
*   בנינו **סוכן MCP** עם גישה למערכת הקבצים ולחיפוש אינטרנט

**3. דפוסי ארכיטקטורה**
*   שלטנו בתצורה מודולרית ובמפעלי מודלים
*   יישמנו טיפול בשגיאות ו**מנגנוני ניסיון חוזר** לפתרונות מוכנים לייצור

### יתרונות מפתח של הגישה
**LangGraph + MCP** נותנים לנו:
*   **שקיפות** — כל שלב של הסוכן מתועד וניתן למעקב
*   **הרחבה** — תכונות חדשות מתווספות באופן הצהרתי
*   **אמינות** — טיפול בשגיאות ושחזור מובנים
*   **גמישות** — תמיכה במודלים וספקים מרובים מהקופסה

### מסקנה

סוכני AI אינם בדיה עתידנית, אלא **טכנולוגיה אמיתית של היום**. עם LangGraph ו-MCP, אנו יכולים ליצור מערכות הפותרות בעיות עסקיות ספציפיות, אוטומטיות שגרות ופותחות אפשרויות חדשות.

**העיקר — להתחיל.** קח את הקוד מהדוגמאות, התאם אותו למשימות שלך, התנסה. כל פרויקט הוא חוויה חדשה וצעד לקראת שליטה בתחום פיתוח AI.

בהצלחה בפרויקטים שלך!

---
*תגיות: python, ai, mcp, langchain, ai-assistant, ollama, ai-agents, local llm, langgraph, mcp-server*
*האבים: בלוג חברת Amvera, עיבוד שפה טבעית, בינה מלאכותית, Python, תכנות*
