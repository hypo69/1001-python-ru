### **מחזור "לא סלניום". מבוא**

אלה העוסקים בגירוד אתרים, בדיקות ואוטומציה מכירים את Selenium, את Playwright המודרני יותר ו/או את פריימוורק Crawlee. הם חזקים, הם יכולים לעשות כמעט הכל, והם... לא תמיד נחוצים. יתרה מכך, במקרים רבים, שימוש בכלים אלה הוא כמו לדפוק מסמרים במיקרוסקופ: העבודה אמנם תיעשה, אך במחיר של הוצאות בלתי מוצדקות – מהירות, משאבי מערכת ומורכבות הגדרה.

ברוכים הבאים לסדרת המאמרים "לא סלניום". כאן אראה דרכים אחרות (לא תמיד ברורות) לאינטראקציה עם תוכן האינטרנט.

#### פרדיגמה מס' 1: תקשורת ישירה. לקוחות HTTP

*   **`Requests`** — יוצר ושולח בקשת רשת לכתובת היעד (URL), בדיוק כפי שעושה הדפדפן שלך ברגע הראשון של טעינת הדף, אך ללא הדפדפן עצמו. בבקשה זו הוא אורז את השיטה (לדוגמה, `GET` כדי לקבל נתונים), כותרות (`Headers`) המייצגות את האתר (לדוגמה, `User-Agent: "אני-דפדפן"`), ופרמטרים אחרים. בתגובה מהשרת, הוא מקבל נתונים גולמיים — לרוב, זהו קוד ה-HTML המקורי של הדף או מחרוזת בפורמט JSON, וכן קוד סטטוס (לדוגמה, `200 OK`).

*   **`HTTPX`** — הוא יורש מודרני של `Requests`. ברמה הבסיסית, הוא עושה את אותו הדבר: שולח את אותן בקשות HTTP עם אותן כותרות ומקבל את אותן תגובות. אבל יש הבדל מהותי: `Requests` עובד **באופן סינכרוני** — הוא שולח בקשה, יושב ומחכה לתגובה, מקבל תגובה, שולח את הבאה. `HTTPX`, לעומת זאת, יכול לעבוד **באופן אסינכרוני** — הוא יכול "לשגר" מאה בקשות בבת אחת מבלי לחכות לתגובות, ולאחר מכן לעבד אותן ביעילות ככל שהן מגיעות.

הם מצוינים לאיסוף נתונים מאתרים סטטיים, עבודה עם ממשקי API, ניתוח אלפי דפים שבהם אין צורך בהפעלת JavaScript.

*   **יתרונות:** **מהירות ויעילות.** בזכות האופי האסינכרוני של `HTTPX`, במקום שבו `Requests` יבצע ברצף 100 בקשות במשך מספר דקות, `HTTPX` יטפל בזה תוך מספר שניות.
*   **חסרונות:** לא מתאימים לאתרים שבהם התוכן נוצר באמצעות JavaScript.

#### פרדיגמה מס' 2: פרוטוקול Chrome DevTools (CDP)

מה לעשות אם האתר דינמי והתוכן נוצר באמצעות JavaScript? לדפדפנים מודרניים (Chrome, Chromium, Edge) יש פרוטוקול מובנה לניפוי באגים ובקרה — **Chrome DevTools Protocol (CDP)**. הוא מאפשר לשלוח פקודות ישירות לדפדפן, תוך עקיפת שכבת WebDriver המסורבלת שבה משתמש Selenium.

*   **כלים:** הנציג העיקרי של גישה זו כיום הוא `Pydoll`, שהחליף את `pyppeteer` שהיה פעם פופולרי אך אינו נתמך עוד.
*   **מתי להשתמש:** כאשר נדרש רינדור JavaScript, אך רוצים לשמור על מהירות גבוהה ולהימנע ממורכבות עם דרייברים.
*   **יתרונות:** **איזון.** אתה מקבל את העוצמה של דפדפן אמיתי, אך עם תקורה נמוכה בהרבה ולעתים קרובות עם מנגנוני עקיפת הגנות מובנים.
*   **חסרונות:** יכול להיות קשה יותר לניפוי באגים מ-Playwright ודורש הבנה עמוקה יותר של פעולת הדפדפן.

#### פרדיגמה מס' 3: סוכני LLM אוטונומיים

זהו הקצה המוביל. מה אם במקום לכתוב קוד שאומר "לחץ כאן, הקלד זאת", פשוט ניתן משימה בשפה טבעית? "מצא לי את כל הספקים באתר זה ואסוף את קטגוריות המוצרים שלהם".

זו בדיוק הבעיה שסוכני LLM פותרים. באמצעות "מוח" בצורת מודל שפה גדול (GPT, Gemini) ו"ידיים" בצורת סט כלים (דפדפן, חיפוש Google), סוכנים אלה יכולים לתכנן ולבצע באופן עצמאי משימות מורכבות באינטרנט.

*   **כלים:** חבילות כמו `LangChain` + `Pydoll` או פתרונות מותאמים אישית, כמו ב-`simple_browser.py`, שננתח בהמשך.
*   **מתי להשתמש:** למשימות מחקר מורכבות שבהן השלבים אינם ידועים מראש ונדרשת התאמה בזמן אמת.
*   **יתרונות:** **אינטליגנציה.** היכולת לפתור בעיות לא מובנות ולהתאים את עצמך לשינויים תוך כדי תנועה.
*   **חסרונות:** "אי-דטרמיניזם" (התוצאות עשויות להשתנות מהרצה להרצה), עלות קריאות API ל-LLM, מהירות נמוכה יותר בהשוואה לקוד ישיר.

#### פרדיגמה מס' 4: גירוד ללא קוד

לפעמים המשימה כל כך פשוטה, שכתיבת קוד היא מיותרת. צריך לשלוף במהירות טבלה מדף אחד? לשם כך קיימים פתרונות אלגנטיים שאינם דורשים תכנות.

*   **כלים:** פונקציות Google Sheets (`IMPORTXML`, `IMPORTHTML`), הרחבות דפדפן.
*   **מתי להשתמש:** למשימות חד-פעמיות, אב-טיפוס מהיר, או כשאתה פשוט לא רוצה לכתוב קוד.
*   **יתרונות:** **פשטות.** פתח, ציין מה לאסוף, — קיבל את התוצאה.
*   **חסרונות:** פונקציונליות מוגבלת, לא מתאימים למשימות מורכבות או לכמויות גדולות של נתונים.

### מה הלאה?

מאמר זה הוא רק מבוא. בגיליונות הבאים של סדרת "לא סלניום" שלנו, נעבור מתיאוריה לפרקטיקה קשה. נצלול עמוק לכל אחת מהפרדיגמות הללו ונראה כיצד הן פועלות בדוגמאות מהעולם האמיתי:

*   ננתח את **Pydoll** ונראה כיצד הוא עוקף את Cloudflare.
*   נארגן קרב בין **JavaScript ל-Python** על התואר השפה הטובה ביותר לגירוד אתרים.
*   נלמד כיצד לסחוט מהירות מקסימלית מניתוח באמצעות **lxml**.
*   נכתוב סקריפט שאוסף נתונים מ**אמזון** ושומר אותם ב**אקסל**.
*   נראה כיצד **Google Sheets** יכול להפוך לגרדן הראשון שלך.
*   וכמובן, ננתח בפירוט כיצד ליצור ולהשתמש ב**סוכן LLM אוטונומי** לשליטה בדפדפן.

התכונן לשנות את השקפתך על אוטומציה ואיסוף נתונים באינטרנט. זה יהיה מהיר, יעיל ומעניין מאוד. הירשם
