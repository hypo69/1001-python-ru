### **מחזור «לא סלניום». מבוא**

אלה העוסקים בגירוד אתרים, בדיקות ואוטומציה מכירים את Selenium, את Playwright המודרני יותר ו/או את פריימוורק Crawlee. הם חזקים, הם יכולים לעשות כמעט הכל, והם... לא תמיד נחוצים. יתרה מכך, במקרים רבים השימוש בכלים אלה הוא כמו לדפוק מסמרים במיקרוסקופ: העבודה, כמובן, תיעשה, אך במחיר של עלויות בלתי מוצדקות — מהירות, משאבי מערכת ומורכבות הגדרה.

ברוכים הבאים למחזור המאמרים «לא סלניום». כאן אראה דרכים אחרות (לא תמיד ברורות) לאינטראקציה עם תוכן האינטרנט.

#### פרדיגמה מס' 1: תקשורת ישירה. לקוחות HTTP

*   **`Requests`** — יוצר ושולח בקשת רשת לכתובת היעד (URL), בדיוק כפי שעושה הדפדפן שלך ברגע הראשון של טעינת הדף, אך ללא הדפדפן עצמו. בבקשה זו הוא אורז את השיטה (לדוגמה, `GET`, כדי לקבל נתונים), כותרות (`Headers`), המיוצגות לאתר (לדוגמה, `User-Agent: "אני-דפדפן"`), ופרמטרים אחרים. בתגובה מהשרת הוא מקבל נתונים גולמיים — לרוב, זהו קוד ה-HTML המקורי של הדף או מחרוזת בפורמט JSON, וכן קוד סטטוס (לדוגמה, `200 OK`).

*   **`HTTPX`** — הוא יורש מודרני של `Requests`. ברמה בסיסית, הוא עושה את אותו הדבר: שולח את אותן בקשות HTTP עם אותן כותרות ומקבל את אותן תגובות. אבל יש הבדל מהותי: `Requests` עובד **באופן סינכרוני** — שולח בקשה, יושב ומחכה לתגובה, מקבל תגובה, שולח את הבאה. `HTTPX`, לעומת זאת, יכול לעבוד **באופן אסינכרוני** — הוא יכול "לשגר" מאה בקשות בבת אחת מבלי לחכות לתגובות, ולאחר מכן לעבד אותן ביעילות ככל שהן מגיעות.

הם מתאימים מצוין לאיסוף נתונים מאתרים סטטיים, עבודה עם API, ניתוח אלפי דפים שבהם לא נדרש ביצוע JavaScript.

*   **יתרונות:** **מהירות ויעילות.** הודות לאופי האסינכרוני של `HTTPX`, היכן ש-`Requests` יבצע ברצף 100 בקשות במשך מספר דקות, `HTTPX` יסיים זאת תוך מספר שניות.
*   **חסרונות:** לא מתאים לאתרים שבהם התוכן נוצר באמצעות JavaScript.

#### פרדיגמה מס' 2: פרוטוקול כלי מפתחים של Chrome (CDP)

מה לעשות אם האתר דינמי והתוכן נוצר באמצעות JavaScript? לדפדפנים מודרניים (Chrome, Chromium, Edge) יש פרוטוקול מובנה לניפוי באגים ובקרה — **Chrome DevTools Protocol (CDP)**. הוא מאפשר לשלוח פקודות לדפדפן ישירות, תוך עקיפת שכבת ה-WebDriver המסורבלת שבה משתמש Selenium.

*   **כלים:** הנציג העיקרי של גישה זו כיום הוא `Pydoll`, שהחליף את `pyppeteer` שהיה פעם פופולרי אך כיום אינו נתמך.
*   **מתי להשתמש:** כאשר נדרש רינדור JavaScript, אך רוצים לשמור על מהירות גבוהה ולהימנע מסיבוכים עם דרייברים.
*   **יתרונות:** **איזון.** אתה מקבל את העוצמה של דפדפן אמיתי, אך עם תקורה נמוכה בהרבה ולעיתים קרובות עם מנגנוני עקיפת הגנות מובנים.
*   **חסרונות:** יכול להיות קשה יותר לניפוי באגים מ-Playwright ודורש הבנה עמוקה יותר של פעולת הדפדפן.

#### פרדיגמה מס' 3: סוכני LLM אוטונומיים

זהו הגבול המתקדם ביותר. מה אם במקום לכתוב קוד שאומר "לחץ כאן, הקלד זאת", פשוט ניתן משימה בשפה טבעית? "מצא לי את כל הספקים באתר זה ואסוף את קטגוריות המוצרים שלהם".

זו בדיוק המשימה שסוכני LLM פותרים. באמצעות "מוח" בצורת מודל שפה גדול (GPT, Gemini) ו"ידיים" בצורת סט כלים (דפדפן, חיפוש בגוגל), סוכנים אלה יכולים לתכנן ולבצע באופן עצמאי משימות מורכבות באינטרנט.

*   **כלים:** צמדים כמו `LangChain` + `Pydoll` או פתרונות מותאמים אישית, כמו ב-`simple_browser.py`, שננתח בהמשך.
*   **מתי להשתמש:** למשימות מחקר מורכבות שבהן השלבים אינם ידועים מראש ונדרשת התאמה בזמן אמת.
*   **יתרונות:** **אינטליגנציה.** היכולת לפתור בעיות לא מובנות ולהתאים את עצמם לשינויים תוך כדי תנועה.
*   **חסרונות:** "אי-דטרמיניזם" (התוצאות עשויות להשתנות מהרצה להרצה), עלות קריאות API ל-LLM, מהירות נמוכה יותר בהשוואה לקוד ישיר.

#### פרדיגמה מס' 4: גירוד ללא קוד

לפעמים המשימה כל כך פשוטה, שכתיבת קוד היא מיותרת. צריך לשלוף במהירות טבלה מדף אחד? לשם כך קיימים פתרונות אלגנטיים שאינם דורשים תכנות.

*   **כלים:** פונקציות Google Sheets (`IMPORTXML`, `IMPORTHTML`), הרחבות דפדפן.
*   **מתי להשתמש:** למשימות חד פעמיות, אב טיפוס מהיר או כשאתה פשוט לא רוצה לכתוב קוד.
*   **יתרונות:** **פשטות.** פתח, ציין מה לאסוף, — קיבל תוצאה.
*   **חסרונות:** פונקציונליות מוגבלת, לא מתאים למשימות מורכבות או לכמויות גדולות של נתונים.

### מה הלאה?

מאמר זה הוא רק מבוא. בגיליונות הבאים של סדרת «לא סלניום» שלנו, נעבור מתיאוריה לפרקטיקה קשה. נצלול עמוק לכל אחת מהפרדיגמות הללו ונראה כיצד הן פועלות בדוגמאות מהעולם האמיתי:

*   ננתח את **Pydoll** ונראה כיצד הוא עוקף את Cloudflare.
*   נארגן קרב בין **JavaScript ל-Python** על התואר השפה הטובה ביותר לגירוד אתרים.
*   נלמד כיצד לסחוט את המהירות המרבית מניתוח באמצעות **lxml**.
*   נכתוב סקריפט שאוסף נתונים מ-**Amazon** ושומר אותם ב-**Excel**.
*   נראה כיצד **Google Sheets** יכול להפוך לגרדן הראשון שלך.
*   וכמובן, ננתח בפירוט כיצד ליצור ולהשתמש ב**סוכן LLM אוטונומי** לשליטה בדפדפן.

התכונן לשנות את השקפתך על אוטומציה ואיסוף נתונים באינטרנט. זה יהיה מהיר, יעיל ומעניין מאוד. הירשם
