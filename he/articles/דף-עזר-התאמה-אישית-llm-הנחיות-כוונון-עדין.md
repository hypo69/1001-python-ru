## דף עזר: התאמה אישית של LLM: הנחיות, כוונון עדין של מודלים, דוגמאות קוד.

במאמר זה:

1.  כיצד נוצר "אפקט הזיכרון" ב-LLM (סקירה קצרה).
2.  למה ומתי יש צורך בכוונון עדין (Fine-tuning) של מודל.
3.  מתי כוונון עדין אינו הפתרון הטוב ביותר.
4.  הכנת נתונים.
5.  דוגמאות לכוונון עדין עבור **OpenAI (GPT)**, **Google (Gemini)** ו-**Anthropic (Claude)** (שונה).

### 1. כיצד LLM "זוכר" ו"מתאים את עצמו": אשליית ההקשר

לפני שמדברים על כוונון עדין, חשוב להבין כיצד LLM מצליח ליצור תחושה של התאמה אישית מלכתחילה.
זה חשוב כדי לא למהר לכוונון עדין יקר אם המשימה ניתנת לפתרון בדרכים פשוטות יותר:

*   **דרך חלון ההקשר (זיכרון לטווח קצר):** במסגרת שיחה אחת, אתה שולח למודל לא רק את השאלה החדשה, אלא גם **את כל ההתכתבות הקודמת או חלקה**. המודל מעבד את כל הטקסט הזה כ"הקשר" אחד. בזכות זה הוא "זוכר" שורות קודמות וממשיך את המחשבה. המגבלה כאן היא אורך חלון ההקשר (מספר האסימונים).
*   **יצירת הנחיות מערכת (System Prompt):** ניתן להקצות למודל תפקיד, טון וכללי התנהגות בתחילת כל שיחה. לדוגמה: "אתה מומחה לפייתון, ענה בקצרה."
*   **הכללת מספר דוגמאות להתנהגות רצויה בבקשה (Few-Shot Learning):** (זוגות קלט/פלט) מאפשרת למודל "ללמוד" תבנית זו ישירות במסגרת הבקשה הנוכחית.
*   **ניהול מצב בצד האפליקציה:** השיטה החזקה ביותר. האפליקציה (הניגשת ל-API) יכולה לאחסן מידע על המשתמש (העדפות, היסטוריה, נתוני פרופיל) ולהוסיף אותו באופן דינמי להנחיה לפני שליחתו למודל.

### 2. למה ומתי צריך כוונון עדין

כוונון עדין הוא תהליך של אימון נוסף של LLM בסיסי שכבר אומן על מערך נתונים ספציפי משלך. זה מאפשר למודל:

*   **להתאים סגנון וטון:** המודל ידבר "בשפה שלך" - בין אם זה מדע קפדני, שיווק ידידותי או סלנג של קהילה מסוימת.
*   **לעקוב אחר הוראות ופורמטים ספציפיים:** אם אתה זקוק לתשובות במבנה JSON מוגדר בקפדנות, או תמיד עם קבוצה מסוימת של שדות.
*   **להבין שפה ספציפית לתחום:** אימון על התיעוד הפנימי שלך או על טקסטים מהתעשייה יעזור למודל להתמודד טוב יותר עם המינוח של הנישה שלך.
*   **לשפר ביצועים במשימות צרות:** עבור סוגים מסוימים של שאילתות (למשל, סיווג ביקורות, יצירת קוד במסגרת ספציפית), כוונון עדין יכול לספק תשובות מדויקות ורלוונטיות יותר מאשר מודל בסיסי.
*   **לקצר את אורך ההנחיות:** אם המודל כבר "יודע" את ההתנהגות הרצויה בזכות הכוונון, אינך צריך להזכיר לו זאת בכל פעם בהנחיה, מה שחוסך אסימונים ומפחית את ההשהיה.

### 3. מתי כוונון עדין אינו הפתרון הטוב ביותר

כוונון עדין הוא כלי רב עוצמה אך לא אוניברסלי. אין להשתמש בו אם:

*   **המודל צריך לגשת לידע חדש:** כוונון עדין משנה את משקולות המודל, אך אינו "טוען" לתוכו עובדות חדשות בזמן אמת. אם המשימה שלך היא לענות על שאלות על בסיס ידע המשתנה ללא הרף (מסמכי חברה, חדשות אחרונות), עדיף להשתמש ב-**Retrieval Augmented Generation (RAG)**. כאן, המודל הבסיסי מקבל הקשר מבסיס הנתונים שלך *בזמן הבקשה*.
*   **משימה פשוטה נפתרת באמצעות הנדסת הנחיות:** התחל תמיד עם הנדסת ההנחיות היעילה ביותר. אם המשימה נפתרת באמצעות הוראות פשוטות ודוגמאות של few-shot, כוונון עדין מיותר ויקר יותר.
*   **אין לך מספיק נתונים באיכות גבוהה:** נתונים גרועים = מודל מכוונן גרוע.

### 4. הכנת נתונים.

האיכות והכמות של הנתונים שלך הן קריטיות. המודל לומד מהדוגמאות שלך, ולכן הן חייבות להיות מדויקות, מגוונות ועקביות.

*   **פורמט:** לרוב JSON Lines (`.jsonl`) או CSV (`.csv`).
*   **מבנה נתונים:** תלוי במשימה.
    *   **כוונון הוראות (Instruction-Response):** מתאים למשימות כלליות כמו "שאלה-תשובה", פרפרזה, סיכום.
        ```json
        {"input_text": "פרפרז את המשפט: 'טכנולוגיית הבינה המלאכותית מתפתחת במהירות.'", "output_text": "בינה מלאכותית מפגינה התקדמות מהירה."}
        {"input_text": "מהי בירת צרפת?", "output_text": "בירת צרפת היא פריז."}
        ```
    *   **כוונון צ'אט (Chat Tuning):** אידיאלי לאימון מודל לנהל שיחה בתפקיד או בסגנון מסוים.
        ```json
        {"messages": [{"author": "user", "content": "היי! מה תמליץ לארוחת ערב?"}, {"author": "model", "content": "ערב טוב! היום יום מצוין לפסטה קרבונרה, או אם אתה מעדיף משהו קליל, סלט קיסר."}]}
        {"messages": [{"author": "user", "content": "ספר לי על התכונות החדשות בפייתון 3.12."}, {"author": "model", "content": "פייתון 3.12 הציג f-strings עם הזחה, תחביר חדש לגנריים אוניברסליים וטיפול משופר בשגיאות Unicode."}]}
        ```
*   **כמות:** מינימום של 100-200 דוגמאות איכותיות, אבל ככל שיותר, כן ייטב (אלפים למשימות רציניות).
*   **איכות:** דיוק, עקביות של סגנון, טון ופורמט בכל מערך הנתונים.

### 5. מודלים ופלטפורמות

#### 5.1. OpenAI

OpenAI מספקת API פשוט ואינטואיטיבי למדי לכוונון עדין.

**תהליך:**

1.  הכן נתונים בפורמט JSONL, כאשר כל שורה היא אובייקט עם מערך של `messages` (כמו ב-Chat Completions API).
2.  העלה את קובץ הנתונים באמצעות ה-API.
3.  צור עבודת כוונון עדין, תוך ציון הקובץ שהועלה והמודל הבסיסי.
4.  עקוב אחר ההתקדמות.
5.  השתמש במודל החדש והמכוונן לפי המזהה שלו.

**דוגמת נתונים (קובץ `training_data.jsonl`):**

```json
{"messages": [{"role": "system", "content": "אתה עוזר תמיכת לקוחות ידידותי של חברת IT."}, {"role": "user", "content": "האינטרנט שלי לא עובד."}, {"role": "assistant", "content": "אוי, אני מצטער על אי הנוחות! בוא ננסה להבין את זה. איך אתה מחובר לאינטרנט - באמצעות Wi-Fi או כבל?"}]}
{"messages": [{"role": "system", "content": "אתה עוזר תמיכת לקוחות ידידותי של חברת IT."}, {"role": "user", "content": "איך אני יכול לאפס את הסיסמה שלי?"}, {"role": "assistant", "content": "כמובן! כדי לאפס את הסיסמה שלך, אנא עבור לדף הכניסה ולחץ על 'שכחת סיסמה?' מתחת לשדה הקלט. לאחר מכן עקוב אחר ההוראות שעל המסך."}]
```

**דוגמת קוד פייתון:**

תחילה, התקן: `pip install openai`

```python
import openai
from openai import OpenAI
import os

# הגדר את מפתח ה-API של OpenAI. מומלץ להשתמש במשתנה סביבה.
# os.environ["OPENAI_API_KEY"] = "sk-..."
client = OpenAI()

# 1. העלאת קובץ הנתונים
try:
    file_response = client.files.create(
        file=open("training_data.jsonl", "rb"),
        purpose="fine-tune"
    )
    file_id = file_response.id
    print(f"הקובץ הועלה בהצלחה. מזהה קובץ: {file_id}")
except openai.APIStatusError as e:
    print(f"שגיאת העלאת קובץ: {e.status_code} - {e.response}")
    exit()

# 2. יצירת עבודת כוונון עדין
try:
    ft_job_response = client.fine_tuning.jobs.create(
        training_file=file_id,
        model="gpt-3.5-turbo" # ניתן לציין גרסה ספציפית, למשל, "gpt-3.5-turbo-0125"
    )
    job_id = ft_job_response.id
    print(f"עבודת כוונון עדין נוצרה. מזהה עבודה: {job_id}")
    print("עקוב אחר סטטוס העבודה באמצעות ה-API או במגרש המשחקים של OpenAI.")
except openai.APIStatusError as e:
    print(f"שגיאה ביצירת עבודה: {e.status_code} - {e.response}")
    exit()

# דוגמה למעקב אחר סטטוס וקבלת שם המודל (להפעיל לאחר יצירת העבודה):
# # job_id = "ftjob-..." # החלף במזהה העבודה שלך
# # job_status = client.fine_tuning.jobs.retrieve(job_id)
# # print(f"סטטוס עבודה נוכחי: {job_status.status}")
# # if job_status.status == "succeeded":
# #     fine_tuned_model_name = job_status.fine_tuned_model
# #     print(f"שם המודל המכוונן: {fine_tuned_model_name}")

# 3. שימוש במודל המכוונן (לאחר שהוא מוכן)
# # החלף בשם האמיתי של המודל שלך, שהתקבל לאחר כוונון עדין מוצלח
# # fine_tuned_model_name = "ft:gpt-3.5-turbo-0125:my-org::abcd123"

# # if 'fine_tuned_model_name' in locals() and fine_tuned_model_name:
# #     try:
# #         response = client.chat.completions.create(
# #             model=fine_tuned_model_name,
# #             messages=[
# #                 {"role": "user", "content": "יש לי בעיה עם הכניסה שלי."}
# #             ]
# #         )
# #         print("\nתגובה מהמודל המכוונן:")
# #         print(response.choices[0].message.content)
# #     except openai.APIStatusError as e:
# #         print(f"שגיאה בשימוש במודל: {e.status_code} - {e.response}")
```

#### 5.2. Anthropic

Anthropic **אינה מספקת API ציבורי לכוונון עדין של מודלי Claude 3 שלה (Opus, Sonnet, Haiku) באותו מובן כמו OpenAI או Google.**

Anthropic מתמקדת ביצירת מודלי בסיס חזקים מאוד, שלטענתם, עובדים מצוין עם הנדסת הנחיות מתקדמת ותבניות RAG, וממזערים את הצורך בכוונון עדין ברוב המקרים.
עבור לקוחות ארגוניים גדולים או שותפים, ייתכנו תוכניות ליצירת מודלים "מותאמים אישית" או אינטגרציות מיוחדות, אך זו אינה תכונת כוונון עדין זמינה לציבור באמצעות API.

אם אתה עובד עם Claude 3, המיקוד העיקרי שלך צריך להיות על:

*   **הנדסת הנחיות איכותית:** התנסה עם הנחיות מערכת, דוגמאות של few-shot ועיצוב בקשות ברור. Claude ידוע ביכולתו לעקוב בקפדנות אחר הוראות, במיוחד בתגי XML.
*   **מערכות RAG:** השתמש בבסיסי ידע חיצוניים כדי לספק למודל הקשר עדכני.

#### 5.3. Google (Gemini)

Google מפתחת באופן פעיל יכולות כוונון עדין באמצעות פלטפורמת **Google Cloud Vertex AI** שלה.
זוהי פלטפורמת ML מלאה המספקת כלים להכנת נתונים, הפעלת עבודות אימון ופריסת מודלים.
כוונון עדין זמין עבור משפחת המודלים של Gemini.

**תהליך:**

1.  הכן נתונים (JSONL או CSV) בפורמט `input_text`/`output_text` (עבור כוונון הוראות) או `messages` (עבור כוונון צ'אט).
2.  העלה נתונים ל-Google Cloud Storage (GCS).
3.  צור והפעל עבודת כוונון עדין באמצעות Vertex AI Console או SDK.
4.  פרוס את המודל המכוונן לנקודת קצה (Endpoint).
5.  השתמש במודל המכוונן דרך נקודת קצה זו.

**דוגמת נתונים (קובץ `gemini_tuning_data.jsonl`):**

```json
{"input_text": "סכם את הרעיונות המרכזיים של ספר זה: 'הספר עוסק במסעו של גיבור, המתגבר על מכשולים ומוצא את עצמו.'", "output_text": "הדמות הראשית בספר יוצאת למסע טרנספורמטיבי, מתמודדת עם קשיים וזוכה להכרה עצמית."}
{"input_text": "הסבר את עקרון הפעולה של כור היתוך תרמו-גרעיני במילים פשוטות.", "output_text": "כור היתוך תרמו-גרעיני מנסה לשכפל את התהליך המתרחש בשמש: היתוך של גרעיני אטום קלים בטמפרטורות גבוהות מאוד, המשחרר כמות עצומה של אנרגיה."}
```

**דוגמת קוד פייתון (דורש `google-cloud-aiplatform`):**

תחילה, התקן: `pip install google-cloud-aiplatform` ו-`pip install google-cloud-storage`

```python
import os
from google.cloud import aiplatform
from google.cloud import storage

# --- הגדרות ---
# החלף בערכים שלך:
PROJECT_ID = "your-gcp-project-id"
REGION = "us-central1"               # בחר אזור התומך ב-Gemini וב-Vertex AI
BUCKET_NAME = "your-gcs-bucket-for-tuning" # שם דלי ה-GCS שלך (חייב להיווצר מראש)
DATA_FILE_LOCAL_PATH = "gemini_tuning_data.jsonl"
GCS_DATA_URI = f"gs://{BUCKET_NAME}/{DATA_FILE_LOCAL_PATH}"
TUNED_MODEL_DISPLAY_NAME = "my-tuned-gemini-model"
# --- סוף הגדרות ---

# אתחול Vertex AI
aiplatform.init(project=PROJECT_ID, location=REGION)

# 1. יצירת קובץ נתונים (אם אינו קיים)
with open(DATA_FILE_LOCAL_PATH, "w", encoding="utf-8") as f:
    f.write('{"input_text": "סכם את הרעיונות המרכזיים של ספר זה: \'הספר עוסק במסעו של גיבור, המתגבר על מכשולים ומוצא את עצמו.\'", "output_text": "הדמות הראשית בספר יוצאת למסע טרנספורמטיבי, מתמודדת עם קשיים וזוכה להכרה עצמית."}\n')
    f.write('{"input_text": "הסבר את עקרון הפעולה של כור היתוך תרמו-גרעיני במילים פשוטות.", "output_text": "כור היתוך תרמו-גרעיני מנסה לשכפל את התהליך המתרחש בשמש: היתוך של גרעיני אטום קלים בטמפרטורות גבוהות מאוד, המשחרר כמות עצומה של אנרגיה."}\n')
print(f"קובץ נתונים '{DATA_FILE_LOCAL_PATH}' נוצר.")


# 2. העלאת נתונים ל-Google Cloud Storage
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """מעלה קובץ לדלי GCS."""
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"קובץ '{source_file_name}' הועלה ל-'gs://{bucket_name}/{destination_blob_name}'.")

try:
    upload_blob(BUCKET_NAME, DATA_FILE_LOCAL_PATH, DATA_FILE_LOCAL_PATH)
except Exception as e:
    print(f"שגיאה בהעלאת קובץ ל-GCS. ודא שהדלי קיים ושיש לך הרשאות: {e}")
    exit()

# 3. יצירה והרצה של עבודת כוונון עדין
print(f"\nמתחיל כוונון עדין של המודל '{TUNED_MODEL_DISPLAY_NAME}'...")
try:
    # `tune_model` מפעיל את העבודה ומחזיר את המודל המכוונן לאחר השלמה
    tuned_model = aiplatform.Model.tune_model(
        model_display_name=TUNED_MODEL_DISPLAY_NAME,
        source_model_name="gemini-1.0-pro-001", # מודל בסיס Gemini Pro
        training_data_path=GCS_DATA_URI,
        tuning_method="SUPERVISED_TUNING",
        train_steps=100, # מספר שלבי האימון. הערך האופטימלי תלוי בגודל הנתונים.
        # batch_size=16, # ניתן לציין
        # learning_rate_multiplier=1.0 # ניתן לציין
    )
    print(f"מודל '{TUNED_MODEL_DISPLAY_NAME}' כוונן בהצלחה. מזהה מודל: {tuned_model.name}")
    print("תהליך הכוונון העדין יכול לקחת זמן רב.")
except Exception as e:
    print(f"שגיאת כוונון עדין. בדוק את היומנים ב-Vertex AI Console: {e}")
    exit()

# 4. פריסת המודל המכוונן (לשימוש)
print(f"\nפורס את המודל המכוונן '{TUNED_MODEL_DISPLAY_NAME}' לנקודת קצה...")
try:
    endpoint = tuned_model.deploy(
        machine_type="n1-standard-4", # סוג מכונה לנקודת הקצה. בחר אחד מתאים.
        min_replica_count=1,
        max_replica_count=1
    )
    print(f"המודל נפרס לנקודת הקצה: {endpoint.name}")
    print("הפריסה יכולה גם לקחת מספר דקות.")
except Exception as e:
    print(f"שגיאה בפריסת המודל: {e}")
    exit()

# 5. שימוש במודל המכוונן
print("\nבודק את המודל המכוונן...")
prompt = "ספר לי על היכולות שלך לאחר האימון."
instances = [{"prompt": prompt}] # לכוונון הוראות. אם כוונון צ'אט, אז {"messages": [...]}

try:
    response = endpoint.predict(instances=instances)
    print("\nתגובה מהמודל המכוונן:")
    print(response.predictions[0])
except Exception as e:
    print(f"שגיאה בשימוש במודל המכוונן: {e}")

# לאחר סיום העבודה, אל תשכח למחוק את נקודת הקצה והמודל כדי למנוע עלויות מיותרות:
# endpoint.delete()
# tuned_model.delete()
```

### 6. המלצות כלליות

*   **התחל בקטן:** אל תנסה לאמן מודל על אלפי דוגמאות מיד. התחל עם מערך נתונים קטן אך איכותי.
*   **חזור על הפעולה:** כוונון עדין הוא תהליך איטרטיבי. אמן, הערך, התאם את הנתונים או ההיפרפרמטרים, חזור על הפעולה.
*   **ניטור:** עקוב בקפידה אחר מדדי האימון (אובדן) והשתמש במערך נתוני אימות כדי למנוע התאמת יתר.
*   **הערכה:** בדוק תמיד את המודל המכוונן על נתונים שהוא *מעולם לא ראה* במהלך האימון כדי להעריך את יכולת ההכללה שלו.
*   **עלות:** זכור שכוונון עדין ופריסת נקודות קצה הם שירותים בתשלום. קח זאת בחשבון בתקציב שלך.
*   **תיעוד:** עיין תמיד בתיעוד הרשמי של ספק ה-LLM. ממשקי API ויכולות מתפתחים כל הזמן.
