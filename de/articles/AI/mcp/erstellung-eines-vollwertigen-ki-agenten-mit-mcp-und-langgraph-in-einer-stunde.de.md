# Wie man ein neuronales Netz lehrt, mit den H√§nden zu arbeiten: Erstellung eines vollwertigen KI-Agenten mit MCP und LangGraph in einer Stunde


Freunde, Gr√º√üe! Ich hoffe, ihr habt mich vermisst.

In den letzten Monaten habe ich mich intensiv mit der Integration von KI-Agenten in meine eigenen Python-Projekte besch√§ftigt. Dabei habe ich viel praktisches Wissen und Beobachtungen gesammelt, die ich einfach teilen muss. Deshalb kehre ich heute zu Habr zur√ºck ‚Äì mit einem neuen Thema, einer frischen Perspektive und der Absicht, √∂fter zu schreiben.

Auf der Tagesordnung stehen LangGraph und MCP: Tools, mit denen man wirklich n√ºtzliche KI-Agenten erstellen kann.

Wenn wir fr√ºher dar√ºber stritten, welches neuronale Netz besser auf Russisch antwortet, hat sich das Schlachtfeld heute auf angewandtere Aufgaben verlagert: Wer bew√§ltigt die Rolle eines KI-Agenten besser? Welche Frameworks vereinfachen die Entwicklung wirklich? Und wie integriert man all das Gute in ein echtes Projekt?

Doch bevor wir in die Praxis und den Code eintauchen, lassen Sie uns die grundlegenden Konzepte verstehen. Insbesondere zwei Schl√ºsselkonzepte: **KI-Agenten und MCP**. Ohne sie w√§re das Gespr√§ch √ºber LangGraph unvollst√§ndig.

### KI-Agenten in einfachen Worten

KI-Agenten sind nicht nur ‚Äûaufgepumpte‚Äú Chatbots. Sie sind komplexere, autonome Entit√§ten, die zwei entscheidende Merkmale besitzen:

1.  **F√§higkeit zur Interaktion und Koordination**

    Moderne Agenten k√∂nnen Aufgaben in Unteraufgaben zerlegen, andere Agenten aufrufen, externe Daten anfordern und im Team arbeiten. Dies ist kein einzelner Assistent mehr, sondern ein verteiltes System, in dem jede Komponente ihren Beitrag leisten kann.

2.  **Zugriff auf externe Ressourcen**

    Ein KI-Agent ist nicht mehr auf die Grenzen eines Dialogs beschr√§nkt. Er kann auf Datenbanken zugreifen, API-Aufrufe ausf√ºhren, mit lokalen Dateien, vektorbasierten Wissensdatenbanken interagieren und sogar Befehle im Terminal ausf√ºhren. All dies wurde durch das Aufkommen von MCP erm√∂glicht ‚Äì einer neuen Ebene der Integration zwischen Modell und Umgebung.

---

Einfach ausgedr√ºckt: **MCP ist eine Br√ºcke zwischen einem neuronalen Netz und seiner Umgebung**. Es erm√∂glicht dem Modell, den Kontext der Aufgabe zu ‚Äûverstehen‚Äú, auf Daten zuzugreifen, Aufrufe zu t√§tigen und begr√ºndete Aktionen zu formulieren, anstatt nur Textantworten zu geben.

**Stellen wir uns eine Analogie vor:**

*   Sie haben ein **neuronales Netz** ‚Äì es kann Texte argumentieren und generieren.
*   Es gibt **Daten und Tools** ‚Äì Dokumente, APIs, Wissensdatenbanken, Terminal, Code.
*   Und es gibt **MCP** ‚Äì es ist eine Schnittstelle, die es dem Modell erm√∂glicht, mit diesen externen Quellen zu interagieren, als w√§ren sie Teil seiner inneren Welt.

**Ohne MCP:**

Das Modell ist eine isolierte Dialog-Engine. Sie geben ihm Text ‚Äì es antwortet. Und das war's.

**Mit MCP:**

Das Modell wird zu einem vollwertigen **Aufgabenausf√ºhrer**:

*   erh√§lt Zugriff auf Datenstrukturen und APIs;
*   ruft externe Funktionen auf;
*   orientiert sich im aktuellen Zustand des Projekts oder der Anwendung;
*   kann den Kontext im Verlauf des Dialogs speichern, verfolgen und √§ndern;
*   verwendet Erweiterungen wie Suchwerkzeuge, Code-Runner, Vektor-Embedding-Datenbanken usw.

Im technischen Sinne ist **MCP ein Protokoll f√ºr die Interaktion zwischen LLM und seiner Umgebung**, wobei der Kontext in Form von strukturierten Objekten (anstelle von ‚Äûrohem‚Äú Text) bereitgestellt wird und Aufrufe als interaktive Operationen (z. B. Funktionsaufrufe, Tool-Nutzung oder Agentenaktionen) formalisiert werden. Dies ist es, was ein gew√∂hnliches Modell zu einem **echten KI-Agenten** macht, der mehr tun kann, als nur ‚Äûreden‚Äú.

### Und nun ‚Äì zur Sache!

Nachdem wir die grundlegenden Konzepte behandelt haben, ist es logisch zu fragen: ‚ÄûWie setzen wir das alles in Python in die Praxis um?‚Äú

Hier kommt **LangGraph** ins Spiel ‚Äì ein leistungsstarkes Framework zum Erstellen von Zustandsgraphen, Agentenverhalten und Denkketten. Es erm√∂glicht Ihnen, die Interaktionslogik zwischen Agenten, Tools und dem Benutzer zu ‚Äûverkn√ºpfen‚Äú und so eine lebendige KI-Architektur zu schaffen, die sich an Aufgaben anpasst.

In den folgenden Abschnitten werden wir uns ansehen, wie man:

*   einen Agenten von Grund auf neu erstellt;
*   Zust√§nde, √úberg√§nge und Ereignisse erstellt;
*   Funktionen und Tools integriert;
*   und wie dieses gesamte √ñkosystem in einem realen Projekt funktioniert.

### Ein wenig Theorie: Was ist LangGraph?

Bevor wir in die Praxis eintauchen, ein paar Worte zum Framework selbst.

**LangGraph** ist ein Projekt des **LangChain**-Teams, genau derjenigen, die als erste das Konzept der ‚ÄûKetten‚Äú (chains) f√ºr die Interaktion mit LLMs vorgeschlagen haben. W√§hrend zuvor der Schwerpunkt auf linearen oder bedingt verzweigten Pipelines (langchain.chains) lag, setzen die Entwickler nun auf ein **Graphenmodell**, und LangGraph ist das, was sie als neuen ‚ÄûKern‚Äú f√ºr den Aufbau komplexer KI-Systeme empfehlen.

**LangGraph** ist ein Framework zum Erstellen von endlichen Automaten und Zustandsgraphen, wobei jeder **Knoten** einen Teil der Agentenlogik darstellt: einen Modellaufruf, ein externes Tool, eine Bedingung, Benutzereingaben usw.

### Wie es funktioniert: Graphen und Knoten

Konzeptionell basiert LangGraph auf den folgenden Ideen:

*   **Graph** ‚Äì ist eine Struktur, die die m√∂glichen Ausf√ºhrungspfade der Logik beschreibt. Man kann ihn sich wie eine Karte vorstellen: Von einem Punkt kann man je nach Bedingungen oder Ausf√ºhrungsergebnis zu einem anderen wechseln.
*   **Knoten** ‚Äì sind spezifische Schritte innerhalb des Graphen. Jeder Knoten f√ºhrt eine Funktion aus: ruft ein Modell auf, ruft eine externe API auf, pr√ºft eine Bedingung oder aktualisiert einfach den internen Zustand.
*   **√úberg√§nge zwischen Knoten** ‚Äì ist die Routing-Logik: Wenn das Ergebnis des vorherigen Schritts so und so ist, dann gehen Sie dorthin.
*   **Zustand** ‚Äì wird zwischen Knoten √ºbergeben und sammelt alles Notwendige: Verlauf, Zwischenergebnisse, Benutzereingaben, Ergebnisse der Tool-Operationen usw.

So erhalten wir einen **flexiblen Mechanismus zur Steuerung der Agentenlogik**, in dem sowohl einfache als auch sehr komplexe Szenarien beschrieben werden k√∂nnen: Schleifen, Bedingungen, parallele Aktionen, verschachtelte Aufrufe und vieles mehr.

### Warum ist das praktisch?

LangGraph erm√∂glicht es Ihnen, **transparente, reproduzierbare und erweiterbare Logik** zu erstellen:

*   einfach zu debuggen;
*   einfach zu visualisieren;
*   einfach f√ºr neue Aufgaben zu skalieren;
*   einfach externe Tools und MCP-Protokolle zu integrieren.

Im Wesentlichen ist LangGraph das **‚ÄûGehirn‚Äú des Agenten**, wobei jeder Schritt dokumentiert, kontrollierbar und ohne Chaos und ‚ÄûMagie‚Äú ge√§ndert werden kann.

### Nun, das reicht an Theorie!

Man k√∂nnte noch lange √ºber Graphen, Zust√§nde, Logikkomposition und die Vorteile von LangGraph gegen√ºber klassischen Pipelines sprechen. Aber, wie die Praxis zeigt, ist es besser, es einmal im Code zu sehen.

**Es ist Zeit, zur Praxis √ºberzugehen.** Als N√§chstes ‚Äì ein Python-Beispiel: Wir werden einen einfachen, aber n√ºtzlichen KI-Agenten auf Basis von LangGraph erstellen, der externe Tools, Speicher und eigene Entscheidungen verwendet.

### Vorbereitung: Cloud- und lokale neuronale Netze

Um mit der Erstellung von KI-Agenten zu beginnen, ben√∂tigen wir zun√§chst ein **Gehirn** ‚Äì ein Sprachmodell. Hier gibt es zwei Ans√§tze:

*   **Cloud-L√∂sungen verwenden**, bei denen alles ‚Äûout of the box‚Äú fertig ist;
*   oder das **Modell lokal aufsetzen** ‚Äì f√ºr vollst√§ndige Autonomie und Vertraulichkeit.

Betrachten wir beide Optionen.

#### Cloud-Dienste: schnell und bequem

Der einfachste Weg ist, die Leistungsf√§higkeit gro√üer Anbieter zu nutzen: OpenAI, Anthropic, und zu verwenden...

### Wo man Schl√ºssel und Token erh√§lt:

*   **OpenAI** ‚Äì ChatGPT und andere Produkte;
*   **Anthropic** ‚Äì Claude;
*   **OpenRouter.ai** ‚Äì Dutzende von Modellen (ein Token ‚Äì viele Modelle √ºber eine OpenAI-kompatible API);
*   **Amvera Cloud** ‚Äì die M√∂glichkeit, LLAMA mit Rubelzahlung und integriertem Proxying zu OpenAI und Anthropic zu verbinden.

Dieser Weg ist bequem, besonders wenn Sie:

*   keine Infrastruktur konfigurieren m√∂chten;
*   mit Fokus auf Geschwindigkeit entwickeln;
*   mit begrenzten Ressourcen arbeiten.

### Lokale Modelle: volle Kontrolle

Wenn Ihnen **Datenschutz, Offline-Arbeit** wichtig sind oder Sie **vollst√§ndig autonome Agenten** erstellen m√∂chten, ist es sinnvoll, das neuronale Netz lokal bereitzustellen.

**Hauptvorteile:**

*   **Vertraulichkeit** ‚Äì Daten bleiben bei Ihnen;
*   **Offline-Arbeit** ‚Äì n√ºtzlich in isolierten Netzwerken;
*   **Keine Abonnements und Token** ‚Äì nach der Einrichtung kostenlos.

**Nachteile sind offensichtlich:**

*   Ressourcenanforderungen (insbesondere f√ºr den Videospeicher);
*   Die Einrichtung kann Zeit in Anspruch nehmen;
*   Einige Modelle sind ohne Erfahrung schwer bereitzustellen.

Dennoch gibt es Tools, die den lokalen Start erleichtern. Eines der besten heute ist **Ollama**.

### Bereitstellung von lokalem LLM √ºber Ollama + Docker

Wir werden eine lokale Bereitstellung des Qwen 2.5-Modells (qwen2.5:32b) mithilfe eines Docker-Containers und des Ollama-Systems vorbereiten. Dies erm√∂glicht die Integration des neuronalen Netzes mit MCP und dessen Verwendung in Ihren eigenen LangGraph-basierten Agenten.

Sollten die Rechenressourcen Ihres Computers oder Servers f√ºr die Arbeit mit dieser Version des Modells nicht ausreichen, k√∂nnen Sie jederzeit ein weniger ‚Äûressourcenhungriges‚Äú neuronales Netz w√§hlen ‚Äì der Installations- und Startvorgang bleibt √§hnlich.

**Schnelle Installation (Zusammenfassung der Schritte)**

1.  **Docker + Docker Compose installieren**
2.  **Projektstruktur erstellen:**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **`docker-compose.yml` erstellen**
(universelle Option, GPU wird automatisch erkannt)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **Container starten:**
```bash
docker compose up -d
```

5.  **Modell herunterladen:**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **Betrieb √ºber API pr√ºfen:**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Hallo!", "stream": false}'
```
*(Bild mit dem Ergebnis der curl-Befehlsausf√ºhrung)*

7.  **Integration mit Python:**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Erkl√§ren Sie die Quantenverschr√§nkung"))
```
Sie haben jetzt ein vollwertiges lokales LLM, das bereit ist, mit MCP und LangGraph zu arbeiten.

**Was kommt als N√§chstes?**

Wir haben die Wahl zwischen Cloud- und lokalen Modellen, und wir haben gelernt, wie man beide verbindet. Das Interessanteste steht noch bevor ‚Äì die **Erstellung von KI-Agenten auf LangGraph**, die das ausgew√§hlte Modell, den Speicher, die Tools und ihre eigene Logik verwenden.

Kommen wir zum leckersten Teil ‚Äì Code und Praxis!

---

Bevor wir zur Praxis √ºbergehen, ist es wichtig, die Arbeitsumgebung vorzubereiten. Ich gehe davon aus, dass Sie bereits mit den Grundlagen von Python vertraut sind, wissen, was Bibliotheken und Abh√§ngigkeiten sind, und verstehen, warum eine virtuelle Umgebung verwendet werden sollte.

Wenn all dies neu f√ºr Sie ist ‚Äì empfehle ich Ihnen, zuerst einen kurzen Kurs oder Leitfaden zu den Python-Grundlagen zu absolvieren und dann zum Artikel zur√ºckzukehren.

#### Schritt 1: Erstellen einer virtuellen Umgebung

Erstellen Sie eine neue virtuelle Umgebung im Projektordner:
```bash
python -m venv venv
source venv/bin/activate  # f√ºr Linux/macOS
virtualenv\Scripts\activate   # f√ºr Windows
```

#### Schritt 2: Abh√§ngigkeiten installieren

Erstellen Sie eine Datei `requirements.txt` und f√ºgen Sie die folgenden Zeilen hinzu:
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ‚ö†Ô∏è **Aktuelle Versionen sind vom 21. Juli 2025.** Sie k√∂nnen sich seit der Ver√∂ffentlichung ge√§ndert haben ‚Äì **pr√ºfen Sie die Relevanz vor der Installation.**

Installieren Sie dann die Abh√§ngigkeiten:
```bash
pip install -r requirements.txt```

#### Schritt 3: Umgebungsvariablen konfigurieren

Erstellen Sie im Projektstamm eine Datei `.env` und f√ºgen Sie die erforderlichen API-Schl√ºssel hinzu:
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Zweck der Variablen:**

*   **OPENAI_API_KEY** ‚Äì Schl√ºssel f√ºr den Zugriff auf GPT-Modelle von OpenAI;
*   **DEEPSEEK_API_KEY** ‚Äì Schl√ºssel f√ºr die Verwendung von Deepseek-Modellen;
*   **OPENROUTER_API_KEY** ‚Äì einziger Schl√ºssel f√ºr den Zugriff auf viele Modelle √ºber OpenRouter

--- 
Einige MCP-Tools (z.B. `brave-web-search`) ben√∂tigen einen Schl√ºssel, um zu funktionieren. Ohne ihn werden sie einfach nicht aktiviert.

**Was, wenn Sie keine API-Schl√ºssel haben?**

Kein Problem. Sie k√∂nnen die Entwicklung mit einem lokalen Modell (z.B. √ºber Ollama) beginnen, ohne externe Dienste zu verbinden. In diesem Fall m√ºssen Sie die `.env`-Datei √ºberhaupt nicht erstellen.

Fertig! Jetzt haben wir alles, was wir brauchen, um loszulegen ‚Äì eine isolierte Umgebung, Abh√§ngigkeiten und, falls erforderlich, Zugriff auf Cloud-Neuronale Netze und MCP-Integrationen.

Als N√§chstes ‚Äì wir werden unseren LLM-Agenten auf verschiedene Arten starten.

### Einfacher Start von LLM-Agenten √ºber LangGraph: grundlegende Integration

Beginnen wir mit dem Einfachsten: wie man dem zuk√ºnftigen Agenten ein ‚ÄûGehirn‚Äú anschlie√üt. Wir werden die grundlegenden M√∂glichkeiten zum Starten von Sprachmodellen (LLM) mit LangChain analysieren, damit wir im n√§chsten Schritt zur Integration mit LangGraph und zum Aufbau eines vollwertigen KI-Agenten √ºbergehen k√∂nnen.

#### Importe
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` und `load_dotenv()` ‚Äì zum Laden von Variablen aus der `.env`-Datei.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` ‚Äì Wrapper zum Verbinden von Sprachmodellen √ºber LangChain.

> üí° Wenn Sie alternative Ans√§tze zur Arbeit mit Konfigurationen verwenden (z.B. Pydantic Settings), k√∂nnen Sie `load_dotenv()` durch Ihre √ºbliche Methode ersetzen.

#### Laden von Umgebungsvariablen
```python
load_dotenv()
```
Dadurch werden alle Variablen aus `.env` geladen, einschlie√ülich der Schl√ºssel f√ºr den Zugriff auf OpenAI, DeepSeek, OpenRouter und andere APIs.

#### Einfache Funktionen zum Abrufen von LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
Wenn die Variable `OPENAI_API_KEY` korrekt gesetzt ist, ersetzt LangChain sie automatisch ‚Äì die explizite Angabe `api_key=...` ist hier optional.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
√Ñhnlich, aber mit dem `ChatDeepSeek`-Wrapper.

**OpenRouter (und andere kompatible APIs)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Merkmale:**

*   `ChatOpenAI` wird verwendet, obwohl das Modell nicht von OpenAI stammt ‚Äì weil OpenRouter dasselbe Protokoll verwendet.
*   `base_url` ist obligatorisch: Es verweist auf die OpenRouter-API.
*   Das Modell `moonshotai/kimi-k2:free` wurde zum Zeitpunkt der Erstellung als eine der ausgewogensten Optionen in Bezug auf Qualit√§t und Geschwindigkeit ausgew√§hlt.
*   Der `OpenRouter`-API-Schl√ºssel muss explizit √ºbergeben werden ‚Äì die automatische Substitution funktioniert hier nicht.

#### Mini-Test: √úberpr√ºfung des Modellbetriebs
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Wer bist du?")
    print(response.content)
```
*(Bild mit dem Ausf√ºhrungsergebnis: `Ich bin ein KI-Assistent, erstellt von Moonshot AI...`)*

Wenn alles richtig konfiguriert ist, erhalten Sie eine aussagekr√§ftige Antwort vom Modell. Herzlichen Gl√ºckwunsch ‚Äì der erste Schritt ist getan!

### Aber das ist noch kein Agent

Im aktuellen Stadium haben wir LLM verbunden und einen einfachen Aufruf get√§tigt. Dies √§hnelt eher einem Konsolen-Chatbot als einem KI-Agenten.

**Warum?**

*   Wir schreiben **synchronen, linearen Code** ohne Zustandslogik oder Ziele.
*   Der Agent trifft keine Entscheidungen, speichert keinen Kontext und verwendet keine Tools.
*   MCP und LangGraph sind noch nicht involviert.

**Was kommt als N√§chstes?**

Als N√§chstes werden wir einen **vollwertigen KI-Agenten** mit **LangGraph** implementieren ‚Äì zun√§chst ohne MCP, um uns auf die Architektur, Zust√§nde und Logik des Agenten selbst zu konzentrieren.

Tauchen wir ein in die echte Agentenmechanik. Los geht's!

### Job-Klassifizierungsagent: von der Theorie zur Praxis

...Konzepte von LangGraph in der Praxis und ein n√ºtzliches Tool f√ºr HR-Plattformen und Freelance-B√∂rsen erstellen.

#### Agentenaufgabe

Unser Agent nimmt eine Textbeschreibung einer Vakanz oder Dienstleistung als Eingabe entgegen und f√ºhrt eine dreistufige Klassifizierung durch:

1.  **Jobtyp**: Projektarbeit oder Festanstellung
2.  **Berufskategorie**: aus √ºber 45 vordefinierten Spezialisierungen
3.  **Suchtyp**: ob eine Person Arbeit sucht oder einen Ausf√ºhrenden sucht

Das Ergebnis wird in einem strukturierten JSON-Format mit einem Konfidenzwert f√ºr jede Klassifizierung zur√ºckgegeben.

#### üìà Agentenarchitektur auf LangGraph

Nach den Prinzipien von LangGraph erstellen wir einen **Zustandsgraphen** aus vier Knoten:

- Eingabebeschreibung
- ‚Üì
- Jobtyp-Klassifizierungsknoten
- ‚Üì
- Kategorie-Klassifizierungsknoten
- ‚Üì
- Suchtyp-Bestimmungsknoten
- ‚Üì
- Konfidenzberechnungsknoten
- ‚Üì
- JSON-Ergebnis

Jeder Knoten ist eine **spezialisierte Funktion**, die:

*   Den aktuellen Agentenzustand empf√§ngt
*   Ihren Teil der Analyse durchf√ºhrt
*   Den Zustand aktualisiert und weitergibt

#### Zustandsverwaltung

Wir definieren die **Speicherstruktur des Agenten** √ºber `TypedDict`:

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """Agentenzustand zum Speichern von Informationen √ºber den Klassifizierungsprozess"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

Dies ist der **Arbeitsspeicher des Agenten** ‚Äì alles, was er w√§hrend der Analyse speichert und ansammelt. √Ñhnlich wie ein menschlicher Experte den Aufgabenkontext bei der Analyse eines Dokuments im Kopf beh√§lt.

Schauen wir uns den vollst√§ndigen Code an und konzentrieren uns dann auf die Hauptpunkte.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Kategorien von Berufen
CATEGORIES = [
    "2D-Animator", "3D-Animator", "3D-Modellierer",
    "Business Analyst", "Blockchain-Entwickler", ...
]

class JobType(Enum):
    PROJECT = "Projektarbeit"
    PERMANENT = "Festanstellung"

class SearchType(Enum):
    LOOKING_FOR_WORK = "Arbeitssuche"
    LOOKING_FOR_PERFORMER = "Suche nach Ausf√ºhrendem"

class State(TypedDict):
    """Agentenzustand zum Speichern von Informationen √ºber den Klassifizierungsprozess"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Asynchroner Agent zur Klassifizierung von Vakanzen und Dienstleistungen"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Agenteninitialisierung"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Erstellt den Workflow des Agenten basierend auf LangGraph"""
        workflow = StateGraph(State)
        
        # Knoten zum Graphen hinzuf√ºgen
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # Reihenfolge der Knotenausf√ºhrung definieren
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """Knoten zur Bestimmung des Jobtyps: Projekt oder Festanstellung"""
        # ... (Implementierung folgt)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """Knoten zur Bestimmung der Berufskategorie"""
        # ... (Implementierung folgt)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """Knoten zur Bestimmung des Suchtyps"""
        # ... (Implementierung folgt)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """Knoten zur Berechnung des Konfidenzniveaus in der Klassifizierung"""
        # ... (Implementierung folgt)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Findet die n√§chstgelegene Kategorie aus der Liste der verf√ºgbaren"""
        # ... (Implementierung folgt)

    async def classify(self, description: str) -> Dict[str, Any]:
        """Hauptmethode zur Klassifizierung von Vakanzen/Dienstleistungen"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Workflow ausf√ºhren
        result = await self.workflow.ainvoke(initial_state)
        
        # Endg√ºltige JSON-Antwort formatieren
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Demonstration des Agentenbetriebs"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "Ben√∂tigt Python-Entwickler zur Erstellung einer Webanwendung in Django. Festanstellung.",
        "Suche Auftr√§ge zur Erstellung von Logos und Corporate Identity. Ich arbeite in Adobe Illustrator.",
        "Ben√∂tige einen 3D-Animator f√ºr ein kurzfristiges Projekt zur Erstellung eines Werbespots.",
        "Lebenslauf: erfahrener Marketingexperte, suche Remote-Arbeit im Bereich Digitalmarketing",
        "Suchen React-Frontend-Entwickler f√ºr unser Team auf Festanstellungsbasis"
    ]
    
    print("ü§ñ Demonstration des Betriebs des Vakanzklassifizierungsagenten\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Test {i}: ---")
        print(f"Beschreibung: {description}")
        try:
            result = await agent.classify(description)
            print("Klassifizierungsergebnis:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå Fehler: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...der Rest des Codes mit den Methodenimplementierungen wurde im Artikel vorgestellt...)*

### Hauptvorteile der Architektur
1.  **Modularit√§t** ‚Äì jeder Knoten l√∂st eine Aufgabe, leicht separat zu testen und zu verbessern
2.  **Erweiterbarkeit** ‚Äì neue Funktionen werden deklarativ hinzugef√ºgt
3.  **Transparenz** ‚Äì der gesamte Entscheidungsprozess ist dokumentiert und nachvollziehbar
4.  **Leistung** ‚Äì asynchrone Verarbeitung mehrerer Anfragen
5.  **Zuverl√§ssigkeit** ‚Äì integrierte Fehlerbehandlung und Wiederherstellung

### Echter Nutzen
Ein solcher Agent kann verwendet werden in:
*   **HR-Plattformen** zur automatischen Kategorisierung von Lebensl√§ufen und Vakanzen
*   **Freelance-B√∂rsen** zur Verbesserung der Suche und Empfehlungen
*   **Internen Systemen** von Unternehmen zur Bearbeitung von Antr√§gen und Projekten
*   **Analytischen L√∂sungen** zur Arbeitsmarktforschung

### MCP in Aktion: Erstellen eines Agenten mit Dateisystem und Websuche
Nachdem wir uns mit den Grundprinzipien von LangGraph vertraut gemacht und einen einfachen Klassifizierungsagenten erstellt haben, erweitern wir seine F√§higkeiten, indem wir ihn √ºber MCP mit der Au√üenwelt verbinden.

Jetzt werden wir einen vollwertigen KI-Assistenten erstellen, der Folgendes kann:
*   Mit dem Dateisystem arbeiten (Dateien lesen, erstellen, √§ndern)
*   Relevante Informationen im Internet suchen
*   Den Dialogkontext speichern
*   Fehler behandeln und sich von Ausf√§llen erholen

#### Von der Theorie zu echten Tools
Erinnern Sie sich, wie wir am Anfang des Artikels dar√ºber gesprochen haben, dass **MCP eine Br√ºcke zwischen einem neuronalen Netz und seiner Umgebung ist**? Jetzt werden Sie dies in der Praxis sehen. Unser Agent erh√§lt Zugriff auf **echte Tools**:
```
# Dateisystem-Tools
- read_file ‚Äì Dateien lesen
- write_file ‚Äì Dateien schreiben und erstellen
- list_directory ‚Äì Ordnerinhalte anzeigen
- create_directory ‚Äì Ordner erstellen

# Websuch-Tools
- brave_web_search ‚Äì im Internet suchen
- get_web_content ‚Äì Seiteninhalte abrufen
```
Dies ist kein ‚ÄûSpielzeug‚Äú-Agent mehr ‚Äì es ist ein **Arbeitswerkzeug**, das echte Probleme l√∂sen kann.

#### üìà Architektur: vom Einfachen zum Komplexen

**1. Konfiguration als Basis f√ºr Stabilit√§t**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Vereinfachte KI-Agentenkonfiguration"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Konfigurationsvalidierung"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Pfad existiert nicht: {self.filesystem_path}")
```
**Warum ist das wichtig?** Im Gegensatz zum Klassifizierungsbeispiel interagiert der Agent hier mit externen Systemen. Ein Fehler im Dateipfad oder ein fehlender API-Schl√ºssel ‚Äì und der gesamte Agent funktioniert nicht mehr. **Die Validierung beim Start** spart Stunden beim Debuggen.

**2. Modellfabrik: Flexibilit√§t der Wahl**
```python
def create_model(config: AgentConfig):
    """Erstellt ein Modell gem√§√ü der Konfiguration"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... andere Anbieter
```
Ein Code ‚Äì viele Modelle. M√∂chten Sie ein kostenloses lokales Modell? Verwenden Sie Ollama. Ben√∂tigen Sie maximale Genauigkeit? Wechseln Sie zu GPT-4. Ist Geschwindigkeit wichtig? Probieren Sie DeepSeek. Der Code bleibt derselbe.

**3. MCP-Integration: Verbindung zur realen Welt**
```python
async def _init_mcp_client(self):
    """MCP-Client-Initialisierung"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
Hier findet die Schl√ºsselarbeit von MCP statt: Wir verbinden externe MCP-Server mit dem Agenten, die eine Reihe von Tools und Funktionen bereitstellen. Der Agent wiederum erh√§lt nicht nur einzelne Funktionen, sondern ein umfassendes Kontextverst√§ndnis, wie er mit dem Dateisystem und dem Internet arbeiten kann.

#### Fehlertoleranz
In der realen Welt geht alles kaputt: Das Netzwerk ist nicht verf√ºgbar, Dateien sind gesperrt, API-Schl√ºssel sind abgelaufen. Unser Agent ist darauf vorbereitet:
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
Der `@retry_on_failure`-Decorator wiederholt Operationen bei tempor√§ren Fehlern automatisch. Der Benutzer wird nicht einmal bemerken, dass etwas schief gelaufen ist.

### Zusammenfassung: von der Theorie zur Praxis von KI-Agenten

Heute haben wir den Weg von grundlegenden Konzepten zur Erstellung funktionierender KI-Agenten zur√ºckgelegt. Fassen wir zusammen, was wir gelernt und erreicht haben.

**Was wir gemeistert haben**

**1. Grundlegende Konzepte**
*   Den Unterschied zwischen Chatbots und echten KI-Agenten verstanden
*   Die Rolle von **MCP (Model Context Protocol)** als Br√ºcke zwischen dem Modell und der Au√üenwelt verstanden
*   Die Architektur von **LangGraph** zum Aufbau komplexer Agentenlogik studiert

**2. Praktische F√§higkeiten**
*   Eine Arbeitsumgebung mit Unterst√ºtzung f√ºr Cloud- und lokale Modelle konfiguriert
*   Einen **Klassifizierungsagenten** mit einer asynchronen Architektur und Zustandsverwaltung erstellt
*   Einen **MCP-Agenten** mit Zugriff auf das Dateisystem und die Websuche erstellt

**3. Architekturmuster**
*   Modulare Konfiguration und Modellfabriken beherrscht
*   Fehlerbehandlung und **Wiederholungsmechanismen** f√ºr produktionsreife L√∂sungen implementiert

### Hauptvorteile des Ansatzes
**LangGraph + MCP** bieten uns:
*   **Transparenz** ‚Äì jeder Schritt des Agenten ist dokumentiert und nachvollziehbar
*   **Erweiterbarkeit** ‚Äì neue Funktionen werden deklarativ hinzugef√ºgt
*   **Zuverl√§ssigkeit** ‚Äì integrierte Fehlerbehandlung und Wiederherstellung
*   **Flexibilit√§t** ‚Äì Unterst√ºtzung f√ºr mehrere Modelle und Anbieter out of box

### Fazit

KI-Agenten sind keine futuristische Fiktion, sondern **reale Technologie von heute**. Mit LangGraph und MCP k√∂nnen wir Systeme erstellen, die spezifische Gesch√§ftsprobleme l√∂sen, Routinen automatisieren und neue M√∂glichkeiten er√∂ffnen.

**Das Wichtigste ist, anzufangen.** Nehmen Sie den Code aus den Beispielen, passen Sie ihn an Ihre Aufgaben an, experimentieren Sie. Jedes Projekt ist eine neue Erfahrung und ein Schritt zur Meisterschaft im Bereich der KI-Entwicklung.

Viel Erfolg bei Ihren Projekten!

---
*Tags: python, ki, mcp, langchain, ki-assistent, ollama, ki-agenten, local llm, langgraph, mcp-server*
*Hubs: Blog des Unternehmens Amvera, Verarbeitung nat√ºrlicher Sprache, K√ºnstliche Intelligenz, Python, Programmierung*