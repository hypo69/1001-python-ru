# Wie man einem neuronalen Netzwerk beibringt, mit den H√§nden zu arbeiten: Erstellung eines vollwertigen KI-Agenten mit MCP und LangGraph in einer Stunde

Freunde, Gr√º√üe! Ich hoffe, ihr habt mich vermisst.

In den letzten Monaten habe ich mich intensiv mit der Integration von KI-Agenten in meine eigenen Python-Projekte besch√§ftigt. Dabei habe ich viele praktische Kenntnisse und Beobachtungen gesammelt, die ich einfach teilen muss. Deshalb kehre ich heute zu Habr zur√ºck ‚Äì mit einem neuen Thema, einer frischen Perspektive und der Absicht, h√§ufiger zu schreiben.

Auf der Tagesordnung stehen LangGraph und MCP: Tools, mit denen Sie wirklich n√ºtzliche KI-Agenten erstellen k√∂nnen.

Wenn wir uns fr√ºher dar√ºber gestritten haben, welches neuronale Netzwerk auf Russisch besser antwortet, hat sich das Schlachtfeld heute auf anwendungsbezogenere Aufgaben verlagert: Wer bew√§ltigt die Rolle eines KI-Agenten besser? Welche Frameworks vereinfachen die Entwicklung wirklich? Und wie integriert man all das Gute in ein reales Projekt?

Doch bevor wir in die Praxis und den Code eintauchen, kl√§ren wir die grundlegenden Konzepte. Insbesondere zwei Schl√ºsselkonzepte: **KI-Agenten und MCP**. Ohne sie w√§re das Gespr√§ch √ºber LangGraph unvollst√§ndig.

### KI-Agenten einfach erkl√§rt

KI-Agenten sind nicht nur ‚Äûaufgepumpte‚Äú Chatbots. Sie stellen komplexere, autonome Entit√§ten dar, die zwei entscheidende Merkmale besitzen:

1.  **F√§higkeit zur Interaktion und Koordination**

    Moderne Agenten k√∂nnen Aufgaben in Unteraufgaben zerlegen, andere Agenten aufrufen, externe Daten anfordern, im Team arbeiten. Dies ist kein einzelner Assistent mehr, sondern ein verteiltes System, in dem jede Komponente ihren Beitrag leisten kann.

2.  **Zugriff auf externe Ressourcen**

    Ein KI-Agent ist nicht mehr auf die Grenzen eines Dialogs beschr√§nkt. Er kann auf Datenbanken zugreifen, API-Aufrufe t√§tigen, mit lokalen Dateien, vektorbasierten Wissensdatenbanken interagieren und sogar Befehle im Terminal ausf√ºhren. All dies wurde durch die Entstehung von MCP m√∂glich ‚Äì einer neuen Ebene der Integration zwischen Modell und Umgebung.

---

Einfach ausgedr√ºckt: **MCP ist eine Br√ºcke zwischen einem neuronalen Netzwerk und seiner Umgebung**. Es erm√∂glicht dem Modell, den Kontext der Aufgabe zu ‚Äûverstehen‚Äú, auf Daten zuzugreifen, Aufrufe zu t√§tigen und begr√ºndete Aktionen zu bilden, anstatt nur Textantworten auszugeben.

**Stellen wir uns eine Analogie vor:**

*   Sie haben ein **neuronales Netzwerk** ‚Äì es kann denken und Texte generieren.
*   Es gibt **Daten und Tools** ‚Äì Dokumente, APIs, Wissensdatenbanken, Terminal, Code.
*   Und es gibt **MCP** ‚Äì es ist eine Schnittstelle, die es dem Modell erm√∂glicht, mit diesen externen Quellen zu interagieren, als w√§ren sie Teil seiner inneren Welt.

**Ohne MCP:**

Modell ‚Äì ist eine isolierte Dialog-Engine. Sie geben ihm Text ‚Äì es antwortet. Und das war's.

**Mit MCP:**

Das Modell wird zu einem vollwertigen **Aufgabenausf√ºhrer**:

*   erh√§lt Zugriff auf Datenstrukturen und APIs;
*   ruft externe Funktionen auf;
*   navigiert im aktuellen Zustand des Projekts oder der Anwendung;
*   kann den Kontext w√§hrend des Dialogs speichern, verfolgen und √§ndern;
*   verwendet Erweiterungen wie Suchwerkzeuge, Code-Runner, Vektor-Embedding-Datenbanken usw.

Im technischen Sinne ist **MCP ein Protokoll f√ºr die Interaktion zwischen einem LLM und seiner Umgebung**, wobei der Kontext als strukturierte Objekte (anstelle von ‚Äûrohem‚Äú Text) bereitgestellt wird und Aufrufe als interaktive Operationen (z. B. Funktionsaufrufe, Tool-Nutzung oder Agentenaktionen) formatiert werden. Dies ist es, was ein gew√∂hnliches Modell in einen **echten KI-Agenten** verwandelt, der mehr als nur ‚Äûreden‚Äú kann.

### Und nun ‚Äì zur Sache!

Nachdem wir die grundlegenden Konzepte behandelt haben, ist es logisch zu fragen: ‚ÄûWie setzen wir das alles in der Praxis in Python um?‚Äú

Hier kommt **LangGraph** ins Spiel ‚Äì ein leistungsstarkes Framework zum Erstellen von Zustandsgraphen, Agentenverhalten und Denkketten. Es erm√∂glicht Ihnen, die Logik der Interaktion zwischen Agenten, Tools und dem Benutzer zu ‚Äûverkn√ºpfen‚Äú und eine lebendige KI-Architektur zu schaffen, die sich an Aufgaben anpasst.

In den folgenden Abschnitten werden wir uns ansehen, wie man:

*   einen Agenten von Grund auf neu erstellt;
*   Zust√§nde, √úberg√§nge und Ereignisse erstellt;
*   Funktionen und Tools integriert;
*   und wie dieses gesamte √ñkosystem in einem realen Projekt funktioniert.

### Ein wenig Theorie: Was ist LangGraph

Bevor wir in die Praxis eintauchen, ein paar Worte zum Framework selbst.

**LangGraph** ist ein Projekt des **LangChain**-Teams, genau derjenigen, die als erste das Konzept der ‚ÄûKetten‚Äú (chains) f√ºr die Interaktion mit LLMs vorgeschlagen haben. Wenn der Schwerpunkt fr√ºher auf linearen oder bedingt verzweigten Pipelines (langchain.chains) lag, setzen die Entwickler nun auf ein **Graphenmodell**, und LangGraph ist das, was sie als neuen ‚ÄûKern‚Äú f√ºr den Aufbau komplexer KI-Systeme empfehlen.

**LangGraph** ist ein Framework zum Erstellen von endlichen Automaten und Zustandsgraphen, wobei jeder **Knoten** einen Teil der Agentenlogik darstellt: einen Modellaufruf, ein externes Tool, eine Bedingung, Benutzereingaben usw.

### Wie es funktioniert: Graphen und Knoten

Konzeptionell basiert LangGraph auf den folgenden Ideen:

*   **Graph** ‚Äì ist eine Struktur, die die m√∂glichen Ausf√ºhrungspfade der Logik beschreibt. Man kann ihn sich wie eine Karte vorstellen: Von einem Punkt kann man je nach Bedingungen oder Ausf√ºhrungsergebnis zu einem anderen wechseln.
*   **Knoten** ‚Äì sind spezifische Schritte innerhalb des Graphen. Jeder Knoten f√ºhrt eine Funktion aus: ruft ein Modell auf, ruft eine externe API auf, pr√ºft eine Bedingung oder aktualisiert einfach den internen Zustand.
*   **√úberg√§nge zwischen Knoten** ‚Äì ist die Routing-Logik: Wenn das Ergebnis des vorherigen Schritts so und so ist, dann gehen Sie dorthin.
*   **Zustand** ‚Äì wird zwischen Knoten √ºbergeben und sammelt alles Notwendige: Verlauf, Zwischenergebnisse, Benutzereingaben, Ergebnisse von Tool-Operationen usw.

So erhalten wir einen **flexiblen Mechanismus zur Steuerung der Agentenlogik**, in dem sowohl einfache als auch sehr komplexe Szenarien beschrieben werden k√∂nnen: Schleifen, Bedingungen, parallele Aktionen, verschachtelte Aufrufe und vieles mehr.

### Warum ist das praktisch?

LangGraph erm√∂glicht den Aufbau einer **transparenten, reproduzierbaren und erweiterbaren Logik**:

*   einfach zu debuggen;
*   einfach zu visualisieren;
*   einfach an neue Aufgaben anzupassen;
*   einfach externe Tools und MCP-Protokolle zu integrieren.

Im Wesentlichen ist LangGraph das **‚ÄûGehirn‚Äú des Agenten**, wobei jeder Schritt dokumentiert, kontrollierbar und ohne Chaos und ‚ÄûMagie‚Äú ge√§ndert werden kann.

### Und nun ‚Äì genug Theorie!

Wir k√∂nnten noch lange √ºber Graphen, Zust√§nde, Logikkomposition und die Vorteile von LangGraph gegen√ºber klassischen Pipelines sprechen. Aber, wie die Praxis zeigt, ist es besser, es einmal im Code zu sehen.

**Zeit, zur Praxis √ºberzugehen.** Als N√§chstes ‚Äì ein Python-Beispiel: Wir erstellen einen einfachen, aber n√ºtzlichen KI-Agenten auf Basis von LangGraph, der externe Tools, Speicher und eigene Entscheidungen verwendet.

### Vorbereitung: Cloud- und lokale neuronale Netze

Um mit der Erstellung von KI-Agenten zu beginnen, ben√∂tigen wir zun√§chst ein **Gehirn** ‚Äì ein Sprachmodell. Hier gibt es zwei Ans√§tze:

*   **Cloud-L√∂sungen verwenden**, bei denen alles ‚Äûout of the box‚Äú fertig ist;
*   oder **das Modell lokal hochfahren** ‚Äì f√ºr vollst√§ndige Autonomie und Vertraulichkeit.

Betrachten wir beide Optionen.

#### Cloud-Dienste: schnell und bequem

Der einfachste Weg ist, die Leistung gro√üer Anbieter zu nutzen: OpenAI, Anthropic und zu verwenden...

### Woher man Schl√ºssel und Token bekommt:

*   **OpenAI** ‚Äì ChatGPT und andere Produkte;
*   **Anthropic** ‚Äì Claude;
*   **OpenRouter.ai** ‚Äì Dutzende von Modellen (ein Token ‚Äì viele Modelle √ºber eine OpenAI-kompatible API);
*   **Amvera Cloud** ‚Äì M√∂glichkeit, LLAMA mit Rubelzahlung und integriertem Proxy zu OpenAI und Anthropic zu verbinden.

Dieser Weg ist bequem, besonders wenn Sie:

*   keine Infrastruktur konfigurieren m√∂chten;
*   mit Fokus auf Geschwindigkeit entwickeln;
*   mit begrenzten Ressourcen arbeiten.

#### Lokale Modelle: volle Kontrolle

Wenn Ihnen **Datenschutz, Offline-Arbeit** wichtig sind oder Sie **vollst√§ndig autonome Agenten** erstellen m√∂chten, ist es sinnvoll, das neuronale Netzwerk lokal bereitzustellen.

**Hauptvorteile:**

*   **Vertraulichkeit** ‚Äì Daten bleiben bei Ihnen;
*   **Offline-Arbeit** ‚Äì n√ºtzlich in isolierten Netzwerken;
*   **Keine Abonnements und Token** ‚Äì nach der Einrichtung kostenlos.

**Nachteile sind offensichtlich:**

*   Ressourcenanforderungen (insbesondere f√ºr den Videospeicher);
*   Die Einrichtung kann Zeit in Anspruch nehmen;
*   Einige Modelle sind ohne Erfahrung schwer bereitzustellen.

Dennoch gibt es Tools, die den lokalen Start erleichtern. Eines der besten heute ist **Ollama**.

### Bereitstellung eines lokalen LLM √ºber Ollama + Docker

Wir werden eine lokale Bereitstellung des Qwen 2.5-Modells (qwen2.5:32b) unter Verwendung eines Docker-Containers und des Ollama-Systems vorbereiten. Dies erm√∂glicht die Integration des neuronalen Netzwerks mit MCP und dessen Verwendung in Ihren eigenen Agenten auf Basis von LangGraph.

Sollten die Rechenressourcen Ihres Computers oder Servers f√ºr die Arbeit mit dieser Version des Modells nicht ausreichen, k√∂nnen Sie jederzeit ein weniger ‚Äûressourcenhungriges‚Äú neuronales Netzwerk w√§hlen ‚Äì der Installations- und Startvorgang bleibt √§hnlich.

**Schnelle Installation (Zusammenfassung der Schritte)**

1.  **Docker + Docker Compose installieren**
2.  **Projektstruktur erstellen:**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **`docker-compose.yml` erstellen**
(universelle Option, GPU wird automatisch erkannt)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **Container starten:**
```bash
docker compose up -d
```

5.  **Modell herunterladen:**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **API-Betrieb pr√ºfen:**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Hallo!", "stream": false}'
```
*(Bild mit dem Ergebnis der Befehlsausf√ºhrung)*

7.  **Integration mit Python:**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Erkl√§ren Sie die Quantenverschr√§nkung"))
```
Jetzt haben Sie ein vollwertiges lokales LLM, das bereit ist, mit MCP und LangGraph zu arbeiten.

**Was kommt als N√§chstes?**

Wir haben die Wahl zwischen Cloud- und lokalen Modellen, und wir haben gelernt, wie man beide verbindet. Das Interessanteste steht noch bevor ‚Äì **die Erstellung von KI-Agenten auf LangGraph**, die das ausgew√§hlte Modell, den Speicher, die Tools und ihre eigene Logik verwenden.

**Kommen wir zum leckersten Teil ‚Äì Code und Praxis!**

---

Bevor wir zur Praxis √ºbergehen, ist es wichtig, die Arbeitsumgebung vorzubereiten. Ich gehe davon aus, dass Sie bereits mit den Grundlagen von Python vertraut sind, wissen, was Bibliotheken und Abh√§ngigkeiten sind, und verstehen, warum eine virtuelle Umgebung verwendet werden sollte.

Wenn all dies neu f√ºr Sie ist ‚Äì empfehle ich Ihnen, zuerst einen kurzen Kurs oder Leitfaden zu den Python-Grundlagen zu absolvieren und dann zum Artikel zur√ºckzukehren.

#### Schritt 1: Erstellen einer virtuellen Umgebung

Erstellen Sie einen neuen virtuellen Umgebung im Projektordner:
```bash
python -m venv venv
source venv/bin/activate  # f√ºr Linux/macOS
virtualenv\Scripts\activate   # f√ºr Windows
```

#### Schritt 2: Installieren von Abh√§ngigkeiten

Erstellen Sie eine Datei `requirements.txt` und f√ºgen Sie die folgenden Zeilen hinzu:
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ‚ö†Ô∏è **Aktuelle Versionen sind vom 21. Juli 2025.** Sie k√∂nnten sich seit der Ver√∂ffentlichung ge√§ndert haben ‚Äì **pr√ºfen Sie die Relevanz vor der Installation.**

Installieren Sie dann die Abh√§ngigkeiten:
```bash
pip install -r requirements.txt```

#### Schritt 3: Konfigurieren von Umgebungsvariablen

Erstellen Sie eine `.env`-Datei im Projektstammverzeichnis und f√ºgen Sie die erforderlichen API-Schl√ºssel hinzu:
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Zweck der Variablen:**

*   **OPENAI_API_KEY** ‚Äì Schl√ºssel f√ºr den Zugriff auf GPT-Modelle von OpenAI;
*   **DEEPSEEK_API_KEY** ‚Äì Schl√ºssel f√ºr die Verwendung von Deepseek-Modellen;
*   **OPENROUTER_API_KEY** ‚Äì Einzelschl√ºssel f√ºr den Zugriff auf viele Modelle √ºber OpenRouter

---

Einige MCP-Tools (z. B. `brave-web-search`) ben√∂tigen einen Schl√ºssel, um zu funktionieren. Ohne ihn werden sie einfach nicht aktiviert.

**Was ist, wenn Sie keine API-Schl√ºssel haben?**

Kein Problem. Sie k√∂nnen die Entwicklung mit einem lokalen Modell (z. B. √ºber Ollama) beginnen, ohne externe Dienste zu verbinden. In diesem Fall muss die `.env`-Datei √ºberhaupt nicht erstellt werden.

Fertig! Jetzt haben wir alles, was wir brauchen, um zu beginnen ‚Äì eine isolierte Umgebung, Abh√§ngigkeiten und, falls erforderlich, Zugriff auf Cloud-neuronale Netze und MCP-Integrationen.

Als N√§chstes ‚Äì lassen Sie uns unseren LLM-Agenten auf verschiedene Arten ausf√ºhren.

### Einfacher Start von LLM-Agenten √ºber LangGraph: grundlegende Integration

Beginnen wir mit dem Einfachsten: wie man dem zuk√ºnftigen Agenten ‚Äûein Gehirn anschlie√üt‚Äú. Wir werden die grundlegenden M√∂glichkeiten zum Starten von Sprachmodellen (LLMs) mit LangChain analysieren, um im n√§chsten Schritt zur Integration mit LangGraph und zum Aufbau eines vollwertigen KI-Agenten √ºberzugehen.

#### Importe
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` und `load_dotenv()` ‚Äì zum Laden von Variablen aus der `.env`-Datei.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` ‚Äì Wrapper zum Verbinden von Sprachmodellen √ºber LangChain.

> üí° Wenn Sie alternative Ans√§tze zur Arbeit mit Konfigurationen verwenden (z. B. Pydantic Settings), k√∂nnen Sie `load_dotenv()` durch Ihre √ºbliche Methode ersetzen.

#### Laden von Umgebungsvariablen
```python
load_dotenv()
```
Dadurch werden alle Variablen aus `.env` geladen, einschlie√ülich der Schl√ºssel f√ºr den Zugriff auf die OpenAI-, DeepSeek-, OpenRouter-APIs und andere.

#### Einfache Funktionen zum Abrufen von LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
Wenn die Variable `OPENAI_API_KEY` korrekt gesetzt ist, ersetzt LangChain sie automatisch ‚Äì die explizite Angabe von `api_key=...` ist hier optional.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
√Ñhnlich, aber wir verwenden den `ChatDeepSeek`-Wrapper.

**OpenRouter (und andere kompatible APIs)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Merkmale:**

*   `ChatOpenAI` wird verwendet, obwohl das Modell nicht von OpenAI stammt ‚Äì weil OpenRouter dasselbe Protokoll verwendet.
*   `base_url` ist erforderlich: Es verweist auf die OpenRouter-API.
*   Das Modell `moonshotai/kimi-k2:free` wurde zum Zeitpunkt der Erstellung als eine der ausgewogensten Optionen in Bezug auf Qualit√§t und Geschwindigkeit ausgew√§hlt.
*   Der `OpenRouter`-API-Schl√ºssel muss explizit √ºbergeben werden ‚Äì die automatische Ersetzung funktioniert hier nicht.

#### Mini-Test: √úberpr√ºfung des Modellbetriebs
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Wer bist du?")
    print(response.content)
```
*(Bild mit Ausf√ºhrungsergebnis: `Ich bin ein KI-Assistent, erstellt von Moonshot AI...`)*

Wenn alles richtig konfiguriert ist, erhalten Sie eine sinnvolle Antwort vom Modell. Herzlichen Gl√ºckwunsch ‚Äì der erste Schritt ist getan!

### Aber das ist noch kein Agent

Im aktuellen Stadium haben wir LLM verbunden und einen einfachen Aufruf get√§tigt. Das √§hnelt eher einem Konsolen-Chatbot als einem KI-Agenten.

**Warum?**

*   Wir schreiben **synchronen, linearen Code** ohne Zustandslogik oder Ziele.
*   Der Agent trifft keine Entscheidungen, speichert keinen Kontext und verwendet keine Tools.
*   MCP und LangGraph sind noch nicht beteiligt.

**Was kommt als N√§chstes?**

Als N√§chstes werden wir einen **vollwertigen KI-Agenten** mit **LangGraph** implementieren ‚Äì zun√§chst ohne MCP, um uns auf die Architektur, Zust√§nde und Logik des Agenten selbst zu konzentrieren.

Tauchen wir ein in die echte Agentenmechanik. Los geht's!

### Stellenklassifizierungsagent: von der Theorie zur Praxis

...Konzepte von LangGraph in der Praxis und ein n√ºtzliches Tool f√ºr HR-Plattformen und Freelance-B√∂rsen erstellen.

#### Agentenaufgabe

Unser Agent nimmt eine Textbeschreibung einer Vakanz oder Dienstleistung als Eingabe entgegen und f√ºhrt eine dreistufige Klassifizierung durch:

1.  **Art der Arbeit**: Projektarbeit oder Festanstellung
2.  **Berufskategorie**: aus √ºber 45 vordefinierten Spezialit√§ten
3.  **Art der Suche**: ob eine Person Arbeit sucht oder einen Ausf√ºhrenden sucht

Das Ergebnis wird in einem strukturierten JSON-Format mit einem Konfidenzwert f√ºr jede Klassifizierung zur√ºckgegeben.

#### üìà Agentenarchitektur auf LangGraph

Nach den Prinzipien von LangGraph erstellen wir einen **Zustandsgraphen** aus vier Knoten:

- Eingabebeschreibung
- ‚Üì
- Knoten zur Klassifizierung der Arbeitsart
- ‚Üì
- Knoten zur Klassifizierung der Kategorie
- ‚Üì
- Knoten zur Bestimmung der Suchart
- ‚Üì
- Knoten zur Konfidenzberechnung
- ‚Üì
- JSON-Ergebnis

Jeder Knoten ist eine **spezialisierte Funktion**, die:

*   Den aktuellen Zustand des Agenten empf√§ngt
*   Ihren Teil der Analyse durchf√ºhrt
*   Den Zustand aktualisiert und weitergibt

#### Zustandsverwaltung

Definieren Sie die **Speicherstruktur des Agenten** √ºber `TypedDict`:

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """Agentenzustand zum Speichern von Informationen √ºber den Klassifizierungsprozess"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

Dies ist der **Arbeitsspeicher des Agenten** ‚Äì alles, was er w√§hrend der Analyse speichert und ansammelt. √Ñhnlich wie ein menschlicher Experte den Aufgabenkontext bei der Analyse eines Dokuments im Kopf beh√§lt.

Schauen wir uns den vollst√§ndigen Code an und konzentrieren uns dann auf die Hauptpunkte.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Berufskategorien
CATEGORIES = [
    "2D-Animator", "3D-Animator", "3D-Modellierer",
    "Business Analyst", "Blockchain-Entwickler", ...
]

class JobType(Enum):
    PROJECT = "Projektarbeit"
    PERMANENT = "Festanstellung"

class SearchType(Enum):
    LOOKING_FOR_WORK = "Arbeitssuchend"
    LOOKING_FOR_PERFORMER = "Suche nach einem Ausf√ºhrenden"

class State(TypedDict):
    """Agentenzustand zum Speichern von Informationen √ºber den Klassifizierungsprozess"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Asynchroner Agent zur Klassifizierung von Vakanzen und Dienstleistungen"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Agenteninitialisierung"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Erstellt den Workflow des Agenten basierend auf LangGraph"""
        workflow = StateGraph(State)
        
        # Knoten zum Graphen hinzuf√ºgen
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # Reihenfolge der Knotenausf√ºhrung definieren
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """Knoten zur Bestimmung der Arbeitsart: Projekt oder Festanstellung"""
        # ... (Implementierung folgt)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """Knoten zur Bestimmung der Berufskategorie"""
        # ... (Implementierung folgt)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """Knoten zur Bestimmung der Suchart"""
        # ... (Implementierung folgt)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """Knoten zur Berechnung des Konfidenzniveaus in der Klassifizierung"""
        # ... (Implementierung folgt)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Findet die n√§chstgelegene Kategorie aus der Liste der verf√ºgbaren"""
        # ... (Implementierung folgt)

    async def classify(self, description: str) -> Dict[str, Any]:
        """Hauptmethode zur Klassifizierung von Vakanzen/Dienstleistungen"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Workflow starten
        result = await self.workflow.ainvoke(initial_state)
        
        # Endg√ºltige JSON-Antwort erstellen
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Demonstration des Agentenbetriebs"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "Ben√∂tigt Python-Entwickler zur Erstellung einer Webanwendung mit Django. Festanstellung.",
        "Suche Auftr√§ge zur Erstellung von Logos und Corporate Identity. Ich arbeite in Adobe Illustrator.",
        "Ben√∂tige 3D-Animator f√ºr ein kurzfristiges Projekt zur Erstellung eines Werbespots.",
        "Lebenslauf: erfahrener Marketingexperte, suche Remote-Arbeit im Bereich Digitalmarketing",
        "Suchen React-Frontend-Entwickler f√ºr unser Team auf Festanstellung"
    ]
    
    print("ü§ñ Demonstration des Betriebs des Stellenklassifizierungsagenten\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Test {i}: ---")
        print(f"Beschreibung: {description}")
        try:
            result = await agent.classify(description)
            print("Klassifizierungsergebnis:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå Fehler: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...der Rest des Codes mit der Implementierung der Methoden wurde im Artikel vorgestellt...)*

### Hauptvorteile der Architektur
1.  **Modularit√§t** ‚Äì jeder Knoten l√∂st eine Aufgabe, leicht separat zu testen und zu verbessern
2.  **Erweiterbarkeit** ‚Äì neue Analyseknoten k√∂nnen hinzugef√ºgt werden, ohne bestehende zu √§ndern
3.  **Transparenz** ‚Äì der gesamte Entscheidungsprozess ist dokumentiert und nachvollziehbar
4.  **Leistung** ‚Äì asynchrone Verarbeitung mehrerer Anfragen
5.  **Zuverl√§ssigkeit** ‚Äì integrierte Fallback-Mechanismen und Fehlerbehandlung

### Echter Nutzen
Ein solcher Agent kann verwendet werden in:
*   **HR-Plattformen** zur automatischen Kategorisierung von Lebensl√§ufen und Vakanzen
*   **Freelance-B√∂rsen** zur Verbesserung der Suche und Empfehlungen
*   **Internen Systemen** von Unternehmen zur Bearbeitung von Anfragen und Projekten
*   **Analytischen L√∂sungen** zur Arbeitsmarktforschung

### MCP in Aktion: Erstellung eines Agenten mit Dateisystem und Websuche
Nachdem wir die Grundprinzipien von LangGraph verstanden und einen einfachen Klassifizierungsagenten erstellt haben, erweitern wir seine F√§higkeiten, indem wir ihn √ºber MCP mit der Au√üenwelt verbinden.

Jetzt werden wir einen vollwertigen KI-Assistenten erstellen, der Folgendes kann:
*   Mit dem Dateisystem arbeiten (Dateien lesen, erstellen, √§ndern)
*   Relevante Informationen im Internet suchen
*   Den Kontext des Dialogs speichern
*   Fehler behandeln und sich von Ausf√§llen erholen

#### Von der Theorie zu realen Tools
Erinnern Sie sich, wie wir am Anfang des Artikels dar√ºber gesprochen haben, dass **MCP eine Br√ºcke zwischen einem neuronalen Netzwerk und seiner Umgebung** ist? Jetzt werden Sie dies in der Praxis sehen. Unser Agent erh√§lt Zugriff auf **reale Tools**:
```
# Dateisystem-Tools
- read_file ‚Äì Dateien lesen
- write_file ‚Äì Dateien schreiben und erstellen
- list_directory ‚Äì Ordnerinhalte anzeigen
- create_directory ‚Äì Ordner erstellen

# Websuch-Tools
- brave_web_search ‚Äì im Internet suchen
- get_web_content ‚Äì Seiteninhalte abrufen
```
Dies ist kein ‚ÄûSpielzeug‚Äú-Agent mehr ‚Äì es ist ein **Arbeitswerkzeug**, das reale Probleme l√∂sen kann.

#### üìà Architektur: vom Einfachen zum Komplexen

**1. Konfiguration als Basis der Stabilit√§t**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Vereinfachte KI-Agentenkonfiguration"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Konfigurationsvalidierung"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Pfad existiert nicht: {self.filesystem_path}")
```
**Warum ist das wichtig?** Im Gegensatz zum Klassifizierungsbeispiel interagiert der Agent hier mit externen Systemen. Ein Fehler im Dateipfad oder ein fehlender API-Schl√ºssel ‚Äì und der gesamte Agent funktioniert nicht mehr. **Die Validierung beim Start** spart Stunden beim Debuggen.

**2. Modellfabrik: flexible Auswahl**
```python
def create_model(config: AgentConfig):
    """Erstellt ein Modell gem√§√ü der Konfiguration"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... andere Anbieter
```
Ein Code ‚Äì viele Modelle. M√∂chten Sie ein kostenloses lokales Modell? Verwenden Sie Ollama. Ben√∂tigen Sie maximale Genauigkeit? Wechseln Sie zu GPT-4. Ist Geschwindigkeit wichtig? Probieren Sie DeepSeek. Der Code bleibt derselbe.

**3. MCP-Integration: Verbindung zur realen Welt**
```python
async def _init_mcp_client(self):
    """MCP-Client-Initialisierung"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
Hier findet die entscheidende MCP-Arbeit statt: Wir verbinden externe MCP-Server mit dem Agenten, die eine Reihe von Tools und Funktionen bereitstellen. Der Agent erh√§lt dabei nicht nur einzelne Funktionen, sondern ein vollst√§ndiges kontextuelles Verst√§ndnis daf√ºr, wie er mit dem Dateisystem und dem Internet arbeiten kann.

#### Fehlertoleranz
In der realen Welt geht alles kaputt: Das Netzwerk ist nicht verf√ºgbar, Dateien sind blockiert, API-Schl√ºssel sind abgelaufen. Unser Agent ist darauf vorbereitet:
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
Der `@retry_on_failure`-Decorator wiederholt Operationen bei tempor√§ren Fehlern automatisch. Der Benutzer wird nicht einmal bemerken, dass etwas schiefgelaufen ist.

### Ergebnisse: von der Theorie zur Praxis der KI-Agenten

Heute haben wir den Weg von den grundlegenden Konzepten bis zur Erstellung funktionierender KI-Agenten zur√ºckgelegt. Fassen wir zusammen, was wir gelernt und erreicht haben.

**Was wir gemeistert haben**

**1. Grundlegende Konzepte**
*   Den Unterschied zwischen Chatbots und echten KI-Agenten verstanden
*   Die Rolle von **MCP (Model Context Protocol)** als Br√ºcke zwischen dem Modell und der Au√üenwelt verstanden
*   Die Architektur von **LangGraph** zum Aufbau komplexer Agentenlogik studiert

**2. Praktische F√§higkeiten**
*   Eine Arbeitsumgebung mit Unterst√ºtzung f√ºr Cloud- und lokale Modelle konfiguriert
*   Einen **Klassifizierungsagenten** mit asynchroner Architektur und Zustandsverwaltung erstellt
*   Einen **MCP-Agenten** mit Zugriff auf das Dateisystem und die Websuche erstellt

**3. Architekturmuster**
*   Modulare Konfiguration und Modellfabriken beherrscht
*   Fehlerbehandlung und **Wiederholungsmechanismen** f√ºr produktionsreife L√∂sungen implementiert

### Hauptvorteile des Ansatzes
**LangGraph + MCP** bieten uns:
*   **Transparenz** ‚Äì jeder Agentenschritt ist dokumentiert und nachvollziehbar
*   **Erweiterbarkeit** ‚Äì neue Funktionen werden deklarativ hinzugef√ºgt
*   **Zuverl√§ssigkeit** ‚Äì integrierte Fehlerbehandlung und Wiederherstellung
*   **Flexibilit√§t** ‚Äì Unterst√ºtzung f√ºr mehrere Modelle und Anbieter out of box

### Fazit

KI-Agenten sind keine futuristische Fiktion, sondern **reale Technologie von heute**. Mit LangGraph und MCP k√∂nnen wir Systeme erstellen, die spezifische Gesch√§ftsprobleme l√∂sen, Routinen automatisieren und neue M√∂glichkeiten er√∂ffnen.

**Das Wichtigste ist, anzufangen.** Nehmen Sie den Code aus den Beispielen, passen Sie ihn an Ihre Aufgaben an, experimentieren Sie. Jedes Projekt ist eine neue Erfahrung und ein Schritt zur Meisterschaft im Bereich der KI-Entwicklung.

Viel Erfolg bei Ihren Projekten!

---
*Tags: python, ki, mcp, langchain, ki-assistent, ollama, ki-agenten, local llm, langgraph, mcp-server*
*Hubs: Amvera Unternehmensblog, Natural Language Processing, K√ºnstliche Intelligenz, Python, Programmierung*
