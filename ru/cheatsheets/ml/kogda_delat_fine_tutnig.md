# Fine Tuning

Файн-тюнинг стоит делать не "как можно чаще", а **когда это действительно необходимо и оправдано** с точки зрения наличия новых данных, ожидаемого улучшения качества и затрат ресурсов (времени и денег). Вместо частого файн-тюнинга на малых объемах данных иногда эффективнее использовать другие подходы, например, Retrieval-Augmented Generation (RAG), где модель получает актуальную информацию в момент запроса из внешней базы знаний.
Технически, **нет строгих ограничений со стороны платформ** (как Google Vertex AI, например) на то, как часто вы можете запускать процесс файн-тюнинга (fine-tuning), кроме общих квот на использование ресурсов и вашего бюджета. Вы можете запустить новый процесс файн-тюнинга хоть через час после предыдущего, если есть такая необходимость и ресурсы.

Однако, **с практической точки зрения**, частота файн-тюнинга определяется следующими факторами:

1.  **Наличие новых данных:** Самая главная причина для повторного файн-тюнинга – появление **значительного объема новых, качественных данных**, которые могут улучшить производительность модели для вашей задачи. Файн-тюнинг на небольшом количестве новых данных может не дать заметного эффекта или даже ухудшить модель (переобучение на новых данных).
2.  **Стоимость:** Каждый процесс файн-тюнинга требует вычислительных ресурсов (GPU/TPU) и, соответственно, стоит денег. Частый файн-тюнинг может быть весьма затратным.
3.  **Время:** Процесс файн-тюнинга занимает время – от часов до дней, в зависимости от размера модели, объема данных и конфигурации. Плюс время на подготовку данных и оценку результата.
4.  **Необходимость:** Действительно ли нужно обновлять модель?
    *   Изменилась ли задача, для которой вы используете модель?
    *   Ухудшилось ли качество ответов существующей дообученной модели со временем (например, из-за изменений во внешнем мире, которые отражены в новых данных)?
    *   Даст ли добавление новых данных ощутимый прирост качества, оправдывающий затраты?
5.  **Оценка (Evaluation):** После каждого файн-тюнинга необходимо проводить тщательную оценку модели на тестовом наборе данных, чтобы убедиться, что новая версия действительно лучше предыдущей и не произошло "катастрофического забывания" (когда модель ухудшает свои способности в областях, на которых ее не дообучали в этот раз).

**Когда обычно делают повторный файн-тюнинг:**

*   При накоплении **существенного** блока новых релевантных данных (например, данные за последний квартал, месяц и т.д.).
*   При **значительных изменениях** в предметной области или требованиях к задаче.
*   Когда **мониторинг производительности** показывает заметное снижение качества текущей модели.
*   Периодически (например, раз в квартал или полгода), если данные поступают непрерывно и есть бюджет/ресурсы на поддержание модели в актуальном состоянии.

Структура файла для файн-тюнинга (fine-tuning) зависит от **платформы**, которую вы используете (например, Google Vertex AI, OpenAI API, Hugging Face, etc.) и **типа задачи**, для которой вы дообучаете модель.

Однако, наиболее распространенным форматом является **JSON Lines (JSONL)**. В этом формате каждая строка файла представляет собой отдельный, валидный JSON-объект, содержащий один пример для обучения.

**Общая идея структуры для генеративных моделей (как Gemini):**

Каждый JSON-объект (каждая строка в файле .jsonl) обычно содержит пары "вход" -> "ожидаемый выход".

**Примеры форматов данных для файн-тюнинга:**

**1. Задачи инструктивного типа / чата / вопрос-ответ:**

* **Вариант 1 (Простой):**

```jsonl
{"prompt": "Переведи на английский: Как дела?", "completion": "How are you?"}
{"prompt": "Напиши короткое стихотворение о весне.", "completion": "Звенит капель, ручьи бегут,\nПодснежник первый тут как тут.\nПроснулся лес, проснулся луг,\nВесна пришла, мой милый друг!"}
{"prompt": "Столица Франции?", "completion": "Париж"}
{"instruction": "Напиши код на Python для вычисления факториала числа 5.", "output_text": "def factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\nprint(factorial(5))"}
{"instruction": "Суммируй числа от 1 до 10.", "output_text": "55"}
{"input_text": "Объясни концепцию квантовой запутанности простым языком.", "output_text": "Представь две монеты, которые каким-то образом связаны. Если одна выпадает орлом, другая автоматически выпадает решкой, независимо от расстояния между ними."}
```

* **Вариант 2 (Более структурированный, для чатов):**

```jsonl
{"messages": [{"role": "user", "content": "Привет! Как дела?"}, {"role": "assistant", "content": "Привет! У меня всё отлично. Чем могу помочь?"}]}
{"messages": [{"role": "user", "content": "Объясни теорию относительности простыми словами."}, {"role": "assistant", "content": "Представь, что время и пространство - это как эластичная ткань..."}]}
{"messages": [{"role": "user", "content": "Расскажи анекдот про программистов."}, {"role": "assistant": "Почему программисты путают Рождество и Хэллоуин? Потому что Dec 25 == Oct 31"}]}
{"messages":[{"role":"user","content":"Какой сегодня день?"},{"role":"assistant","content":"Сегодня [дата]"} ]}
```

**2. Задачи генерации текста (продолжение текста):**

* **Вариант 1 (Объединенный):**

```jsonl
{"text": "Однажды зимним вечером, когда снег падал крупными хлопьями, и ветер выл, как голодный волк,  в маленьком домике на опушке леса сидела старушка и вязала.  Вдруг раздался стук в дверь…  Это был…"}
{"text": "Рецепт яблочного пирога. Нам понадобятся: 200 г муки, 150 г сахара, 2 яйца, 100 г сливочного масла, 3 средних яблока… Способ приготовления: 1. Разогреть духовку до 180 градусов. 2. Взбить яйца с сахаром…  Затем…"}
{"text": "В начале было Слово, и Слово было у Бога, и Слово было Бог. Оно было в начале у Бога. Все чрез Него начало быть, и без Него ничто не начало быть, что начало быть. В Нем была жизнь, и жизнь была свет человеков... "}
```

* **Вариант 2 (Префикс/Продолжение):**

```jsonl
{"prompt": "Рецепт борща. Ингредиенты:", "completion": " свекла, капуста, картофель, морковь, лук, томатная паста..."}
{"prompt": "The quick brown fox jumps over the lazy dog. This sentence is a pangram because it ", "completion": "contains every letter of the alphabet."}
{"prompt":"Once upon a time, in a land far, far away, there lived a princess who...", "completion": "was known for her kindness and her love of adventure."}
```

**3. Задачи классификации:**

```jsonl
{"text": "Этот ресторан предлагает превосходную кухню и отличное обслуживание.", "label": "положительный"}
{"text": "К сожалению, заказ был доставлен с опозданием на два часа.", "label": "отрицательный"}
{"text": "Продукт соответствует описанию, но цена немного высоковата.", "label": "нейтральный"}
{"text": "Фильм скучный и неинтересный, я бы не рекомендовала его к просмотру.", "label": "отрицательный"}
{"text": "Книга написана хорошим языком и имеет увлекательный сюжет, рекомендую к прочтению.", "label": "положительный"}
{"text":"Этот отзыв очень полезен, спасибо автору!","label":"положительный"}
```

**Важные моменты:**

1.  **Обязательно смотрите документацию той платформы, где вы проводите файн-тюнинг!** Названия полей (`input_text`, `output_text`, `prompt`, `completion`, `text`, `label` и т.д.) могут отличаться. Например, Google Vertex AI часто использует `input_text` и `output_text` для supervised tuning.
2.  **Формат Файла:** Убедитесь, что это именно JSON Lines (.jsonl), а не обычный JSON-массив. Каждая строка - отдельный JSON.
3.  **Кодировка:** Обычно требуется UTF-8.
4.  **Самое важное – это качество и релевантность ваших данных**. Они должны быть чистыми, точными и соответствовать той задаче, которую вы хотите, чтобы модель решала.
5.  **Размер Данных:** Количество примеров влияет на качество файн-тюнинга. Обычно требуются сотни или тысячи примеров, в зависимости от сложности задачи и модели.

**Пример файла `my_finetuning_data.jsonl` для Google Vertex AI (инструктивный тюнинг):**

```jsonl
{"input_text": "Напиши слоган для кофейни 'Утренний Луч'", "output_text": "Утренний Луч: Начни свой день с идеальной чашки!"}
{"input_text": "Кратко перескажи сюжет 'Гамлета'", "output_text": "Датский принц Гамлет мстит своему дяде Клавдию за убийство отца и узурпацию трона, что приводит к трагической гибели большинства главных героев."}
{"input_text": "Составь список из 5 идей для летнего отпуска в России", "output_text": "1. Поход по Алтаю\n2. Отдых на Байкале\n3. Путешествие по Золотому Кольцу\n4. Пляжный отдых в Сочи или Крыму\n5. Сплав по рекам Карелии"}
```




**JSONL (JSON Lines) Формат:**
Каждая строка в файле является валидным JSON объектом. Объекты не разделяются запятыми, каждая строка - отдельная запись.

```json
{"поле1": "значение1", "поле2": "значение2", ...}
{"поле1": "значение3", "поле2": "значение4", ...}
...
```

**1. Формат "Prompt / Completion" (Запрос / Завершение)**

Это классический формат, особенно популярный для моделей завершения текста или простых инструкций.

*   `prompt`: Входной текст, который подается модели (инструкция, вопрос, начало текста).
*   `completion` (или `output`, `response`): Ожидаемый ответ или продолжение текста, которое модель должна сгенерировать.

**Структура:**

```json
{"prompt": "Ваш входной текст или инструкция...", "completion": "Ожидаемый результат модели..."}
{"prompt": "Переведи на английский: Привет, мир!", "completion": "Hello, world!"}
{"prompt": "Столица Франции?", "completion": "Париж"}
{"prompt": "Напиши короткий стих о коте.", "completion": "Пушистый зверь на солнце спит,\nМурлычет тихо и сопит.\nЗеленый глаз лениво щурит,\nМечтая рыбу утащить."}
```

**2. Формат "Instruction / Input / Output" (Инструкция / Входные данные / Выходные данные)**

Этот формат более структурирован для задач следования инструкциям, где инструкция отделена от конкретных данных, к которым она применяется.

*   `instruction`: Описание задачи, которую должна выполнить модель.
*   `input` (опционально): Конкретные данные или контекст для инструкции. Может быть пустым, если инструкция самодостаточна.
*   `output` (или `response`): Ожидаемый результат.

**Структура:**

```json
{"instruction": "Переведи следующий текст на английский.", "input": "Привет, мир!", "output": "Hello, world!"}
{"instruction": "Ответь на вопрос, используя предоставленный контекст.", "input": "Контекст: Париж - столица Франции. Вопрос: Какая столица Франции?", "output": "Париж"}
{"instruction": "Напиши короткий стих о коте.", "input": "", "output": "Пушистый зверь на солнце спит,\nМурлычет тихо и сопит.\nЗеленый глаз лениво щурит,\nМечтая рыбу утащить."}
{"instruction": "Определи тональность текста: позитивная, негативная или нейтральная.", "input": "Этот фильм был просто ужасен.", "output": "негативная"}
```

**3. Формат "Messages" (Сообщения) для чат-моделей**

Этот формат используется для файн-тюнинга диалоговых моделей (например, GPT-3.5/4 через API OpenAI, или модели в Hugging Face с использованием Chat Templates). Он представляет диалог как последовательность сообщений с указанием роли отправителя.

*   `messages`: Список словарей, где каждый словарь представляет одно сообщение.
*   `role`: Роль отправителя (`system`, `user`, `assistant`).
    *   `system`: Задает общее поведение или контекст для ассистента.
    *   `user`: Сообщение от пользователя.
    *   `assistant`: Ожидаемый ответ модели (ассистента).
*   `content`: Текст сообщения.

**Структура:**

```json
{"messages": [{"role": "system", "content": "Ты - полезный ассистент."}, {"role": "user", "content": "Привет, как дела?"}, {"role": "assistant", "content": "Привет! У меня все отлично, я готова помочь. Чем могу быть полезна?"}]}
{"messages": [{"role": "user", "content": "Какая столица Франции?"}, {"role": "assistant", "content": "Столица Франции - Париж."}]}
{"messages": [{"role": "user", "content": "Напиши функцию на Python для сложения двух чисел."}, {"role": "assistant", "content": "Конечно! Вот простая функция:\n```python\ndef add_numbers(a, b):\n  return a + b\n```"}]}
```

**4. Формат "Text" (Текст) для Causal LM**

Иногда для обучения моделей автодополнения (Causal Language Models) используется простой формат, где каждая строка содержит цельный кусок текста, который модель учится продолжать. Часто это комбинация инструкции/вопроса и ответа в одном текстовом поле.

*   `text`: Полный текст примера (включая и "запрос", и "ответ").

**Структура:**

```json
{"text": "Инструкция: Переведи на английский: Привет, мир!\nОтвет: Hello, world!"}
{"text": "Вопрос: Столица Франции?\nОтвет: Париж"}
{"text": "Диалог:\nПользователь: Привет!\nАссистент: Привет! Чем могу помочь?\nПользователь: Расскажи шутку.\nАссистент: Почему программисты путают Хэллоуин и Рождество? Потому что OCT 31 == DEC 25!"}
```

**Важно:**

*   **Проверьте документацию:** Всегда сверяйтесь с документацией конкретной платформы или библиотеки (OpenAI, Hugging Face TRL, Axolotl, Llama-Factory, Vertex AI и т.д.), чтобы узнать точные требования к формату данных для файн-тюнинга. Названия полей (`prompt`, `completion`, `instruction`, `output`, `text`, `messages`) могут отличаться.
*   **Консистентность:** Используйте выбранный формат консистентно во всем файле данных.
*   **Качество данных:** Качество и количество данных в вашем JSONL файле напрямую влияют на результат файн-тюнинга.

Какой формат вам больше подходит, зависит от вашей задачи и инструмента, который вы будете использовать. Для простых задач часто достаточно формата 1 или 4, для инструкций - 2, для диалогов - 3.