<h2>Шпаргалка. Персонализация LLM: промпты, тонкая настройка моделей, примеры кода.</h2>
<p>В этой статье:</p>
<ol>
<li>Как создается "эффект памяти" в LLM (краткий обзор).</li>
<li>Зачем и когда нужна тонкая настройка (Fine-tuning) модели.</li>
<li>Когда тонкая настройка – не лучшее решение.</li>
<li>Подготовка данных.</li>
<li>Примеры тонкой настройки для <strong>OpenAI (GPT)</strong>, <strong>Google (Gemini)</strong> и <strong>Anthropic (Claude)</strong> (отличается).</li>
</ol>
<h3>1. Как LLM "помнит" и "подстраивается": Иллюзия контекста</h3>
<p>Прежде чем говорить о тонкой настройке, важно понять, как LLM вообще удается создавать ощущение персонализации.
Это важно, чтобы не бросаться в дорогостоящую тонкую настройку, если задача решается более простыми способами:</p>
<ul>
<li>Через <strong>Контекстное окно (Short-Term Memory):</strong> В рамках одного диалога вы отправляете модели не только новый вопрос, но и <strong>всю или часть предыдущей переписки</strong>. Модель обрабатывает весь этот текст как единый "контекст". Именно благодаря этому она "помнит" предыдущие реплики и продолжает мысль. Ограничение здесь — длина контекстного окна (количество токенов).</li>
<li>Составление <strong>Системных инструкций (System Prompt):</strong> Вы можете задать модели роль, тон, правила поведения в начале каждого диалога. Например: "Ты – эксперт по Python, отвечай кратко".</li>
<li>Включение в запрос нескольких примеров желаемого поведения <strong>Few-Shot Learning:</strong> (input/output pairs) позволяет модели "научиться" этому паттерну прямо в рамках текущего запроса.</li>
<li><strong>Управление состоянием на стороне приложения:</strong> Самый мощный способ. Приложение (которое обращается к API) может хранить информацию о пользователе (предпочтения, историю, данные профиля) и динамически добавлять её в промпт перед отправкой модели.</li>
</ul>
<h3>2.</h3>
<p>Тонкая настройка – это процесс дообучения уже готовой базовой LLM на вашем собственном, специфическом наборе данных. Это позволяет модели:</p>
<ul>
<li><strong>Адаптировать стиль и тон:</strong> Модель будет говорить "на вашем языке" – будь то строгий научный, дружелюбный маркетинговый или сленг определенного сообщества.</li>
<li><strong>Следовать специфическим инструкциям и форматам:</strong> Если вам нужны ответы в строго определенной JSON-структуре, или всегда с определенным набором полей.</li>
<li><strong>Понимать домен-специфический язык:</strong> Обучение на вашей внутренней документации или отраслевых текстах поможет модели лучше справляться с терминологией вашей ниши.</li>
<li><strong>Улучшить производительность на узких задачах:</strong> Для определенных типов запросов (например, классификация отзывов, генерация кода в специфическом фреймворке) тонкая настройка может дать более точные и релевантные ответы, чем базовая модель.</li>
<li><strong>Сократить длину промптов:</strong> Если модель уже "знает" желаемое поведение благодаря настройке, вам не нужно каждый раз напоминать ей об этом в промпте, что экономит токены и снижает задержку.</li>
</ul>
<h3>3.</h3>
<p>Тонкая настройка – мощный, но не универсальный инструмент. Не стоит использовать её, если:</p>
<ul>
<li><strong>Модель должна получать доступ к новым знаниям:</strong> Тонкая настройка изменяет веса модели, но не "загружает" в неё новые факты в реальном времени. Если ваша задача – отвечать на вопросы по постоянно меняющейся базе знаний (документы компании, последние новости), лучше использовать <strong>Retrieval Augmented Generation (RAG)</strong>. Здесь базовая модель получает контекст из вашей базы данных <em>во время выполнения запроса</em>.</li>
<li><strong>Простая задача решается промпт-инжинирингом:</strong> Всегда начинайте с максимально эффективного промпт-инжиниринга. Если задача решается простыми инструкциями и few-shot примерами, тонкая настройка избыточна и более затратна.</li>
<li><strong>У вас нет достаточного количества высококачественных данных:</strong> Плохие данные = плохая настроенная модель.</li>
</ul>
<h3>4. Подготовка данных.</h3>
<p>Качество и количество ваших данных критически важны. Модель учится на ваших примерах, поэтому они должны быть точными, разнообразными и последовательными.</p>
<ul>
<li><strong>Формат:</strong> Чаще всего JSON Lines (<code>.jsonl</code>) или CSV (<code>.csv</code>).</li>
<li><strong>Структура данных:</strong> Зависит от задачи.
<ul>
<li><strong>Instruction Tuning (Инструкция-Ответ):</strong> Подходит для обобщенных задач типа "вопрос-ответ", перефразирования, суммаризации.
<pre class="line-numbers"><code class="language-json">{"input_text": "Перефразируй предложение: 'Технология ИИ стремительно развивается.'", "output_text": "Искусственный интеллект демонстрирует стремительный прогресс."} 
{"input_text": "Назови столицу Франции.", "output_text": "Столица Франции — Париж."}</code></pre>
</li>
<li><strong>Chat Tuning (Чат):</strong> Идеально для обучения модели ведению диалога в определенной роли или стиле.
<pre class="line-numbers"><code class="language-json">{"messages": [{"author": "user", "content": "Привет! Что порекомендуешь на ужин?"}, {"author": "model", "content": "Добрый вечер! Сегодня отличный день для пасты Карбонара, или, если вы предпочитаете что-то легкое, салат Цезарь."}]} 
{"messages": [{"author": "user", "content": "Расскажи про новые фичи в Python 3.12."}, {"author": "model", "content": "В Python 3.12 появились f-строки с отступами, новый синтаксис для универсальных генериков и улучшенная обработка ошибок Unicode."}]}</code></pre>
</li>
</ul>
</li>
<li><strong>Количество:</strong> Минимум 100-200 качественных примеров, но чем больше, тем лучше (тысячи для серьезных задач).</li>
<li><strong>Качество:</strong> Точность, последовательность стиля, тона, формата во всем наборе данных.</li>
</ul>
<h3>5. Модели и платформы</h3>
<h4>5.1. OpenAI</h4>
<p>OpenAI предоставляет довольно простой и интуитивно понятный API для тонкой настройки.</p>
<p><strong>Процесс:</strong></p>
<ol>
<li>Подготовка данных в формате JSONL, где каждая строка — это объект с массивом <code>messages</code> (как в Chat Completions API).</li>
<li>Загрузка файла данных через API.</li>
<li>Создание задания на тонкую настройку, указав загруженный файл и базовую модель.</li>
<li>Отслеживание прогресса.</li>
<li>Использование новой, настроенной модели по её ID.</li>
</ol>
<p><strong>Пример данных (файл <code>training_data.jsonl</code>):</strong></p>
<pre class="line-numbers"><code class="language-json">{"messages": [{"role": "system", "content": "Ты - дружелюбный ассистент по поддержке клиентов IT-компании."}, {"role": "user", "content": "Мой интернет не работает."}, {"role": "assistant", "content": "Ох, извините за неудобства! Давайте попробуем разобраться. Каким образом вы подключены к интернету – по Wi-Fi или через кабель?"}]} 
{"messages": [{"role": "system", "content": "Ты - дружелюбный ассистент по поддержке клиентов IT-компании."}, {"role": "user", "content": "Как я могу сбросить пароль?"}, {"role": "assistant", "content": "Конечно! Чтобы сбросить пароль, перейдите, пожалуйста, на страницу входа и нажмите 'Забыли пароль?' под полем для ввода. Далее следуйте инструкциям на экране."}</code></pre>
<p><strong>Пример Python кода:</strong></p>
<p>Предварительно установите: <code>pip install openai</code></p>
<pre class="line-numbers"><code class="language-python">import openai
from openai import OpenAI
import os

# Установите ваш API-ключ OpenAI. Рекомендуется использовать переменную окружения.
# os.environ["OPENAI_API_KEY"] = "sk-..."
client = OpenAI()

# 1. Загрузка файла данных
try:
    file_response = client.files.create(
        file=open("training_data.jsonl", "rb"),
        purpose="fine-tune"
    )
    file_id = file_response.id
    print(f"Файл успешно загружен. ID файла: {file_id}")
except openai.APIStatusError as e:
    print(f"Ошибка загрузки файла: {e.status_code} - {e.response}")
    exit()

# 2. Создание задания на тонкую настройку
try:
    ft_job_response = client.fine_tuning.jobs.create(
        training_file=file_id,
        model="gpt-3.5-turbo" # Можно указать конкретную версию, например, "gpt-3.5-turbo-0125"
    )
    job_id = ft_job_response.id
    print(f"Задание на тонкую настройку создано. ID задания: {job_id}")
    print("Отслеживайте статус задания через API или в OpenAI Playground.")
except openai.APIStatusError as e:
    print(f"Ошибка создания задания: {e.status_code} - {e.response}")
    exit()

# Пример отслеживания статуса и получения имени модели (выполнять после создания задания):
# # job_id = "ftjob-..." # Замените на ID вашего задания
# # job_status = client.fine_tuning.jobs.retrieve(job_id)
# # print(f"Текущий статус задания: {job_status.status}")
# # if job_status.status == "succeeded":
# #     fine_tuned_model_name = job_status.fine_tuned_model
# #     print(f"Имя настроенной модели: {fine_tuned_model_name}")

# 3. Использование настроенной модели (после её готовности)
# # Замените на реальное имя вашей модели, полученное после успешной тонкой настройки
# # fine_tuned_model_name = "ft:gpt-3.5-turbo-0125:my-org::abcd123"

# # if 'fine_tuned_model_name' in locals() and fine_tuned_model_name:
# #     try:
# #         response = client.chat.completions.create(
# #             model=fine_tuned_model_name,
# #             messages=[
# #                 {"role": "user", "content": "У меня проблема с логином."}
# #             ]
# #         )
# #         print("\nОтвет настроенной модели:")
# #         print(response.choices[0].message.content)
# #     except openai.APIStatusError as e:
# #         print(f"Ошибка при использовании модели: {e.status_code} - {e.response}")</code></pre>
<h4>5.2. Anthropic</h4>
<p>Anthropic <strong>не предоставляет публичного API для тонкой настройки своих моделей Claude 3 (Opus, Sonnet, Haiku) в том же смысле, как это делает OpenAI или Google.</strong></p>
<p>Anthropic сфокусирован на создании очень мощных базовых моделей, которые, по их утверждениям, отлично работают с продвинутым промпт-инжинирингом и RAG-паттернами, минимизируя необходимость в тонкой настройке для большинства случаев.
Для крупных корпоративных клиентов или партнеров могут существовать программы по созданию "кастомных" моделей или специализированных интеграций, но это не является общедоступной функцией тонкой настройки через API.</p>
<p>Если вы работаете с Claude 3, ваш основной упор должен быть на:</p>
<ul>
<li><strong>Качественный промпт-инжиниринг:</strong> Экспериментируйте с системными инструкциями, few-shot примерами, четким форматированием запросов. Claude известен своей способностью строго следовать инструкциям, особенно в XML-тегах.</li>
<li><strong>RAG-системы:</strong> Используйте внешние базы данных знаний, чтобы предоставлять модели актуальный контекст.</li>
</ul>
<h4>5.3. Google (Gemini)</h4>
<p>Google активно развивает возможности тонкой настройки через свою платформу <strong>Google Cloud Vertex AI</strong>.
Это полноценная ML-платформа, которая предоставляет инструменты для подготовки данных, запуска обучающих заданий и развертывания моделей.
Тонкая настройка доступна для моделей семейства Gemini.</p>
<p><strong>Процесс:</strong></p>
<ol>
<li>Подготовка данных (JSONL или CSV) в формате <code>input_text</code>/<code>output_text</code> (для instruction tuning) или <code>messages</code> (для chat tuning).</li>
<li>Загрузка данных в Google Cloud Storage (GCS).</li>
<li>Создание и запуск задания по тонкой настройке через Vertex AI Console или SDK.
<li>Развертывание настроенной модели на конечной точке (Endpoint).</li>
<li>Использование настроенной модели через эту конечную точку.</li>
</ol>
<p><strong>Пример данных (файл <code>gemini_tuning_data.jsonl</code>):</strong></p>
<pre class="line-numbers"><code class="language-json">{"input_text": "Суммируй основные идеи этой книги: 'Книга рассказывает о путешествии героя, который преодолевает препятствия и находит себя.'", "output_text": "Главный герой книги отправляется в трансформирующее путешествие, сталкиваясь с трудностями и обретая самопознание."} 
{"input_text": "Объясни принцип работы термоядерного реактора простыми словами.", "output_text": "Термоядерный реактор пытается воспроизвести процесс, который происходит на Солнце: слияние легких атомных ядер при очень высоких температурах, высвобождая огромное количество энергии."}</code></pre>
<p><strong>Пример Python кода (требует <code>google-cloud-aiplatform</code>):</strong></p>
<p>Предварительно установите: <code>pip install google-cloud-aiplatform</code> и <code>pip install google-cloud-storage</code></p>
<pre class="line-numbers"><code class="language-python">import os
from google.cloud import aiplatform
from google.cloud import storage

# --- Настройки ---
# ЗАМЕНИТЕ на свои значения:
PROJECT_ID = "your-gcp-project-id"
REGION = "us-central1"               # Выберите регион, поддерживающий Gemini и Vertex AI
BUCKET_NAME = "your-gcs-bucket-for-tuning" # Имя вашего бакета GCS (должен быть создан заранее)
DATA_FILE_LOCAL_PATH = "gemini_tuning_data.jsonl"
GCS_DATA_URI = f"gs://{BUCKET_NAME}/{DATA_FILE_LOCAL_PATH}"
TUNED_MODEL_DISPLAY_NAME = "my-tuned-gemini-model"
# --- Конец настроек ---

# Инициализация Vertex AI
aiplatform.init(project=PROJECT_ID, location=REGION)

# 1. Создание файла с данными (если его нет)
with open(DATA_FILE_LOCAL_PATH, "w", encoding="utf-8") as f:
    f.write('{"input_text": "Суммируй основные идеи этой книги: \'Книга рассказывает о путешествии героя, который преодолевает препятствия и находит себя.\'", "output_text": "Главный герой книги отправляется в трансформирующее путешествие, сталкиваясь с трудностями и обретая самопознание."}')
    f.write('\n')
    f.write('{"input_text": "Объясни принцип работы термоядерного реактора простыми словами.", "output_text": "Термоядерный реактор пытается воспроизвести процесс, который происходит на Солнце: слияние легких атомных ядер при очень высоких температурах, высвобождая огромное количество энергии."}')
print(f"Файл данных '{DATA_FILE_LOCAL_PATH}' создан.")


# 2. Загрузка данных в Google Cloud Storage
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """Загружает файл в бакет GCS."""
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"Файл '{source_file_name}' загружен в 'gs://{bucket_name}/{destination_blob_name}'.")

try:
    upload_blob(BUCKET_NAME, DATA_FILE_LOCAL_PATH, DATA_FILE_LOCAL_PATH)
except Exception as e:
    print(f"Ошибка загрузки файла в GCS. Убедитесь, что бакет существует и у вас есть права: {e}")
    exit()

# 3. Создание и запуск задания на тонкую настройку
print(f"\nЗапуск тонкой настройки модели '{TUNED_MODEL_DISPLAY_NAME}'...")
try:
    # `tune_model` запускает задание и возвращает настроенную модель после завершения
    tuned_model = aiplatform.Model.tune_model(
        model_display_name=TUNED_MODEL_DISPLAY_NAME,
        source_model_name="gemini-1.0-pro-001", # Базовая модель Gemini Pro
        training_data_path=GCS_DATA_URI,
        tuning_method="SUPERVISED_TUNING",
        train_steps=100, # Количество шагов обучения. Оптимальное значение зависит от размера данных.
        # batch_size=16, # Можно указать
        # learning_rate_multiplier=1.0 # Можно указать
    )
    print(f"Модель '{TUNED_MODEL_DISPLAY_NAME}' успешно настроена. ID модели: {tuned_model.name}")
    print("Процесс тонкой настройки может занять значительное время.")
except Exception as e:
    print(f"Ошибка тонкой настройки. Проверьте логи в Vertex AI Console: {e}")
    exit()

# 4. Развертывание настроенной модели (для использования)
print(f"\nРазвертывание настроенной модели '{TUNED_MODEL_DISPLAY_NAME}' на конечную точку...")
try:
    endpoint = tuned_model.deploy(
        machine_type="n1-standard-4", # Тип машины для конечной точки. Выберите подходящий.
        min_replica_count=1,
        max_replica_count=1
    )
    print(f"Модель развернута на конечную точку: {endpoint.name}")
    print("Развертывание также может занять несколько минут.")
except Exception as e:
    print(f"Ошибка развертывания модели: {e}")
    exit()

# 5. Использование настроенной модели
print("\nТестирование настроенной модели...")
prompt = "Расскажи мне о своих возможностях после обучения."
instances = [{"prompt": prompt}] # Для Instruction Tuning. Если Chat Tuning, то {"messages": [...]}

try:
    response = endpoint.predict(instances=instances)
    print("\nОтвет настроенной модели:")
    print(response.predictions[0])
except Exception as e:
    print(f"Ошибка при использовании настроенной модели: {e}")

# После завершения работы, не забудьте удалить конечную точку и модель, чтобы избежать лишних расходов:
# endpoint.delete()
# tuned_model.delete()</code></pre>
<h3>6. Общие рекомендации</h3>
<ul>
<li><strong>Начните с малого:</strong> Не пытайтесь сразу обучить модель на тысячах примеров. Начните с небольшого, но качественного набора данных.</li>
<li><strong>Итерируйте:</strong> Тонкая настройка — это итеративный процесс. Обучайте, оценивайте, корректируйте данные или гиперпараметры, повторяйте.</li>
<li><strong>Мониторинг:</strong> Внимательно отслеживайте метрики обучения (потери) и используйте набор валидационных данных, чтобы избежать переобучения.</li>
<li><strong>Оценка:</strong> Всегда тестируйте настроенную модель на данных, которые она <em>никогда не видела</em> во время обучения, чтобы оценить её обобщающую способность.</li>
<li><strong>Стоимость:</strong> Помните, что тонкая настройка и развертывание конечных точек платные. Учитывайте это в бюджете.</li>
<li><strong>Документация:</strong> Всегда сверяйтесь с официальной документацией поставщика LLM. API и возможности постоянно развиваются.</li>
</ul>
