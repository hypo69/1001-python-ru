<h3><strong>Цикл «НЕ Selenium». Интро</strong></h3>
<p>Те, кто занимается веб-скрапингом, тестированием и автоматизацией, знакомы с Selenium, более современным Playwright и/или фреймворком Crawlee. Они мощные, они могут почти все, и они... не всегда нужны. Более того, во многих случаях использование этих инструментов — забивание гвоздей микроскопом: работа, конечно, будет сделана, но ценой неоправданных затрат — скорости, системных ресурсов и сложности настройки.</p>
<p>Добро пожаловать в цикл статей «НЕ Selenium». Здесь я покажу другие способы (не всегда очевидные) взаимодействия с содержимым интернета.</p>
<h4>Парадигма №1: Прямое общение. HTTP-клиенты</h4>
<ul>
<li><strong><code>Requests</code></strong> — Формирует и отправляет сетевой запрос к целевому адресу (URL), точно так же, как это делает ваш браузер в самый первый момент загрузки страницы, но без самого браузера. В этот запрос он упаковывает метод (например, <code>GET</code>, чтобы получить данные), заголовки (<code>Headers</code>), которые представляются сайту (например, <code>User-Agent: &quot;я-браузер&quot;</code>), и другие параметры. В ответ от сервера он получает сырые данные — чаще всего, это исходный HTML-код страницы или строка в формате JSON, а также код статуса (например, <code>200 OK</code>).</li>
<li><strong><code>HTTPX</code></strong> — это современный наследник <code>Requests</code>. На фундаментальном уровне он делает всё то же самое: отправляет те же самые HTTP-запросы с теми же заголовками и получает те же самые ответы. Но есть ключевое отличие: <code>Requests</code> работает <strong>синхронно</strong> — отправил запрос, сидит и ждет ответа, получил ответ, отправил следующий. <code>HTTPX</code> же умеет работать <strong>асинхронно</strong> — он может &quot;забросить&quot; сразу сотню запросов, не дожидаясь ответов, и затем эффективно обрабатывать их по мере поступления.</li>
</ul>
<p>Отлично подходят для сбора данных со статических сайтов, работы с API, парсинга тысяч страниц, где не требуется выполнение JavaScript.</p>
<ul>
<li><strong>Преимущества:</strong> <strong>Скорость и эффективность.</strong> Благодаря асинхронности <code>HTTPX</code>, там, где <code>Requests</code> будет последовательно делать 100 запросов несколько минут, <code>HTTPX</code> справится за несколько секунд.</li>
<li><strong>Недостатки:</strong> Не подходят для сайтов, где контент генерируется с помощью JavaScript.</li>
</ul>
<h4>Парадигма №2: Chrome DevTools Protocol (CDP)</h4>
<p>Что делать, если сайт динамический и контент генерируется с помощью JavaScript? Современные браузеры (Chrome, Chromium, Edge) имеют встроенный протокол для отладки и управления — <strong>Chrome DevTools Protocol (CDP)</strong>. Он позволяет отдавать команды браузеру напрямую, минуя громоздкую прослойку в виде WebDriver, которую использует Selenium.</p>
<ul>
<li><strong>Инструменты:</strong> Основным представителем этого подхода сегодня является <code>Pydoll</code>, который пришел на смену некогда популярному, но ныне не поддерживаемому <code>pyppeteer</code>.</li>
<li><strong>Когда использовать:</strong> Когда нужен рендеринг JavaScript, но хочется сохранить высокую скорость и избежать сложностей с драйверами.</li>
<li><strong>Преимущества:</strong> <strong>Баланс.</strong> Вы получаете мощь настоящего браузера, но с гораздо меньшими накладными расходами и зачастую со встроенными механизмами обхода защит.</li>
<li><strong>Недостатки:</strong> Может быть сложнее в отладке, чем Playwright, и требует более глубокого понимания работы браузера.</li>
</ul>
<h4>Парадигма №3: Автономные LLM-агенты</h4>
<p>Это самый передовой рубеж. Что, если вместо того, чтобы писать код, который говорит &quot;кликни сюда, введи это&quot;, мы просто дадим задачу на естественном языке? &quot;Найди мне всех поставщиков на этом сайте и собери их категории товаров&quot;.</p>
<p>Именно эту задачу решают LLM-агенты. Используя &quot;мозг&quot; в виде большой языковой модели (GPT, Gemini) и &quot;руки&quot; в виде набора инструментов (браузер, поиск Google), эти агенты могут самостоятельно планировать и выполнять сложные задачи в вебе.</p>
<ul>
<li><strong>Инструменты:</strong> Связки вроде <code>LangChain</code> + <code>Pydoll</code> или кастомные решения, как в <code>simple_browser.py</code>, который мы разберем позже.</li>
<li><strong>Когда использовать:</strong> Для сложных исследовательских задач, где шаги заранее неизвестны и требуется адаптация в реальном времени.</li>
<li><strong>Преимущества:</strong> <strong>Интеллект.</strong> Способность решать неструктурированные задачи и адаптироваться к изменениям на лету.</li>
<li><strong>Недостатки:</strong> &quot;Недетерминированность&quot; (результат может меняться от запуска к запуску), стоимость API-вызовов к LLM, более низкая скорость по сравнению с прямым кодом.</li>
</ul>
<h4>Парадигма №4: Скрапинг без кода</h4>
<p>Иногда задача настолько проста, что писать код — это излишество. Нужно быстро вытащить таблицу с одной страницы? Для этого существуют элегантные решения, не требующие программирования.</p>
<ul>
<li><strong>Инструменты:</strong> Функции Google Sheets (<code>IMPORTXML</code>, <code>IMPORTHTML</code>), браузерные расширения.</li>
<li><strong>Когда использовать:</strong> Для одноразовых задач, быстрого прототипирования или когда вы просто не хотите писать код.</li>
<li><strong>Преимущества:</strong> <strong>Простота.</strong> Открыл, указал, что нужно собрать, — получил результат.</li>
<li><strong>Недостатки:</strong> Ограниченная функциональность, не подходят для сложных задач или больших объемов данных.</li>
</ul>
<h3>Что дальше?</h3>
<p>Эта статья — лишь введение. В следующих выпусках нашего цикла «НЕ Selenium» мы перейдем от теории к жесткой практике. Мы глубоко погрузимся в каждую из этих парадигм и покажем, как они работают на реальных примерах:</p>
<ul>
<li>Разберем <strong>Pydoll</strong> и посмотрим, как он обходит Cloudflare.</li>
<li>Устроим битву <strong>JavaScript против Python</strong> за звание лучшего языка для скрапинга.</li>
<li>Научимся выжимать максимум скорости из парсинга с помощью <strong>lxml</strong>.</li>
<li>Напишем скрипт, который собирает данные с <strong>Amazon</strong> и сохраняет их в <strong>Excel</strong>.</li>
<li>Покажем, как <strong>Google Sheets</strong> может стать вашим первым скрапером.</li>
<li>И, конечно же, детально разберем, как создать и использовать <strong>автономного LLM-агента</strong> для управления браузером.</li>
</ul>
<p>Приготовьтесь изменить свой взгляд на автоматизацию и сбор данных в вебе. Будет быстро, эффективно и очень интересно. Подписывайтесь</p>
