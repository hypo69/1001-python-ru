Olive (ONNX LIVE) es un kit de herramientas de optimizaci√≥n de modelos de vanguardia con una CLI que le permite enviar modelos para el tiempo de ejecuci√≥n de ONNX con calidad y rendimiento.

La entrada a Olive es t√≠picamente un modelo de PyTorch o Hugging Face, y la salida es un modelo ONNX optimizado que se ejecuta en un dispositivo (objetivo de implementaci√≥n) que ejecuta el tiempo de ejecuci√≥n de ONNX. Olive optimizar√° el modelo para el acelerador de IA (NPU, GPU, CPU) del objetivo de implementaci√≥n proporcionado por un proveedor de hardware como Qualcomm, AMD, Nvidia o Intel.


Olive ejecuta un flujo de trabajo, que es una secuencia ordenada de tareas individuales de optimizaci√≥n de modelos llamadas pases; los pases de ejemplo incluyen compresi√≥n de modelos, captura de gr√°ficos, cuantificaci√≥n y optimizaci√≥n de gr√°ficos. Cada pase tiene un conjunto de par√°metros que se pueden ajustar para lograr las mejores m√©tricas, como la precisi√≥n y la latencia, que son evaluadas por el evaluador respectivo. Olive emplea una estrategia de b√∫squeda que utiliza un muestreador de b√∫squeda para autoajustar cada pase individualmente o un conjunto de pases juntos.


Beneficios de usar Olive
 Reduzca la frustraci√≥n y el tiempo de experimentaci√≥n manual por prueba y error con diferentes t√©cnicas para la optimizaci√≥n de gr√°ficos, la compresi√≥n y la cuantificaci√≥n. Defina su objetivo y precisi√≥n y deje que Olive produzca autom√°ticamente el mejor modelo para usted.

 M√°s de 40 componentes de optimizaci√≥n de modelos incorporados que cubren t√©cnicas de vanguardia en cuantificaci√≥n, compresi√≥n, optimizaci√≥n de gr√°ficos y ajuste fino.

 CLI f√°cil de usar para tareas comunes de optimizaci√≥n de modelos. Por ejemplo, olive quantize, olive auto-opt, olive finetune. Los usuarios avanzados pueden construir f√°cilmente flujos de trabajo usando YAML/JSON para orquestar transformaciones de modelos y pasos de optimizaci√≥n.

 Empaquetado e implementaci√≥n de modelos incorporados.

 Permite el servicio Multi LoRA.

 Integraci√≥n con Hugging Face y Azure AI.

 Mecanismo de almacenamiento en cach√© incorporado para mejorar la productividad del equipo.

üöÄ Primeros pasos

‚ú® Inicio r√°pido

Si prefiere usar la l√≠nea de comandos directamente en lugar de los cuadernos de Jupyter, hemos descrito los comandos de inicio r√°pido aqu√≠.

1. Instalar Olive CLI

Recomendamos instalar Olive en un entorno virtual o un entorno conda.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
Nota

Olive tiene dependencias opcionales que se pueden instalar para habilitar funciones adicionales. Consulte la configuraci√≥n del paquete de Olive para obtener la lista de extras y sus dependencias.

2. Optimizador autom√°tico

En este inicio r√°pido, optimizar√° Qwen/Qwen2.5-0.5B-Instruct, que tiene muchos archivos de modelo en el repositorio de Hugging Face para diferentes precisiones que no son requeridas por Olive.

Ejecute la optimizaci√≥n autom√°tica:
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
ConsejoUsuarios de PowerShellLa continuaci√≥n de l√≠nea entre Bash y PowerShell no es intercambiable. Si est√° utilizando PowerShell, puede copiar y pegar el siguiente comando que utiliza una continuaci√≥n de l√≠nea compatible.
```powershell
olive optimize `\
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `\
   --output_path models/qwen `\
   --precision int4
```

El optimizador autom√°tico:

Adquirir√° el modelo del repositorio de modelos de Hugging Face.
Cuantificar√° el modelo a int4 usando GPTQ.
Capturar√° el gr√°fico ONNX y almacenar√° los pesos en un archivo de datos ONNX.
Optimizar√° el gr√°fico ONNX.
Olive puede optimizar autom√°ticamente arquitecturas de modelos populares como Llama, Phi, Qwen, Gemma, etc. de forma predeterminada; consulte la lista detallada aqu√≠. Adem√°s, puede optimizar otras arquitecturas de modelos proporcionando detalles sobre las entradas/salidas del modelo (io_config).

3. Inferencia en el tiempo de ejecuci√≥n de ONNX

El tiempo de ejecuci√≥n de ONNX (ORT) es un motor de inferencia multiplataforma r√°pido y ligero con enlaces para lenguajes de programaci√≥n populares como Python, C/C++, C#, Java, JavaScript, etc. ORT le permite infundir modelos de IA en sus aplicaciones para que la inferencia se maneje en el dispositivo.

La aplicaci√≥n de chat de ejemplo para ejecutar se encuentra como model-chat.py en el repositorio de Github de onnxruntime-genai.
```