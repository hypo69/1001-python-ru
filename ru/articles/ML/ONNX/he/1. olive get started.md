Olive (ONNX LIVE) הוא ערכת כלים חדשנית לאופטימיזציית מודלים עם CLI נלווה המאפשרת לך לשלוח מודלים עבור זמן הריצה של ONNX באיכות וביצועים.

הקלט ל-Olive הוא בדרך כלל מודל PyTorch או Hugging Face, והפלט הוא מודל ONNX אופטימלי המבוצע על התקן (יעד פריסה) המריץ את זמן הריצה של ONNX. Olive תבצע אופטימיזציה של המודל עבור מאיץ ה-AI (NPU, GPU, CPU) של יעד הפריסה המסופק על ידי ספק חומרה כגון Qualcomm, AMD, Nvidia או Intel.


Olive מבצעת זרימת עבודה, שהיא רצף מסודר של משימות אופטימיזציית מודלים בודדות הנקראות מעברים – דוגמאות למעברים כוללות דחיסת מודלים, לכידת גרפים, קוונטיזציה ואופטימיזציית גרפים. לכל מעבר יש קבוצה של פרמטרים שניתן לכוונן כדי להשיג את המדדים הטובים ביותר, כגון דיוק והשהיה, המוערכים על ידי המעריך המתאים. Olive משתמשת באסטרטגיית חיפוש המשתמשת בדוגם חיפוש כדי לכוונן אוטומטית כל מעבר בנפרד או קבוצה של מעברים יחד.


יתרונות השימוש ב-Olive
 הפחתת תסכול וזמן של ניסוי וטעייה ידניים עם טכניקות שונות לאופטימיזציית גרפים, דחיסה וקוונטיזציה. הגדר את היעד והדיוק שלך ותן ל-Olive לייצר עבורך אוטומטית את המודל הטוב ביותר.

 40+ רכיבי אופטימיזציית מודלים מובנים המכסים טכניקות חדשניות בקוונטיזציה, דחיסה, אופטימיזציית גרפים וכוונון עדין.

 CLI קל לשימוש למשימות אופטימיזציית מודלים נפוצות. לדוגמה, olive quantize, olive auto-opt, olive finetune. משתמשים מתקדמים יכולים לבנות בקלות זרימות עבודה באמצעות YAML/JSON כדי לתזמר טרנספורמציות מודלים ושלבי אופטימיזציה.

 אריזת ופריסת מודלים מובנית.

 מאפשרת שירות Multi LoRA.

 אינטגרציה עם Hugging Face ו-Azure AI.

 מנגנון שמירה במטמון מובנה לשיפור הפרודוקטיביות של הצוות.

🚀 תחילת העבודה

✨ התחלה מהירה

אם אתה מעדיף להשתמש בשורת הפקודה ישירות במקום במחברות Jupyter, ריכזנו כאן את פקודות ההתחלה המהירה.

1. התקן את Olive CLI

אנו ממליצים להתקין את Olive בסביבה וירטואלית או בסביבת conda.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
הערה

ל-Olive יש תלויות אופציונליות שניתן להתקין כדי לאפשר תכונות נוספות. אנא עיין בתצורת חבילת Olive לרשימת התוספות והתלויות שלהן.

2. אופטימיזטור אוטומטי

בהתחלה מהירה זו תבצע אופטימיזציה של Qwen/Qwen2.5-0.5B-Instruct, שיש לו קבצי מודל רבים במאגר Hugging Face עבור דיוקים שונים שאינם נדרשים על ידי Olive.

הפעל את האופטימיזציה האוטומטית:
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
טיפקוראי PowerShellהמשך שורה בין Bash ל-PowerShell אינו ניתן להחלפה. אם אתה משתמש ב-PowerShell, תוכל להעתיק ולהדביק את הפקודה הבאה המשתמשת בהמשך שורה תואם.
```powershell
olive optimize `\
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `\
   --output_path models/qwen `\
   --precision int4
```

האופטימיזטור האוטומטי יבצע:

רכישת המודל ממאגר המודלים של Hugging Face.
קוונטיזציה של המודל ל-int4 באמצעות GPTQ.
לכידת גרף ONNX ואחסון המשקלים בקובץ נתונים של ONNX.
אופטימיזציה של גרף ONNX.
Olive יכולה לבצע אופטימיזציה אוטומטית של ארכיטקטורות מודלים פופולריות כמו Llama, Phi, Qwen, Gemma וכו' מחוץ לקופסה – ראה רשימה מפורטת כאן. כמו כן, תוכל לבצע אופטימיזציה של ארכיטקטורות מודלים אחרות על ידי מתן פרטים על הקלטים/פלטים של המודל (io_config).

3. הסקה על זמן הריצה של ONNX

זמן הריצה של ONNX (ORT) הוא מנוע הסקה מהיר וקל משקל חוצה פלטפורמות עם כריכות לשפות תכנות פופולריות כגון Python, C/C++, C#, Java, JavaScript וכו'. ORT מאפשר לך להחדיר מודלי AI ליישומים שלך כך שההסקה תטופל במכשיר.

אפליקציית הצ'אט לדוגמה להפעלה נמצאת כ-model-chat.py במאגר Github של onnxruntime-genai.
```