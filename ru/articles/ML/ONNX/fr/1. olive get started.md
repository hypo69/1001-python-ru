Olive (ONNX LIVE) est une bo√Æte √† outils d'optimisation de mod√®les de pointe avec une CLI qui vous permet d'exp√©dier des mod√®les pour le runtime ONNX avec qualit√© et performance.

L'entr√©e d'Olive est g√©n√©ralement un mod√®le PyTorch ou Hugging Face, et la sortie est un mod√®le ONNX optimis√© qui est ex√©cut√© sur un appareil (cible de d√©ploiement) ex√©cutant le runtime ONNX. Olive optimisera le mod√®le pour l'acc√©l√©rateur d'IA (NPU, GPU, CPU) de la cible de d√©ploiement fourni par un fournisseur de mat√©riel tel que Qualcomm, AMD, Nvidia ou Intel.


Olive ex√©cute un flux de travail, qui est une s√©quence ordonn√©e de t√¢ches individuelles d'optimisation de mod√®les appel√©es passes ‚Äì des exemples de passes incluent la compression de mod√®les, la capture de graphes, la quantification et l'optimisation de graphes. Chaque passe a un ensemble de param√®tres qui peuvent √™tre ajust√©s pour obtenir les meilleures m√©triques, telles que la pr√©cision et la latence, qui sont √©valu√©es par l'√©valuateur respectif. Olive utilise une strat√©gie de recherche qui utilise un √©chantillonneur de recherche pour auto-ajuster chaque passe individuellement ou un ensemble de passes ensemble.


Avantages de l'utilisation d'Olive
 R√©duisez la frustration et le temps d'exp√©rimentation manuelle par essais et erreurs avec diff√©rentes techniques d'optimisation de graphes, de compression et de quantification. D√©finissez votre cible et votre pr√©cision et laissez Olive produire automatiquement le meilleur mod√®le pour vous.

 Plus de 40 composants d'optimisation de mod√®les int√©gr√©s couvrant des techniques de pointe en quantification, compression, optimisation de graphes et affinage.

 CLI facile √† utiliser pour les t√¢ches courantes d'optimisation de mod√®les. Par exemple, olive quantize, olive auto-opt, olive finetune. Les utilisateurs avanc√©s peuvent facilement construire des flux de travail √† l'aide de YAML/JSON pour orchestrer les transformations de mod√®les et les √©tapes d'optimisation.

 Emballage et d√©ploiement de mod√®les int√©gr√©s.

 Permet le service Multi LoRA.

 Int√©gration Hugging Face et Azure AI.

 M√©canisme de mise en cache int√©gr√© pour am√©liorer la productivit√© de l'√©quipe.

üöÄ D√©marrage rapide

‚ú® D√©marrage rapide

Si vous pr√©f√©rez utiliser la ligne de commande directement au lieu des notebooks Jupyter, nous avons d√©crit les commandes de d√©marrage rapide ici.

1. Installer Olive CLI

Nous vous recommandons d'installer Olive dans un environnement virtuel ou un environnement conda.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
Note

Olive a des d√©pendances facultatives qui peuvent √™tre install√©es pour activer des fonctionnalit√©s suppl√©mentaires. Veuillez vous r√©f√©rer √† la configuration du package Olive pour la liste des extras et de leurs d√©pendances.

2. Optimiseur automatique

Dans ce d√©marrage rapide, vous optimiserez Qwen/Qwen2.5-0.5B-Instruct, qui a de nombreux fichiers de mod√®le dans le d√©p√¥t Hugging Face pour diff√©rentes pr√©cisions qui ne sont pas requises par Olive.

Ex√©cutez l'optimisation automatique :
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
ConseilUtilisateurs de PowerShellLa continuation de ligne entre Bash et PowerShell n'est pas interchangeable. Si vous utilisez PowerShell, vous pouvez copier-coller la commande suivante qui utilise une continuation de ligne compatible.
```powershell
olive optimize `
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `
   --output_path models/qwen `
   --precision int4
```

L'optimiseur automatique :

Acquiert le mod√®le du d√©p√¥t de mod√®les Hugging Face.
Quantifie le mod√®le en int4 √† l'aide de GPTQ.
Capture le graphe ONNX et stocke les poids dans un fichier de donn√©es ONNX.
Optimise le graphe ONNX.
Olive peut optimiser automatiquement les architectures de mod√®les populaires comme Llama, Phi, Qwen, Gemma, etc. pr√™tes √† l'emploi ‚Äì voir la liste d√©taill√©e ici. De plus, vous pouvez optimiser d'autres architectures de mod√®les en fournissant des d√©tails sur les entr√©es/sorties du mod√®le (io_config).

3. Inf√©rence sur le runtime ONNX

Le runtime ONNX (ORT) est un moteur d'inf√©rence multiplateforme rapide et l√©ger avec des liaisons pour les langages de programmation populaires tels que Python, C/C++, C#, Java, JavaScript, etc. ORT vous permet d'int√©grer des mod√®les d'IA dans vos applications afin que l'inf√©rence soit g√©r√©e sur l'appareil.

L'exemple d'application de chat √† ex√©cuter se trouve sous model-chat.py dans le d√©p√¥t Github onnxruntime-genai.
