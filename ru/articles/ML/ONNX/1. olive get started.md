Olive (ONNX LIVE) is a cutting-edge model optimization toolkit with an accompanying CLI that enables you to ship models for the ONNX runtime with quality and performance.

The input to Olive is typically a PyTorch or Hugging Face model, and the output is an optimized ONNX model that is executed on a device (deployment target) running the ONNX runtime. Olive will optimize the model for the deployment targetâ€™s AI accelerator (NPU, GPU, CPU) provided by a hardware vendor such as Qualcomm, AMD, Nvidia, or Intel.


Olive executes a workflow, which is an ordered sequence of individual model optimization tasks called passes â€“ example passes include model compression, graph capture, quantization, and graph optimization. Each pass has a set of parameters that can be tuned to achieve the best metrics, such as accuracy and latency, that are evaluated by the respective evaluator. Olive employs a search strategy that uses a search sampler to auto-tune each pass individually or a set of passes together.


Benefits of using Olive
 Reduce frustration and time of trial-and-error manual experimentation with different techniques for graph optimization, compression, and quantization. Define your target and precision and let Olive automatically produce the best model for you.

 40+ built-in model optimization components covering cutting-edge techniques in quantization, compression, graph optimization, and finetuning.

 Easy-to-use CLI for common model optimization tasks. For example, olive quantize, olive auto-opt, olive finetune. Advanced users can easily construct workflows using YAML/JSON to orchestrate model transformations and optimizations steps.

 Model packaging and deployment built-in.

 Enables Multi LoRA serving.

 Hugging Face and Azure AI Integration.

 Built-in caching mechanism to improve teamâ€™s productivity.

ðŸš€ Getting Started

âœ¨ Quickstart

If you prefer using the command line directly instead of Jupyter notebooks, weâ€™ve outlined the quickstart commands here.

1. Install Olive CLI

We recommend installing Olive in a virtual environment or a conda environment.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
Note

Olive has optional dependencies that can be installed to enable additional features. Please refer to Olive package config for the list of extras and their dependencies.

2. Automatic Optimizer

In this quickstart youâ€™ll be optimizing Qwen/Qwen2.5-0.5B-Instruct, which has many model files in the Hugging Face repo for different precisions that are not required by Olive.

Run the automatic optimization:
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
TipPowerShell UsersLine continuation between Bash and PowerShell are not interchangable. If you are using PowerShell, then you can copy-and-paste the following command that uses compatible line continuation.
```powershell
olive optimize `
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `
   --output_path models/qwen `
   --precision int4
```

The automatic optimizer will:

Acquire the model from the the Hugging Face model repo.
Quantize the model to int4 using GPTQ.
Capture the ONNX Graph and store the weights in an ONNX data file.
Optimize the ONNX Graph.
Olive can automatically optimize popular model architectures like Llama, Phi, Qwen, Gemma, etc out-of-the-box â€“ see detailed list here. Also, you can optimize other model architectures by providing details on the input/outputs of the model (io_config).

3. Inference on the ONNX Runtime

The ONNX Runtime (ORT) is a fast and light-weight cross-platform inference engine with bindings for popular programming language such as Python, C/C++, C#, Java, JavaScript, etc. ORT enables you to infuse AI models into your applications so that inference is handled on-device.

The sample chat app to run is found as model-chat.py in the onnxruntime-genai Github repository.