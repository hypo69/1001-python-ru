Here is the English translation of the provided document.

### **What is Olive?**

**author: hypo69 / 13/09/2025**

Olive is an advanced command-line toolkit for model optimization that enables efficient preparation of models for ONNX Runtime, considering both quality and performance.

The input for Olive is typically PyTorch or Hugging Face models, and the result is an optimized ONNX model that runs on a target device using ONNX Runtime. Olive optimizes the model by taking into account the AI accelerator of the target platform (NPU, GPU, CPU) from manufacturers like Qualcomm, AMD, Nvidia, or Intel.

---

**(Image: A diagram showing a PyTorch/HF Model being processed by OLIVE's Optimize function to become an ONNX Model, which then runs on a Device using ONNX RUNTIME, supported by Qualcomm, Intel, AMD, and NVIDIA.)**

---

Olive executes a workflow, which is an ordered sequence of individual model optimization tasks called "passes". Examples of such passes include model compression, graph capturing, quantization, and graph optimization. Each pass has a set of parameters that can be configured to achieve the best metrics, such as accuracy and latency, as evaluated by a corresponding evaluator. Olive uses a search strategy with a "search sampler" to automatically tune each pass individually or groups of passes together.

---

### In This Article

**01. Advantages of using Olive**
*   1. Installing Olive CLI
*   2. Automatic optimization
*   3. Inference in ONNX Runtime

**02. Learn more**

---

### **Advantages of using Olive**

*   Reduces the time and effort spent on manually selecting various techniques for graph optimization, compression, and quantization.
*   Over 40 built-in model optimization components, covering advanced methods for quantization, compression, graph optimization, and fine-tuning.
*   An easy-to-use command-line interface for performing standard model optimization tasks like `olive quantize`, `olive auto-opt`, and `olive finetune`.
*   Allows experienced users to easily create workflows using YAML/JSON to organize the transformation and optimization steps of the model.
*   Built-in mechanisms for model packaging and deployment.
*   Support for Multi LoRA serving.
*   Integration with Hugging Face and Azure AI.
*   Built-in caching mechanism to improve team productivity.

---

### **Getting started**

ðŸš€ **Quick start**

If you prefer to use the command line directly instead of Jupyter notebooks, the quick start commands are provided below.

#### **1. Installing Olive CLI**

It is recommended to install Olive in a virtual environment or a conda environment.

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

**Note:**
Olive has optional dependencies that can be installed to enable additional features. Please refer to the Olive package configuration for a list of these options and their dependencies.

---

#### **2. Automatic optimization**

In this quick start, you will optimize the `Qwen/Qwen2.5-0.5B-Instruct` model, which contains many model files in the Hugging Face repository for different precisions, but Olive does not require all of them.

Run the automatic optimization:

```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```

**Tip: PowerShell Users**
The line continuation characters are not compatible between Bash and PowerShell. If you are using PowerShell, copy and paste the following command with the compatible line continuation:

```powershell
olive optimize `
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `
    --output_path models/qwen `
    --precision int4
```

The automatic optimizer will perform the following actions:

*   Downloads the model from the Hugging Face repository.
*   Performs int4 quantization of the model using GPTQ.
*   Captures the ONNX graph and saves the weights in an ONNX data file.
*   Optimizes the ONNX graph.

Olive can automatically optimize popular model architectures like Llama, Phi, Qwen, Gemma, etc. â€” see the full list here. It is also possible to optimize other architectures by providing information about the model's inputs and outputs (io_config).

---

#### **3. Inference in ONNX Runtime**

ONNX Runtime (ORT) is a fast and lightweight cross-platform inference engine with support for popular programming languages such as Python, C/C++, C#, Java, JavaScript, and more. ORT allows you to embed AI models into your applications, performing inference directly on the device.

An example of a chat application is available in the `model-chat.py` file in the onnxruntime-genai repository on GitHub.

---

### **Learn more**

*   Documentation
*   Recipes