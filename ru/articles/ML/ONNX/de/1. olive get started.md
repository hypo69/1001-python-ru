Olive (ONNX LIVE) ist ein hochmodernes Toolkit zur Modelloptimierung mit einer begleitenden CLI, das es Ihnen erm√∂glicht, Modelle f√ºr die ONNX-Laufzeitumgebung mit Qualit√§t und Leistung zu liefern.

Die Eingabe f√ºr Olive ist typischerweise ein PyTorch- oder Hugging Face-Modell, und die Ausgabe ist ein optimiertes ONNX-Modell, das auf einem Ger√§t (Bereitstellungsziel) ausgef√ºhrt wird, das die ONNX-Laufzeitumgebung verwendet. Olive optimiert das Modell f√ºr den AI-Beschleuniger (NPU, GPU, CPU) des Bereitstellungsziels, der von einem Hardwareanbieter wie Qualcomm, AMD, Nvidia oder Intel bereitgestellt wird.


Olive f√ºhrt einen Workflow aus, der eine geordnete Abfolge einzelner Modelloptimierungsaufgaben, sogenannter P√§sse, darstellt ‚Äì Beispielp√§sse umfassen Modellkomprimierung, Grafikerfassung, Quantisierung und Grafikoptimierung. Jeder Pass verf√ºgt √ºber eine Reihe von Parametern, die optimiert werden k√∂nnen, um die besten Metriken wie Genauigkeit und Latenz zu erzielen, die vom jeweiligen Evaluator bewertet werden. Olive verwendet eine Suchstrategie, die einen Suchsampler verwendet, um jeden Pass einzeln oder eine Reihe von P√§ssen zusammen automatisch abzustimmen.


Vorteile der Verwendung von Olive
 Reduzieren Sie Frustration und Zeit f√ºr manuelles Experimentieren durch Versuch und Irrtum mit verschiedenen Techniken zur Grafikoptimierung, Komprimierung und Quantisierung. Definieren Sie Ihr Ziel und Ihre Pr√§zision und lassen Sie Olive automatisch das beste Modell f√ºr Sie erstellen.

 √úber 40 integrierte Modelloptimierungskomponenten, die modernste Techniken in den Bereichen Quantisierung, Komprimierung, Grafikoptimierung und Feinabstimmung abdecken.

 Einfach zu bedienende CLI f√ºr g√§ngige Modelloptimierungsaufgaben. Zum Beispiel olive quantize, olive auto-opt, olive finetune. Fortgeschrittene Benutzer k√∂nnen Workflows einfach mit YAML/JSON erstellen, um Modelltransformationen und Optimierungsschritte zu orchestrieren.

 Modellpaketierung und -bereitstellung integriert.

 Erm√∂glicht Multi LoRA Serving.

 Hugging Face- und Azure AI-Integration.

 Integrierter Caching-Mechanismus zur Verbesserung der Teamproduktivit√§t.

üöÄ Erste Schritte

‚ú® Schnellstart

Wenn Sie die Befehlszeile direkt anstelle von Jupyter-Notebooks verwenden m√∂chten, haben wir hier die Schnellstartbefehle beschrieben.

1. Olive CLI installieren

Wir empfehlen die Installation von Olive in einer virtuellen Umgebung oder einer Conda-Umgebung.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
Hinweis

Olive verf√ºgt √ºber optionale Abh√§ngigkeiten, die installiert werden k√∂nnen, um zus√§tzliche Funktionen zu aktivieren. Eine Liste der Extras und ihrer Abh√§ngigkeiten finden Sie in der Olive-Paketkonfiguration.

2. Automatischer Optimierer

In diesem Schnellstart optimieren Sie Qwen/Qwen2.5-0.5B-Instruct, das viele Modelldateien im Hugging Face-Repository f√ºr verschiedene Pr√§zisionen enth√§lt, die von Olive nicht ben√∂tigt werden.

F√ºhren Sie die automatische Optimierung aus:
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
TippPowerShell-BenutzerDie Zeilenfortsetzung zwischen Bash und PowerShell ist nicht austauschbar. Wenn Sie PowerShell verwenden, k√∂nnen Sie den folgenden Befehl kopieren und einf√ºgen, der eine kompatible Zeilenfortsetzung verwendet.
```powershell
olive optimize `\
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `\
   --output_path models/qwen `\
   --precision int4
```

Der automatische Optimierer wird:

Das Modell aus dem Hugging Face-Modell-Repository abrufen.
Das Modell mit GPTQ auf int4 quantisieren.
Den ONNX-Graphen erfassen und die Gewichte in einer ONNX-Datendatei speichern.
Den ONNX-Graphen optimieren.
Olive kann beliebte Modellarchitekturen wie Llama, Phi, Qwen, Gemma usw. sofort optimieren ‚Äì eine detaillierte Liste finden Sie hier. Sie k√∂nnen auch andere Modellarchitekturen optimieren, indem Sie Details zu den Ein- und Ausgaben des Modells (io_config) angeben.

3. Inferenz auf der ONNX-Laufzeitumgebung

Die ONNX-Laufzeitumgebung (ORT) ist eine schnelle und leichte plattform√ºbergreifende Inferenz-Engine mit Bindungen f√ºr g√§ngige Programmiersprachen wie Python, C/C++, C#, Java, JavaScript usw. ORT erm√∂glicht es Ihnen, KI-Modelle in Ihre Anwendungen zu integrieren, sodass die Inferenz auf dem Ger√§t verarbeitet wird.

Die Beispiel-Chat-App zum Ausf√ºhren finden Sie als model-chat.py im onnxruntime-genai Github-Repository.
```