Olive (ONNX LIVE) √® un toolkit all'avanguardia per l'ottimizzazione dei modelli con una CLI di accompagnamento che consente di spedire modelli per il runtime ONNX con qualit√† e prestazioni.

L'input per Olive √® tipicamente un modello PyTorch o Hugging Face, e l'output √® un modello ONNX ottimizzato che viene eseguito su un dispositivo (target di distribuzione) che esegue il runtime ONNX. Olive ottimizzer√† il modello per l'acceleratore AI (NPU, GPU, CPU) del target di distribuzione fornito da un fornitore di hardware come Qualcomm, AMD, Nvidia o Intel.


Olive esegue un flusso di lavoro, che √® una sequenza ordinata di singole attivit√† di ottimizzazione del modello chiamate passaggi ‚Äì esempi di passaggi includono la compressione del modello, l'acquisizione del grafo, la quantizzazione e l'ottimizzazione del grafo. Ogni passaggio ha un set di parametri che possono essere ottimizzati per ottenere le migliori metriche, come accuratezza e latenza, che vengono valutate dal rispettivo valutatore. Olive impiega una strategia di ricerca che utilizza un campionatore di ricerca per ottimizzare automaticamente ogni passaggio individualmente o un set di passaggi insieme.


Benefici dell'utilizzo di Olive
 Riduci la frustrazione e il tempo di sperimentazione manuale per tentativi ed errori con diverse tecniche per l'ottimizzazione del grafo, la compressione e la quantizzazione. Definisci il tuo obiettivo e la tua precisione e lascia che Olive produca automaticamente il modello migliore per te.

 Oltre 40 componenti di ottimizzazione del modello integrati che coprono tecniche all'avanguardia nella quantizzazione, compressione, ottimizzazione del grafo e messa a punto.

 CLI facile da usare per le comuni attivit√† di ottimizzazione del modello. Ad esempio, olive quantize, olive auto-opt, olive finetune. Gli utenti avanzati possono facilmente costruire flussi di lavoro utilizzando YAML/JSON per orchestrare le trasformazioni del modello e i passaggi di ottimizzazione.

 Packaging e distribuzione del modello integrati.

 Abilita il servizio Multi LoRA.

 Integrazione con Hugging Face e Azure AI.

 Meccanismo di caching integrato per migliorare la produttivit√† del team.

üöÄ Per iniziare

‚ú® Avvio rapido

Se preferisci utilizzare la riga di comando direttamente invece dei notebook Jupyter, abbiamo delineato i comandi di avvio rapido qui.

1. Installa Olive CLI

Si consiglia di installare Olive in un ambiente virtuale o in un ambiente conda.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
Nota

Olive ha dipendenze opzionali che possono essere installate per abilitare funzionalit√† aggiuntive. Si prega di fare riferimento alla configurazione del pacchetto Olive per l'elenco degli extra e delle loro dipendenze.

2. Ottimizzatore automatico

In questo avvio rapido ottimizzerai Qwen/Qwen2.5-0.5B-Instruct, che ha molti file modello nel repository Hugging Face per diverse precisioni che non sono richieste da Olive.

Esegui l'ottimizzazione automatica:
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
SuggerimentoUtenti PowerShellLa continuazione di riga tra Bash e PowerShell non √® intercambiabile. Se si utilizza PowerShell, √® possibile copiare e incollare il seguente comando che utilizza una continuazione di riga compatibile.
```powershell
olive optimize `
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `
   --output_path models/qwen `
   --precision int4
```

L'ottimizzatore automatico:

Acquisir√† il modello dal repository di modelli Hugging Face.
Quantizzer√† il modello a int4 usando GPTQ.
Acquisir√† il grafo ONNX e memorizzer√† i pesi in un file di dati ONNX.
Ottimizzer√† il grafo ONNX.
Olive pu√≤ ottimizzare automaticamente architetture di modelli popolari come Llama, Phi, Qwen, Gemma, ecc. out-of-the-box ‚Äì vedi l'elenco dettagliato qui. Inoltre, √® possibile ottimizzare altre architetture di modelli fornendo dettagli sugli input/output del modello (io_config).

3. Inferenza sul runtime ONNX

Il runtime ONNX (ORT) √® un motore di inferenza multipiattaforma veloce e leggero con binding per linguaggi di programmazione popolari come Python, C/C++, C#, Java, JavaScript, ecc. ORT consente di infondere modelli AI nelle proprie applicazioni in modo che l'inferenza venga gestita sul dispositivo.

L'applicazione di chat di esempio da eseguire si trova come model-chat.py nel repository Github onnxruntime-genai.
