Olive (ONNX LIVE) to najnowocześniejszy zestaw narzędzi do optymalizacji modeli z towarzyszącym CLI, który umożliwia dostarczanie modeli dla środowiska uruchomieniowego ONNX z jakością i wydajnością.

Wejściem do Olive jest zazwyczaj model PyTorch lub Hugging Face, a wyjściem jest zoptymalizowany model ONNX, który jest wykonywany na urządzeniu (celu wdrożenia) działającym w środowisku uruchomieniowym ONNX. Olive zoptymalizuje model dla akceleratora AI (NPU, GPU, CPU) celu wdrożenia dostarczonego przez dostawcę sprzętu, takiego jak Qualcomm, AMD, Nvidia lub Intel.


Olive wykonuje przepływ pracy, który jest uporządkowaną sekwencją indywidualnych zadań optymalizacji modelu, zwanych przejściami – przykładowe przejścia obejmują kompresję modelu, przechwytywanie grafu, kwantyzację i optymalizację grafu. Każde przejście ma zestaw parametrów, które można dostroić, aby osiągnąć najlepsze metryki, takie jak dokładność i opóźnienie, które są oceniane przez odpowiedni ewaluator. Olive wykorzystuje strategię wyszukiwania, która wykorzystuje próbnik wyszukiwania do automatycznego dostrajania każdego przejścia indywidualnie lub zestawu przejść razem.


Korzyści z używania Olive
 Zmniejsz frustrację i czas ręcznego eksperymentowania metodą prób i błędów z różnymi technikami optymalizacji grafu, kompresji i kwantyzacji. Zdefiniuj swój cel i precyzję i pozwól Olive automatycznie wyprodukować dla Ciebie najlepszy model.

 Ponad 40 wbudowanych komponentów optymalizacji modeli obejmujących najnowocześniejsze techniki kwantyzacji, kompresji, optymalizacji grafu i dostrajania.

 Łatwe w użyciu CLI do typowych zadań optymalizacji modeli. Na przykład, olive quantize, olive auto-opt, olive finetune. Zaawansowani użytkownicy mogą łatwo konstruować przepływy pracy za pomocą YAML/JSON do orkiestracji transformacji modeli i kroków optymalizacji.

 Wbudowane pakowanie i wdrażanie modeli.

 Umożliwia obsługę Multi LoRA.

 Integracja z Hugging Face i Azure AI.

 Wbudowany mechanizm buforowania w celu poprawy produktywności zespołu.

🚀 Rozpoczęcie pracy

✨ Szybki start

Jeśli wolisz używać wiersza polece bezpośrednio zamiast notatników Jupyter, przedstawiliśmy tutaj polecenia szybkiego startu.

1. Zainstaluj Olive CLI

Zalecamy instalowanie Olive w środowisku wirtualnym lub środowisku conda.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
Uwaga

Olive ma opcjonalne zależności, które można zainstalować, aby włączyć dodatkowe funkcje. Zapoznaj się z konfiguracją pakietu Olive, aby uzyskać listę dodatków i ich zależności.

2. Automatyczny optymalizator

W tym szybkim starcie będziesz optymalizować Qwen/Qwen2.5-0.5B-Instruct, który ma wiele plików modelu w repozytorium Hugging Face dla różnych precyzji, które nie są wymagane przez Olive.

Uruchom automatyczną optymalizację:
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
WskazówkaUżytkownicy PowerShellKontynuacja wiersza między Bash a PowerShell nie jest zamienna. Jeśli używasz PowerShell, możesz skopiować i wkleić następujące polecenie, które używa zgodnej kontynuacji wiersza.
```powershell
olive optimize `
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `
   --output_path models/qwen `
   --precision int4
```

Automatyczny optymalizator:

Pobierze model z repozytorium modeli Hugging Face.
Skwantyzuje model do int4 za pomocą GPTQ.
Przechwyci graf ONNX i zapisze wagi w pliku danych ONNX.
Zoptymalizuje graf ONNX.
Olive może automatycznie optymalizować popularne architektury modeli, takie jak Llama, Phi, Qwen, Gemma itp. od razu po wyjęciu z pudełka – zobacz szczegółową listę tutaj. Ponadto możesz optymalizować inne architektury modeli, podając szczegóły dotyczące wejść/wyjść modelu (io_config).

3. Wnioskowanie w środowisku uruchomieniowym ONNX

Środowisko uruchomieniowe ONNX (ORT) to szybki i lekki, wieloplatformowy silnik wnioskowania z powiązaniami dla popularnych języków programowania, takich jak Python, C/C++, C#, Java, JavaScript itp. ORT umożliwia włączanie modeli AI do aplikacji, dzięki czemu wnioskowanie jest obsługiwane na urządzeniu.

Przykładowa aplikacja czatu do uruchomienia znajduje się jako model-chat.py w repozytorium Github onnxruntime-genai.
```