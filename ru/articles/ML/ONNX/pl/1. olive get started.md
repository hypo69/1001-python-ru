Olive (ONNX LIVE) to najnowoczeÅ›niejszy zestaw narzÄ™dzi do optymalizacji modeli z towarzyszÄ…cym CLI, ktÃ³ry umoÅ¼liwia dostarczanie modeli dla Å›rodowiska uruchomieniowego ONNX z jakoÅ›ciÄ… i wydajnoÅ›ciÄ….

WejÅ›ciem do Olive jest zazwyczaj model PyTorch lub Hugging Face, a wyjÅ›ciem jest zoptymalizowany model ONNX, ktÃ³ry jest wykonywany na urzÄ…dzeniu (celu wdroÅ¼enia) dziaÅ‚ajÄ…cym w Å›rodowisku uruchomieniowym ONNX. Olive zoptymalizuje model dla akceleratora AI (NPU, GPU, CPU) celu wdroÅ¼enia dostarczonego przez dostawcÄ™ sprzÄ™tu, takiego jak Qualcomm, AMD, Nvidia lub Intel.


Olive wykonuje przepÅ‚yw pracy, ktÃ³ry jest uporzÄ…dkowanÄ… sekwencjÄ… indywidualnych zadaÅ„ optymalizacji modelu, zwanych przejÅ›ciami â€“ przykÅ‚adowe przejÅ›cia obejmujÄ… kompresjÄ™ modelu, przechwytywanie grafu, kwantyzacjÄ™ i optymalizacjÄ™ grafu. KaÅ¼de przejÅ›cie ma zestaw parametrÃ³w, ktÃ³re moÅ¼na dostroiÄ‡, aby osiÄ…gnÄ…Ä‡ najlepsze metryki, takie jak dokÅ‚adnoÅ›Ä‡ i opÃ³Åºnienie, ktÃ³re sÄ… oceniane przez odpowiedni ewaluator. Olive wykorzystuje strategiÄ™ wyszukiwania, ktÃ³ra wykorzystuje prÃ³bnik wyszukiwania do automatycznego dostrajania kaÅ¼dego przejÅ›cia indywidualnie lub zestawu przejÅ›Ä‡ razem.


KorzyÅ›ci z uÅ¼ywania Olive
 Zmniejsz frustracjÄ™ i czas rÄ™cznego eksperymentowania metodÄ… prÃ³b i bÅ‚Ä™dÃ³w z rÃ³Å¼nymi technikami optymalizacji grafu, kompresji i kwantyzacji. Zdefiniuj swÃ³j cel i precyzjÄ™ i pozwÃ³l Olive automatycznie wyprodukowaÄ‡ dla Ciebie najlepszy model.

 Ponad 40 wbudowanych komponentÃ³w optymalizacji modeli obejmujÄ…cych najnowoczeÅ›niejsze techniki kwantyzacji, kompresji, optymalizacji grafu i dostrajania.

 Åatwe w uÅ¼yciu CLI do typowych zadaÅ„ optymalizacji modeli. Na przykÅ‚ad, olive quantize, olive auto-opt, olive finetune. Zaawansowani uÅ¼ytkownicy mogÄ… Å‚atwo konstruowaÄ‡ przepÅ‚ywy pracy za pomocÄ… YAML/JSON do orkiestracji transformacji modeli i krokÃ³w optymalizacji.

 Wbudowane pakowanie i wdraÅ¼anie modeli.

 UmoÅ¼liwia obsÅ‚ugÄ™ Multi LoRA.

 Integracja z Hugging Face i Azure AI.

 Wbudowany mechanizm buforowania w celu poprawy produktywnoÅ›ci zespoÅ‚u.

ğŸš€ RozpoczÄ™cie pracy

âœ¨ Szybki start

JeÅ›li wolisz uÅ¼ywaÄ‡ wiersza polece bezpoÅ›rednio zamiast notatnikÃ³w Jupyter, przedstawiliÅ›my tutaj polecenia szybkiego startu.

1. Zainstaluj Olive CLI

Zalecamy instalowanie Olive w Å›rodowisku wirtualnym lub Å›rodowisku conda.
```python
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
Uwaga

Olive ma opcjonalne zaleÅ¼noÅ›ci, ktÃ³re moÅ¼na zainstalowaÄ‡, aby wÅ‚Ä…czyÄ‡ dodatkowe funkcje. Zapoznaj siÄ™ z konfiguracjÄ… pakietu Olive, aby uzyskaÄ‡ listÄ™ dodatkÃ³w i ich zaleÅ¼noÅ›ci.

2. Automatyczny optymalizator

W tym szybkim starcie bÄ™dziesz optymalizowaÄ‡ Qwen/Qwen2.5-0.5B-Instruct, ktÃ³ry ma wiele plikÃ³w modelu w repozytorium Hugging Face dla rÃ³Å¼nych precyzji, ktÃ³re nie sÄ… wymagane przez Olive.

Uruchom automatycznÄ… optymalizacjÄ™:
```bash
olive optimize \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --precision int4 \
    --output_path models/qwen
```
WskazÃ³wkaUÅ¼ytkownicy PowerShellKontynuacja wiersza miÄ™dzy Bash a PowerShell nie jest zamienna. JeÅ›li uÅ¼ywasz PowerShell, moÅ¼esz skopiowaÄ‡ i wkleiÄ‡ nastÄ™pujÄ…ce polecenie, ktÃ³re uÅ¼ywa zgodnej kontynuacji wiersza.
```powershell
olive optimize `
   --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct `
   --output_path models/qwen `
   --precision int4
```

Automatyczny optymalizator:

Pobierze model z repozytorium modeli Hugging Face.
Skwantyzuje model do int4 za pomocÄ… GPTQ.
Przechwyci graf ONNX i zapisze wagi w pliku danych ONNX.
Zoptymalizuje graf ONNX.
Olive moÅ¼e automatycznie optymalizowaÄ‡ popularne architektury modeli, takie jak Llama, Phi, Qwen, Gemma itp. od razu po wyjÄ™ciu z pudeÅ‚ka â€“ zobacz szczegÃ³Å‚owÄ… listÄ™ tutaj. Ponadto moÅ¼esz optymalizowaÄ‡ inne architektury modeli, podajÄ…c szczegÃ³Å‚y dotyczÄ…ce wejÅ›Ä‡/wyjÅ›Ä‡ modelu (io_config).

3. Wnioskowanie w Å›rodowisku uruchomieniowym ONNX

Åšrodowisko uruchomieniowe ONNX (ORT) to szybki i lekki, wieloplatformowy silnik wnioskowania z powiÄ…zaniami dla popularnych jÄ™zykÃ³w programowania, takich jak Python, C/C++, C#, Java, JavaScript itp. ORT umoÅ¼liwia wÅ‚Ä…czanie modeli AI do aplikacji, dziÄ™ki czemu wnioskowanie jest obsÅ‚ugiwane na urzÄ…dzeniu.

PrzykÅ‚adowa aplikacja czatu do uruchomienia znajduje siÄ™ jako model-chat.py w repozytorium Github onnxruntime-genai.
```