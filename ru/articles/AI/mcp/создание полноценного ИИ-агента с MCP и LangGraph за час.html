<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>&Pcy;&rcy;&acy;&kcy;&tcy;&icy;&chcy;&iecy;&scy;&kcy;&ocy;&iecy; &rcy;&ucy;&kcy;&ocy;&vcy;&ocy;&dcy;&scy;&tcy;&vcy;&ocy; &pcy;&ocy; &scy;&ocy;&zcy;&dcy;&acy;&ncy;&icy;&yucy; &Icy;&Icy;-&acy;&gcy;&iecy;&ncy;&tcy;&ocy;&vcy; &scy; LangGraph &icy; MCP</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h2 id="практическое-руководство-по-созданию-ии-агентов-с-langgraph-и-mcp">Практическое руководство по созданию ИИ-агентов с LangGraph и MCP</h2>
<p>Эта статья — практическое руководство для разработчиков по созданию автономных ИИ-агентов на Python. Мы не будем повторять теорию о том, что такое LangChain и LangGraph. Вместо этого мы сосредоточимся на коде, архитектуре и решении реальных задач.</p>
<p><strong>Цель:</strong> С нуля создать два проекта:</p>
<ol>
<li><strong>Агент-классификатор:</strong> многошаговый, с управляемым состоянием, но без внешних инструментов.</li>
<li><strong>Агент-ассистент:</strong> полноценный агент с доступом к файловой системе и веб-поиску через протокол MCP, построенный на циклической логике.</li>
</ol>
<p>Мы рассмотрим лучшие практики: управление конфигурацией, выбор моделей и обработку ошибок для создания надежных систем.</p>
<h3 id="кратко-о-концепции-агент-и-мост-mcp">Кратко о концепции: Агент и мост MCP</h3>
<p>Прежде чем перейти к коду, зафиксируем два понятия:</p>
<ul>
<li><strong>ИИ-агент:</strong> это программа, построенная вокруг цикла «рассуждение-действие». Она получает задачу, с помощью LLM решает, что делать дальше (например, вызвать инструмент), выполняет действие и повторяет цикл, пока задача не будет решена.</li>
<li><strong>MCP (Model Context Protocol):</strong> это стандарт, который служит <strong>мостом</strong> между логикой агента и внешними инструментами. Он позволяет агенту унифицированно работать с файлами, API или поиском, не заботясь о деталях их реализации.</li>
</ul>
<h3 id="часть-1-настройка-надежного-окружения">Часть 1: Настройка надежного окружения</h3>
<h4 id="шаг-1-виртуальное-окружение-и-зависимости">Шаг 1: Виртуальное окружение и зависимости</h4>
<p>Создайте и активируйте виртуальное окружение. Затем создайте файл <code>requirements.txt</code>:</p>
<pre><code># Основные фреймворки
langchain
langgraph

# Адаптеры для моделей
langchain-openai
langchain-google-genai
langchain-mistralai
langchain-community # Для Ollama

# Инструменты и протоколы
langchain-mcp-adapters
mcp
ollama

# Вспомогательные
python-dotenv
tenacity # Для надежной обработки ошибок
</code></pre>
<p>Установите зависимости:</p>
<pre><code class="language-bash">pip install -r requirements.txt```

<span class="hljs-comment">#### Шаг 2: Конфигурация API-ключей</span>

Создайте файл `.<span class="hljs-built_in">env</span>` для хранения ключей:
</code></pre>
<p>OPENAI_API_KEY=&quot;sk-...&quot;
GOOGLE_API_KEY=&quot;AIzaSy...&quot;
MISTRAL_API_KEY=&quot;...&quot;
BRAVE_API_KEY=&quot;...&quot; # Для инструмента веб-поиска через MCP</p>
<pre><code>
#### Шаг 3: Паттерн «Фабрика моделей»

Чтобы гибко переключаться между облачными и локальными моделями, не меняя код агента, используем паттерн «фабрика».

```python
# llm_factory.py
import os
from enum import Enum
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_mistralai import ChatMistralAI
from langchain_community.chat_models import ChatOllama

load_dotenv()

class ModelProvider(Enum):
    OPENAI = &quot;openai&quot;
    GEMINI = &quot;gemini&quot;
    MISTRAL_API = &quot;mistral_api&quot;
    OLLAMA = &quot;ollama&quot;

def get_llm(provider: ModelProvider, model_name: str = None):
    &quot;&quot;&quot;Фабрика для создания экземпляров LLM.&quot;&quot;&quot;
    if provider == ModelProvider.OPENAI:
        return ChatOpenAI(model=model_name or &quot;gpt-4o-mini&quot;, temperature=0)
    elif provider == ModelProvider.GEMINI:
        return ChatGoogleGenerativeAI(model=model_name or &quot;gemini-1.5-flash&quot;, temperature=0)
    elif provider == ModelProvider.MISTRAL_API:
        return ChatMistralAI(model=model_name or &quot;mistral-large-latest&quot;, temperature=0)
    elif provider == ModelProvider.OLLAMA:
        # Убедитесь, что у вас запущена Ollama с нужной моделью
        # docker exec -it ollama ollama pull mistral
        return ChatOllama(model=model_name or &quot;mistral&quot;, temperature=0)
    raise ValueError(f&quot;Неизвестный провайдер модели: {provider}&quot;)

# Пример использования
if __name__ == &quot;__main__&quot;:
    # local_llm = get_llm(ModelProvider.OLLAMA)
    openai_llm = get_llm(ModelProvider.OPENAI)
    response = openai_llm.invoke(&quot;Объясни концепцию RAG в трех предложениях.&quot;)
    print(response.content)
</code></pre>
<h3 id="часть-2-проект-1--агент-для-классификации-вакансий">Часть 2: Проект 1 — Агент для классификации вакансий</h3>
<p>Этот агент демонстрирует, как использовать LangGraph для создания <strong>линейного графа</strong> с управляемым состоянием. Он будет принимать описание вакансии и последовательно классифицировать его по трём параметрам.</p>
<h4 id="шаг-1-определение-состояния">Шаг 1: Определение состояния</h4>
<p>Состояние — это «память» нашего графа, которая передается от узла к узлу.</p>
<pre><code class="language-python"><span class="hljs-comment"># vacancy_classifier.py</span>
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> TypedDict, <span class="hljs-type">Dict</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">ClassificationState</span>(<span class="hljs-title class_ inherited__">TypedDict</span>):
    <span class="hljs-string">&quot;&quot;&quot;Состояние агента-классификатора.&quot;&quot;&quot;</span>
    description: <span class="hljs-built_in">str</span>          <span class="hljs-comment"># Исходный текст</span>
    job_type: <span class="hljs-built_in">str</span>             <span class="hljs-comment"># Тип работы (проектная/постоянная)</span>
    category: <span class="hljs-built_in">str</span>             <span class="hljs-comment"># Профессия</span>
    search_type: <span class="hljs-built_in">str</span>          <span class="hljs-comment"># Цель (ищу работу/исполнителя)</span>
    classification_log: <span class="hljs-built_in">list</span>  <span class="hljs-comment"># Лог для отладки</span>
</code></pre>
<h4 id="шаг-2-реализация-узлов-графа">Шаг 2: Реализация узлов графа</h4>
<p>Каждый узел — это функция, которая принимает состояние, выполняет свою часть работы и возвращает обновленное состояние.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> asyncio
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> langchain_core.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate
<span class="hljs-keyword">from</span> llm_factory <span class="hljs-keyword">import</span> get_llm, ModelProvider

<span class="hljs-keyword">class</span> <span class="hljs-title class_">VacancyClassifierAgent</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):
        self.llm = get_llm(ModelProvider.OPENAI, model_name=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span>)

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_job_type</span>(<span class="hljs-params">self, state: ClassificationState</span>) -&gt; ClassificationState:
        <span class="hljs-string">&quot;&quot;&quot;Узел 1: Определяет тип работы.&quot;&quot;&quot;</span>
        prompt = ChatPromptTemplate.from_messages([
            (<span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;Определи тип работы. Ответ должен быть &#x27;проектная&#x27; или &#x27;постоянная&#x27;.&quot;</span>),
            (<span class="hljs-string">&quot;human&quot;</span>, <span class="hljs-string">&quot;Текст вакансии:\n\n{description}&quot;</span>)
        ])
        chain = prompt | self.llm
        result = <span class="hljs-keyword">await</span> chain.ainvoke({<span class="hljs-string">&quot;description&quot;</span>: state[<span class="hljs-string">&quot;description&quot;</span>]})
        
        state[<span class="hljs-string">&quot;job_type&quot;</span>] = result.content.strip()
        state[<span class="hljs-string">&quot;classification_log&quot;</span>].append(<span class="hljs-string">&quot;Определен тип работы.&quot;</span>)
        <span class="hljs-keyword">return</span> state

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_category</span>(<span class="hljs-params">self, state: ClassificationState</span>) -&gt; ClassificationState:
        <span class="hljs-string">&quot;&quot;&quot;Узел 2: Определяет категорию профессии.&quot;&quot;&quot;</span>
        <span class="hljs-comment"># Категории можно загрузить из файла или базы данных</span>
        categories = [<span class="hljs-string">&quot;Python-разработчик&quot;</span>, <span class="hljs-string">&quot;Дизайнер&quot;</span>, <span class="hljs-string">&quot;Маркетолог&quot;</span>, <span class="hljs-string">&quot;3D-аниматор&quot;</span>]
        prompt = ChatPromptTemplate.from_messages([
            (<span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">f&quot;Выбери наиболее подходящую категорию из списка: <span class="hljs-subst">{<span class="hljs-string">&#x27;, &#x27;</span>.join(categories)}</span>.&quot;</span>),
            (<span class="hljs-string">&quot;human&quot;</span>, <span class="hljs-string">&quot;Текст вакансии:\n\n{description}&quot;</span>)
        ])
        chain = prompt | self.llm
        result = <span class="hljs-keyword">await</span> chain.ainvoke({<span class="hljs-string">&quot;description&quot;</span>: state[<span class="hljs-string">&quot;description&quot;</span>]})
        
        state[<span class="hljs-string">&quot;category&quot;</span>] = result.content.strip()
        state[<span class="hljs-string">&quot;classification_log&quot;</span>].append(<span class="hljs-string">&quot;Определена категория.&quot;</span>)
        <span class="hljs-keyword">return</span> state

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_search_type</span>(<span class="hljs-params">self, state: ClassificationState</span>) -&gt; ClassificationState:
        <span class="hljs-string">&quot;&quot;&quot;Узел 3: Определяет цель поиска.&quot;&quot;&quot;</span>
        prompt = ChatPromptTemplate.from_messages([
            (<span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;Определи цель автора. Ответ должен быть &#x27;поиск работы&#x27; или &#x27;поиск исполнителя&#x27;.&quot;</span>),
            (<span class="hljs-string">&quot;human&quot;</span>, <span class="hljs-string">&quot;Текст вакансии:\n\n{description}&quot;</span>)
        ])
        chain = prompt | self.llm
        result = <span class="hljs-keyword">await</span> chain.ainvoke({<span class="hljs-string">&quot;description&quot;</span>: state[<span class="hljs-string">&quot;description&quot;</span>]})
        
        state[<span class="hljs-string">&quot;search_type&quot;</span>] = result.content.strip()
        state[<span class="hljs-string">&quot;classification_log&quot;</span>].append(<span class="hljs-string">&quot;Определена цель поиска.&quot;</span>)
        <span class="hljs-keyword">return</span> state
</code></pre>
<h4 id="шаг-3-сборка-и-запуск-графа">Шаг 3: Сборка и запуск графа</h4>
<p>Собираем узлы в единый рабочий процесс.</p>
<pre><code class="language-python"><span class="hljs-comment"># ... продолжение класса VacancyClassifierAgent ...</span>
<span class="hljs-keyword">from</span> langgraph.graph <span class="hljs-keyword">import</span> StateGraph, END

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_graph</span>(<span class="hljs-params">self</span>):
        <span class="hljs-string">&quot;&quot;&quot;Собирает граф состояний.&quot;&quot;&quot;</span>
        workflow = StateGraph(ClassificationState)
        
        workflow.add_node(<span class="hljs-string">&quot;job_type_classifier&quot;</span>, self._classify_job_type)
        workflow.add_node(<span class="hljs-string">&quot;category_classifier&quot;</span>, self._classify_category)
        workflow.add_node(<span class="hljs-string">&quot;search_type_classifier&quot;</span>, self._classify_search_type)
        
        workflow.set_entry_point(<span class="hljs-string">&quot;job_type_classifier&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;job_type_classifier&quot;</span>, <span class="hljs-string">&quot;category_classifier&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;category_classifier&quot;</span>, <span class="hljs-string">&quot;search_type_classifier&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;search_type_classifier&quot;</span>, END)
        
        <span class="hljs-keyword">return</span> workflow.<span class="hljs-built_in">compile</span>()

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    agent = VacancyClassifierAgent()
    graph = agent.build_graph()
    
    description = <span class="hljs-string">&quot;Ищем опытного Python-разработчика в команду на фултайм для работы над проектом в сфере финтех.&quot;</span>
    
    initial_state = ClassificationState(
        description=description,
        job_type=<span class="hljs-string">&quot;&quot;</span>, category=<span class="hljs-string">&quot;&quot;</span>, search_type=<span class="hljs-string">&quot;&quot;</span>,
        classification_log=[]
    )
    
    final_state = <span class="hljs-keyword">await</span> graph.ainvoke(initial_state)
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;--- Результат классификации ---&quot;</span>)
    <span class="hljs-built_in">print</span>(json.dumps(final_state, indent=<span class="hljs-number">2</span>, ensure_ascii=<span class="hljs-literal">False</span>))

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:
    asyncio.run(main())
</code></pre>
<h3 id="часть-3-проект-2--агент-ассистент-с-инструментами-mcp">Часть 3: Проект 2 — Агент-ассистент с инструментами (MCP)</h3>
<p>Этот агент демонстрирует <strong>циклическую логику</strong>, где он может многократно обращаться к инструментам для решения задачи.</p>
<h4 id="шаг-1-управление-конфигурацией">Шаг 1: Управление конфигурацией</h4>
<p>Для агентов, взаимодействующих с внешним миром, важна надежная конфигурация.</p>
<pre><code class="language-python"><span class="hljs-comment"># mcp_agent_config.py</span>
<span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> llm_factory <span class="hljs-keyword">import</span> ModelProvider

<span class="hljs-meta">@dataclass</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">AgentConfig</span>:
    workdir: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;./agent_workdir&quot;</span>
    model_provider: ModelProvider = ModelProvider.OLLAMA
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__post_init__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-string">&quot;&quot;&quot;Валидация после инициализации.&quot;&quot;&quot;</span>
        os.makedirs(self.workdir, exist_ok=<span class="hljs-literal">True</span>)
</code></pre>
<h4 id="шаг-2-определение-состояния-для-диалога">Шаг 2: Определение состояния для диалога</h4>
<p>Состояние теперь будет хранить историю сообщений.</p>
<pre><code class="language-python"><span class="hljs-comment"># mcp_agent.py</span>
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> TypedDict, Annotated, <span class="hljs-type">Sequence</span>
<span class="hljs-keyword">from</span> langchain_core.messages <span class="hljs-keyword">import</span> BaseMessage
<span class="hljs-keyword">import</span> operator

<span class="hljs-keyword">class</span> <span class="hljs-title class_">AgentState</span>(<span class="hljs-title class_ inherited__">TypedDict</span>):
    messages: Annotated[<span class="hljs-type">Sequence</span>[BaseMessage], operator.add]
</code></pre>
<h4 id="шаг-3-реализация-циклического-графа">Шаг 3: Реализация циклического графа</h4>
<p>Граф будет состоять из двух основных узлов и условного перехода, который создает цикл «рассуждение-действие».</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> langgraph.graph <span class="hljs-keyword">import</span> StateGraph, END
<span class="hljs-keyword">from</span> langgraph.prebuilt <span class="hljs-keyword">import</span> ToolExecutor
<span class="hljs-keyword">from</span> langchain_mcp_adapters.langchain <span class="hljs-keyword">import</span> V1ToolExecutor
<span class="hljs-keyword">from</span> langchain_mcp_adapters.clients <span class="hljs-keyword">import</span> MultiServerMCPClient
<span class="hljs-keyword">from</span> llm_factory <span class="hljs-keyword">import</span> get_llm
<span class="hljs-keyword">from</span> mcp_agent_config <span class="hljs-keyword">import</span> AgentConfig

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MCPAgent</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: AgentConfig</span>):
        self.config = config
        self.llm = get_llm(config.model_provider)
        self.tools = []
        self.tool_executor = <span class="hljs-literal">None</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_tools</span>(<span class="hljs-params">self</span>):
        <span class="hljs-string">&quot;&quot;&quot;Инициализация инструментов через MCP.&quot;&quot;&quot;</span>
        mcp_config = {
            <span class="hljs-string">&quot;filesystem&quot;</span>: {
                <span class="hljs-string">&quot;command&quot;</span>: <span class="hljs-string">&quot;npx&quot;</span>, <span class="hljs-string">&quot;args&quot;</span>: [<span class="hljs-string">&quot;-y&quot;</span>, <span class="hljs-string">&quot;@modelcontextprotocol/server-filesystem&quot;</span>, self.config.workdir],
                <span class="hljs-string">&quot;transport&quot;</span>: <span class="hljs-string">&quot;stdio&quot;</span>
            },
            <span class="hljs-comment"># Добавьте brave-search, если есть ключ BRAVE_API_KEY</span>
        }
        mcp_client = MultiServerMCPClient(mcp_config)
        self.tools = <span class="hljs-keyword">await</span> mcp_client.get_tools()
        self.tool_executor = ToolExecutor([V1ToolExecutor(tool) <span class="hljs-keyword">for</span> tool <span class="hljs-keyword">in</span> self.tools])
        
        <span class="hljs-comment"># Привязываем инструменты к модели</span>
        self.llm = self.llm.bind_tools(self.tools)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_should_continue</span>(<span class="hljs-params">self, state: AgentState</span>):
        <span class="hljs-string">&quot;&quot;&quot;Условный переход: решаем, нужно ли вызывать инструмент.&quot;&quot;&quot;</span>
        last_message = state[<span class="hljs-string">&#x27;messages&#x27;</span>][-<span class="hljs-number">1</span>]
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> last_message.tool_calls:
            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;end&quot;</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;continue&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_call_model</span>(<span class="hljs-params">self, state: AgentState</span>):
        <span class="hljs-string">&quot;&quot;&quot;Узел 1: Вызов LLM для принятия решения.&quot;&quot;&quot;</span>
        response = self.llm.invoke(state[<span class="hljs-string">&#x27;messages&#x27;</span>])
        <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;messages&quot;</span>: [response]}

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_call_tool</span>(<span class="hljs-params">self, state: AgentState</span>):
        <span class="hljs-string">&quot;&quot;&quot;Узел 2: Выполнение вызова инструмента.&quot;&quot;&quot;</span>
        last_message = state[<span class="hljs-string">&#x27;messages&#x27;</span>][-<span class="hljs-number">1</span>]
        tool_call = last_message.tool_calls[<span class="hljs-number">0</span>]
        
        action = {<span class="hljs-string">&quot;tool&quot;</span>: tool_call[<span class="hljs-string">&quot;name&quot;</span>], <span class="hljs-string">&quot;tool_input&quot;</span>: tool_call[<span class="hljs-string">&quot;args&quot;</span>], <span class="hljs-string">&quot;log&quot;</span>: <span class="hljs-string">&quot;&quot;</span>}
        response = self.tool_executor.invoke(action)
        
        <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;messages&quot;</span>: [response]}

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_graph</span>(<span class="hljs-params">self</span>):
        workflow = StateGraph(AgentState)
        workflow.add_node(<span class="hljs-string">&quot;agent&quot;</span>, self._call_model)
        workflow.add_node(<span class="hljs-string">&quot;action&quot;</span>, self._call_tool)
        
        workflow.set_entry_point(<span class="hljs-string">&quot;agent&quot;</span>)
        workflow.add_conditional_edges(
            <span class="hljs-string">&quot;agent&quot;</span>,
            self._should_continue,
            {<span class="hljs-string">&quot;continue&quot;</span>: <span class="hljs-string">&quot;action&quot;</span>, <span class="hljs-string">&quot;end&quot;</span>: END}
        )
        workflow.add_edge(<span class="hljs-string">&quot;action&quot;</span>, <span class="hljs-string">&quot;agent&quot;</span>)
        
        <span class="hljs-keyword">return</span> workflow.<span class="hljs-built_in">compile</span>()
</code></pre>
<h4 id="шаг-4-запуск-и-взаимодействие">Шаг 4: Запуск и взаимодействие</h4>
<pre><code class="language-python"><span class="hljs-comment"># ... продолжение mcp_agent.py ...</span>
<span class="hljs-keyword">import</span> asyncio
<span class="hljs-keyword">from</span> langchain_core.messages <span class="hljs-keyword">import</span> HumanMessage
<span class="hljs-keyword">from</span> tenacity <span class="hljs-keyword">import</span> retry, stop_after_attempt, wait_fixed

<span class="hljs-meta">@retry(<span class="hljs-params">stop=stop_after_attempt(<span class="hljs-params"><span class="hljs-number">3</span></span>), wait=wait_fixed(<span class="hljs-params"><span class="hljs-number">1</span></span>)</span>)</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_agent_task</span>(<span class="hljs-params">graph, task</span>):
    <span class="hljs-string">&quot;&quot;&quot;Запускает задачу с обработкой ошибок.&quot;&quot;&quot;</span>
    <span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> graph.ainvoke({<span class="hljs-string">&quot;messages&quot;</span>: [HumanMessage(content=task)]})

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    config = AgentConfig(model_provider=ModelProvider.OPENAI) <span class="hljs-comment"># или OLLAMA</span>
    agent = MCPAgent(config)
    <span class="hljs-keyword">await</span> agent.setup_tools()
    graph = agent.build_graph()
    
    task = <span class="hljs-string">&quot;Создай файл &#x27;hello.txt&#x27; в рабочей директории и запиши в него &#x27;Привет, мир!&#x27;.&quot;</span>
    result = <span class="hljs-keyword">await</span> run_agent_task(graph, task)
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n--- Финальный ответ агента ---&quot;</span>)
    <span class="hljs-built_in">print</span>(result[<span class="hljs-string">&#x27;messages&#x27;</span>][-<span class="hljs-number">1</span>].content)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:
    asyncio.run(main())
</code></pre>
<p>Здесь мы добавили декоратор <code>tenacity</code> для надежности — если вызов агента упадет из-за временной сетевой ошибки, он автоматически повторится.</p>
<h3 id="заключение">Заключение</h3>
<p>Мы создали два типа агентов, используя современные практики:</p>
<ul>
<li><strong>Линейный граф</strong> отлично подходит для задач с четкой последовательностью шагов, таких как ETL-процессы или многоэтапный анализ.</li>
<li><strong>Циклический граф</strong> является основой для создания интерактивных ассистентов и автономных агентов, способных решать сложные задачи с помощью инструментов.</li>
</ul>
<p>Представленные архитектурные паттерны — фабрика моделей, управление конфигурацией, разделение логики на узлы и использование графов состояний — являются фундаментом для построения масштабируемых и надежных ИИ-систем.</p>
<p>Начав с простых примеров, вы можете постепенно наращивать сложность, создавая все более интеллектуальные и автономные системы для решения ваших уникальных задач.
<img src="https://habr.com/ru/companies/amvera/articles/929568/" alt="habr"></p>

            
            
        </body>
        </html>