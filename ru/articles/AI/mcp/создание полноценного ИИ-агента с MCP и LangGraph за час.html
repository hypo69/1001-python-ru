<h1>Как научить нейросеть работать руками: создание полноценного ИИ-агента с МСР и LangGraph за час</h1>
<p>Друзья, приветствую! Надеюсь, успели соскучиться.</p>
<p>Последние пару месяцев я с головой ушёл в исследование интеграции ИИ-агентов в собственные Python-проекты. В процессе накопилось немало практических знаний и наблюдений, которыми просто грех не поделиться. Поэтому сегодня я возвращаюсь на Хабр — с новой темой, свежим взглядом и с намерением писать чаще.</p>
<p>На повестке дня — LangGraph и MCP: инструменты, с помощью которых можно создавать действительно полезных ИИ-агентов.</p>
<p>Если раньше мы спорили о том, какая нейросеть лучше отвечает на русском языке, то сегодня поле битвы сместилось в сторону более прикладных задач: кто лучше справляется с ролью ИИ-агента? Какие фреймворки действительно упрощают разработку? И как интегрировать всё это добро в реальный проект?</p>
<p>Но прежде чем нырнуть в практику и код, давайте разберёмся с базовыми понятиями. Особенно с двумя ключевыми: <strong>ИИ-агенты и МСР</strong>. Без них разговор про LangGraph будет неполным.</p>
<h3>ИИ-агенты простыми словами</h3>
<p>ИИ-агенты — это не просто «прокачанные» чат-боты. Они представляют собой более сложные, автономные сущности, которые обладают двумя важнейшими особенностями:</p>
<ol>
<li><strong>Умение взаимодействовать и координироваться</strong></li>
</ol>
<p>Современные агенты способны делить задачи на подзадачи, вызывать других агентов, запрашивать внешние данные, работать в команде. Это уже не одиночный ассистент, а распределённая система, где каждый компонент может вносить свой вклад.</p>
<ol start="2">
<li><strong>Доступ к внешним ресурсам</strong></li>
</ol>
<p>ИИ-агент больше не ограничен рамками диалога. Он может обращаться к базам данных, выполнять вызовы к АРІ, взаимодействовать с локальными файлами, векторными хранилищами знаний и даже запускать команды в терминале. Всё это стало возможным благодаря появлению МСР — нового уровня интеграции между моделью и средой.</p>
<hr>
<p>Если говорить просто: <strong>МСР — это мост между нейросетью и её окружением</strong>. Он позволяет модели «понимать» контекст задачи, получать доступ к данным, выполнять вызовы и формировать обоснованные действия, а не просто выдавать текстовые ответы.</p>
<p><strong>Представим аналогию:</strong></p>
<ul>
<li>У вас есть <strong>нейросеть</strong> — она умеет рассуждать и генерировать тексты.</li>
<li>Есть <strong>данные и инструменты</strong> — документы, АРІ, базы знаний, терминал, код.</li>
<li>И есть <strong>МСР</strong> — это интерфейс, который позволяет модели взаимодействовать с этими внешними источниками так, как если бы они были частью её внутреннего мира.</li>
</ul>
<p><strong>Без МСР:</strong></p>
<p>Модель — это изолированный диалоговый движок. Вы подаёте ей текст — она отвечает. И всё.</p>
<p><strong>С МСР:</strong></p>
<p>Модель становится полноценным <strong>исполнителем задач</strong>:</p>
<ul>
<li>получает доступ к структурам данных и АРІ;</li>
<li>вызывает внешние функции;</li>
<li>ориентируется в текущем состоянии проекта или приложения;</li>
<li>может запоминать, отслеживать и изменять контекст по мере диалога;</li>
<li>использует расширения, такие как инструменты поиска, код-раннеры, базу векторных эмбеддингов и пр.</li>
</ul>
<p>В техническом смысле <strong>МСР — это протокол взаимодействия между LLM и её окружением</strong>, где контекст подаётся в виде структурированных объектов (вместо «сырого» текста), а вызовы оформляются как интерактивные операции (например, function calling, tool usage или agent actions). Именно это и превращает обычную модель в <strong>настоящего ИИ-агента</strong>, способного делать больше, чем просто "поговорить".</p>
<h3>А теперь — к делу!</h3>
<p>Теперь, когда мы разобрались с базовыми понятиями, логично задаться вопросом: «Как всё это реализовать на практике в Python?»</p>
<p>Вот здесь и вступает в игру <strong>LangGraph</strong> — мощный фреймворк для построения графов состояний, поведения агентов и цепочек мышления. Он позволяет "прошивать" логику взаимодействия между агентами, инструментами и пользователем, создавая живую архитектуру ИИ, адаптирующуюся к задачам.</p>
<p>В следующих разделах мы посмотрим, как:</p>
<ul>
<li>строится агент с нуля;</li>
<li>создаются состояния, переходы и события;</li>
<li>интегрируются функции и инструменты;</li>
<li>и как вся эта экосистема работает в реальном проекте.</li>
</ul>
<h3>Немного теории: что такое LangGraph</h3>
<p>Прежде чем приступить к практике, нужно сказать пару слов о самом фреймворке.</p>
<p><strong>LangGraph</strong> — это проект от команды <strong>LangChain</strong>, тех самых, кто первыми предложили концепцию «цепочек» (chains) взаимодействия с LLM. Если раньше основной упор делался на линейные или условно-ветвящиеся пайплайны (langchain.chains), то теперь разработчики делают ставку на <strong>графовую модель</strong>, и именно LangGraph они рекомендуют как новое «ядро» для построения сложных ИИ-систем.</p>
<p><strong>LangGraph</strong> — это фреймворк для построения конечных автоматов и графов состояний, в которых каждый <strong>нода (или узел)</strong> представляет собой часть логики агента: вызов модели, внешний инструмент, условие, пользовательский ввод и т.д.</p>
<h3>Как это работает: графы и узлы</h3>
<p>Концептуально, LangGraph строится на следующих идеях:</p>
<ul>
<li><strong>Граф</strong> — это структура, которая описывает возможные пути выполнения логики. Можно думать о нём как о карте: из одной точки можно перейти в другую в зависимости от условий или результата выполнения.</li>
<li><strong>Узлы (ноды)</strong> — это конкретные шаги внутри графа. Каждый узел выполняет какую-то функцию: вызывает модель, вызывает внешний АРІ, проверяет условие или просто обновляет внутреннее состояние.</li>
<li><strong>Переходы между узлами</strong> — это логика маршрутизации: если результат предыдущего шага такой-то, то идём туда-то.</li>
<li><strong>Состояние (state)</strong> — передаётся между узлами и накапливает всё, что нужно: историю, промежуточные выводы, пользовательский ввод, результат работы инструментов и т. д.</li>
</ul>
<p>Таким образом, мы получаем <strong>гибкий механизм управления логикой агента</strong>, в котором можно описывать как простые, так и очень сложные сценарии: циклы, условия, параллельные действия, вложенные вызовы и многое другое.</p>
<h3>Почему это удобно?</h3>
<p>LangGraph позволяет строить <strong>прозрачную, воспроизводимую и расширяемую логику</strong>:</p>
<ul>
<li>легко отлаживать;</li>
<li>легко визуализировать;</li>
<li>легко масштабировать под новые задачи;</li>
<li>легко интегрировать внешние инструменты и МСР-протоколы.</li>
</ul>
<p>По сути, LangGraph — это <strong>«мозг» агента</strong>, где каждый шаг задокументирован, контролируем и может быть изменён без хаоса и «магии».</p>
<h3>Ну а теперь — хватит теории!</h3>
<p>Можно ещё долго рассказывать о графах, состояний, композиции логики и преимуществах LangGraph над классическими пайплайнами. Но, как показывает практика, лучше один раз увидеть в коде.</p>
<p><strong>Пора перейти к практике.</strong> Дальше — пример на Python: создадим простого, но полезного ИИ-агента на базе LangGraph, который будет использовать внешние инструменты, память и принимать решения сам.</p>
<h3>Подготовка: облачные и локальные нейросети</h3>
<p>Для того чтобы приступить к созданию ИИ-агентов, нам в первую очередь нужен <strong>мозг</strong> — языковая модель. Здесь есть два подхода:</p>
<ul>
<li><strong>использовать облачные решения</strong>, где всё готово «из коробки»;</li>
<li>или <strong>поднять модель локально</strong> — для полной автономии и конфиденциальности.</li>
</ul>
<p>Рассмотрим оба варианта.</p>
<h4>Облачные сервисы: быстро и удобно</h4>
<p>Самый простой путь — воспользоваться мощностями крупных провайдеров: OpenAI, Anthropic, и использовать...</p>
<h3>Где взять ключи и токены:</h3>
<ul>
<li><strong>OpenAI</strong> — ChatGPT и другие продукты;</li>
<li><strong>Anthropic</strong> — Claude;</li>
<li><strong>OpenRouter.ai</strong> — десятки моделей (один токен — множество моделей через OpenAI-совместимый АРІ);</li>
<li><strong>Amvera Cloud</strong> — возможность подключить LLAMA с оплатой рублями и встроенным проксированием до OpenAI и Anthropic.</li>
</ul>
<p>Этот путь удобен, особенно если вы:</p>
<ul>
<li>не хотите настраивать инфраструктуру;</li>
<li>разрабатываете с упором на скорость;</li>
<li>работаете с ограниченными ресурсами.</li>
</ul>
<h4>Локальные модели: полный контроль</h4>
<p>Если вам важна <strong>приватность, работа без интернета</strong> или вы хотите строить <strong>полностью автономные агенты</strong>, то имеет смысл развернуть нейросеть локально.</p>
<p><strong>Основные преимущества:</strong></p>
<ul>
<li><strong>Конфиденциальность</strong> — данные остаются у вас;</li>
<li><strong>Работа без интернета</strong> — полезно в изолированных сетях;</li>
<li><strong>Отсутствие подписок и токенов</strong> — бесплатно после настройки.</li>
</ul>
<p><strong>Недостатки очевидны:</strong></p>
<ul>
<li>Требования к ресурсам (особенно к видеопамяти);</li>
<li>Настройка может занять время;</li>
<li>Некоторые модели сложно развернуть без опыта.</li>
</ul>
<p>Тем не менее, есть инструменты, которые делают локальный запуск проще. Один из лучших на сегодня — это <strong>Ollama</strong>.</p>
<h3>Развёртывание локальной LLM через Ollama + Docker</h3>
<p>Мы подготовим локальный запуск модели Qwen 2.5 (qwen2.5:32b) с использованием Docker-контейнера и системы Ollama. Это позволит интегрировать нейросеть с МСР и использовать её в собственных агентах на базе LangGraph.</p>
<p>Если вычислительных ресурсов вашего компьютера или сервера окажется недостаточно для работы с данной версией модели, вы всегда можете выбрать менее "прожорливую" нейросеть — процесс установки и запуска останется аналогичным.</p>
<p><strong>Быстрая установка (сводка шагов)</strong></p>
<ol>
<li><strong>Установите Docker + Docker Compose</strong></li>
<li><strong>Создайте структуру проекта:</strong>
<pre class="line-numbers"><code class="language-bash">mkdir qwen-local && cd qwen-local
</code></pre>
</li>
<li><strong>Создайте <code>docker-compose.yml</code></strong>
(универсальный вариант, GPU определяется автоматически)
<pre class="line-numbers"><code class="language-yaml">services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
</code></pre>
</li>
<li><strong>Запустите контейнер:</strong>
<pre class="line-numbers"><code class="language-bash">docker compose up -d
</code></pre>
</li>
<li><strong>Загрузите модель:</strong>
<pre class="line-numbers"><code class="language-bash">docker exec -it ollama_qwen ollama pull qwen2.5:32b
</code></pre>
</li>
<li><strong>Проверьте работу через API:</strong>
<pre class="line-numbers"><code class="language-bash">curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Привет!", "stream": false}'
</code></pre>
<p>*(Изображение с результатом выполнения команды curl)*</p>
</li>
<li><strong>Интеграция с Python:</strong>
<pre class="line-numbers"><code class="language-python">import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Объясни квантовую запутанность"))
</code></pre>
<p>Теперь у вас полноценная локальная LLM, готовая к работе с МСР и LangGraph.</p>
</li>
</ol>
<p><strong>Что дальше?</strong></p>
<p>У нас есть выбор между облачными и локальными моделями, и мы научились подключать обе. Самое интересное впереди — <strong>создание ИИ-агентов на LangGraph</strong>, которые используют выбранную модель, память, инструменты и собственную логику.</p>
<p><strong>Переходим к самому вкусному — коду и практике!</strong></p>
<hr>
<p>Перед тем как перейти к практике, важно подготовить рабочее окружение. Я предполагаю, что вы уже знакомы с основами Python, знаете, что такое библиотеки и зависимости, и понимаете, зачем использовать виртуальное окружение.</p>
<p>Если всё это вам в новинку — рекомендую сначала пройти короткий курс или гайд по Python-базе, а затем возвращаться к статье.</p>
<h4>Шаг 1: Создание виртуального окружения</h4>
<p>Создайте новое виртуальное окружение в папке проекта:</p>
<pre class="line-numbers"><code class="language-bash">python -m venv venv
source venv/bin/activate  # для Linux/macOS
venc\Scripts\activate   # для Windows
</code></pre>
<h4>Шаг 2: Установка зависимостей</h4>
<p>Создайте файл <code>requirements.txt</code> и добавьте в него следующие строки:</p>
<pre class="line-numbers"><code class="language-text">langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
</code></pre>
<blockquote>
<p>⚠️ <strong>Актуальные версии указаны на 21 июля 2025 года.</strong> С момента публикации они могли измениться — <strong>проверяйте актуальность перед установкой.</strong></p>
</blockquote>
<p>Затем установите зависимости:</p>
<pre class="line-numbers"><code class="language-bash">pip install -r requirements.txt</code></pre>
<h4>Шаг 3: Конфигурация переменных окружения</h4>
<p>Создайте в корне проекта файл <code>.env</code> и добавьте в него нужные АРІ-ключи:</p>
<pre class="line-numbers"><code class="language-text">OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
</code></pre>
<p><strong>Назначение переменных:</strong></p>
<ul>
<li><strong>OPENAI_API_KEY</strong> — ключ для доступа к GPT-моделям от OpenAI;</li>
<li><strong>DEEPSEEK_API_KEY</strong> — ключ для использования моделей Deepseek;</li>
<li><strong>OPENROUTER_API_KEY</strong> — единый ключ для доступа к множеству моделей через OpenRouter</li>
</ul>
<hr>
<p>Некоторые МСР-инструменты (например, <code>brave-web-search</code>) требуют ключ для работы. Без него они просто не активируются.</p>
<p><strong>А если у вас нет АРІ-ключей?</strong></p>
<p>Не проблема. Вы можете начать разработку с локальной моделью (например, через Ollama), не подключая ни одного внешнего сервиса. В этом случае файл <code>.env</code> можно не создавать вовсе.</p>
<p>Готово! Теперь у нас есть всё необходимое для начала — изолированное окружение, зависимости, и, при необходимости, доступ к облачным нейросетям и МСР-интеграциям.</p>
<p>Далее - запустим нашего LLM-агента разными способами.</p>
<h3>Простой запуск LLM-агентов через LangGraph: базовая интеграция</h3>
<p>Начнём с самого простого: как «подключить мозг» к будущему агенту. Мы разберём базовые способы запуска языковых моделей (LLM) с помощью LangChain, чтобы в следующем шаге перейти к интеграции с LangGraph и построению полноценного ИИ-агента.</p>
<h4>Импорты</h4>
<pre class="line-numbers"><code class="language-python">import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
</code></pre>
<ul>
<li><code>os</code> и <code>load_dotenv()</code> — для загрузки переменных из <code>.env</code>-файла.</li>
<li><code>ChatOpenAI</code>, <code>ChatOllama</code>, <code>ChatDeepSeek</code> — обёртки для подключения языковых моделей через LangChain.</li>
</ul>
<blockquote>
<p>💡 Если вы используете альтернативные подходы к работе с конфигурациями (например, Pydantic Settings), можете заменить <code>load_dotenv()</code> на свой привычный способ.</p>
</blockquote>
<h4>Загрузка переменных окружения</h4>
<pre class="line-numbers"><code class="language-python">load_dotenv()
</code></pre>
<p>Это подгрузит все переменные из <code>.env</code>, включая ключи для доступа к API OpenAI, DeepSeek, OpenRouter и другим.</p>
<h4>Простые функции для получения LLM</h4>
<p><strong>OpenAI</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
</code></pre>
<p>Если переменная <code>OPENAI_API_KEY</code> корректно задана, LangChain подставит её автоматически — явное указание <code>api_key=...</code> здесь опционально.</p>
<p><strong>DeepSeek</strong></p>
<pre class="line-numbers"><code class="language-python">def get_deepseek_llm():
    # ...
</code></pre>
<p>Аналогично, но используем обёртку <code>ChatDeepSeek</code>.</p>
<p><strong>OpenRouter (и другие совместимые API)</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
</code></pre>
<p><strong>Особенности:</strong></p>
<ul>
<li><code>ChatOpenAI</code> используется, несмотря на то что модель не от OpenAI — потому что OpenRouter использует тот же протокол.</li>
<li><code>base_url</code> обязателен: он указывает на OpenRouter API.</li>
<li>Модель <code>moonshotai/kimi-k2:free</code> выбрана как один из наиболее сбалансированных вариантов по качеству и скорости на момент написания статьи.</li>
<li>АРІ-ключ <code>OpenRouter</code> нужно передавать явно — автоматическая подстановка здесь не работает.</li>
</ul>
<h4>Мини-тест: проверка работы модели</h4>
<pre class="line-numbers"><code class="language-python">if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Кто ты?")
    print(response.content)
</code></pre>
<p>*(Изображение с результатом выполнения: <code>Я - ИИ-ассистент, созданный компанией Moonshot AI...</code>)*</p>
<p>Если всё настроено правильно, вы получите осмысленный ответ от модели. Поздравляю — первый шаг сделан!</p>
<h3>Но это ещё не агент</h3>
<p>На текущем этапе мы подключили LLM и сделали простой вызов. Это больше похоже на консольного чат-бота, чем на ИИ-агента.</p>
<p><strong>Почему?</strong></p>
<ul>
<li>Мы пишем <strong>синхронный, линейный код</strong> без логики состояния или целей.</li>
<li>Агент не принимает решений, не запоминает контекст и не использует инструменты.</li>
<li>МСР и LangGraph пока не задействованы.</li>
</ul>
<p><strong>Что дальше?</strong></p>
<p>Далее мы реализуем <strong>полноценного ИИ-агента</strong> с использованием <strong>LangGraph</strong> — сначала без МСР, чтобы сфокусироваться на архитектуре, состояниях и логике самого агента.</p>
<p>Погружаемся в настоящую агентную механику. Поехали!</p>
<h3>Агент классификации вакансий: от теории к практике</h3>
<p>...концепции LangGraph на практике и создать полезный инструмент для HR-платформ и бирж фриланса.</p>
<h4>Задача агента</h4>
<p>Наш агент принимает на вход текстовое описание вакансии или услуги и выполняет трёхуровневую классификацию:</p>
<ol>
<li><strong>Тип работы</strong>: проектная работа или постоянная вакансия</li>
<li><strong>Категория профессии</strong>: из 45+ предопределённых специальностей</li>
<li><strong>Тип поиска</strong>: ищет ли человек работу или ищет исполнителя</li>
</ol>
<p>Результат возвращается в структурированном JSON-формате с оценкой уверенности для каждой классификации.</p>
<h4>📈 Архитектура агента на LangGraph</h4>
<p>Следуя принципам LangGraph, создаём <strong>граф состояний</strong> из четырёх узлов:</p>
<ul>
<li>Входное описание</li>
<li>↓</li>
<li>Узел классификации типа работы</li>
<li>↓</li>
<li>Узел классификации категории</li>
<li>↓</li>
<li>Узел определения типа поиска</li>
<li>↓</li>
<li>Узел расчёта уверенности</li>
<li>↓</li>
<li>JSON-результат</li>
</ul>
<p>Каждый узел — это <strong>специализированная функция</strong>, которая:</p>
<ul>
<li>Получает текущее состояние агента</li>
<li>Выполняет свою часть анализа</li>
<li>Обновляет состояние и передаёт его дальше</li>
</ul>
<h4>Управление состоянием</h4>
<p>Определяем <strong>структуру памяти агента</strong> через <code>TypedDict</code>:</p>
<pre class="line-numbers"><code class="language-python">from typing import TypedDict, Dict

class State(TypedDict):
    """Состояние агента для хранения информации о процессе классификации"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
</code></pre>
<p>Это <strong>рабочая память агента</strong> — всё, что он помнит и накапливает в процессе анализа. Подобно тому, как человек-эксперт держит в уме контекст задачи при анализе документа.</p>
<p>Давайте рассмотрим полный код, а после сконцентрируемся на основных моментах.</p>
<pre class="line-numbers"><code class="language-python">import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Категории профессий
CATEGORIES = [
    "2D-аниматор", "3D-аниматор", "3D-моделлер",
    "Бизнес-аналитик", "Блокчейн-разработчик", ...
]

class JobType(Enum):
    PROJECT = "проектная работа"
    PERMANENT = "постоянная работа"

class SearchType(Enum):
    LOOKING_FOR_WORK = "поиск работы"
    LOOKING_FOR_PERFORMER = "поиск исполнителя"

class State(TypedDict):
    """Состояние агента для хранения информации о процессе классификации"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Асинхронный агент для классификации вакансий и услуг"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Инициализация агента"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Создает рабочий процесс агента на основе LangGraph"""
        workflow = StateGraph(State)
        
        # Добавляем узлы в граф
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # Определяем последовательность выполнения узлов
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """Узел для определения типа работы: проектная или постоянная"""
        # ... (implementation follows)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """Узел для определения категории профессии"""
        # ... (implementation follows)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """Узел для определения типа поиска"""
        # ... (implementation follows)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """Узел для расчета уровня уверенности в классификации"""
        # ... (implementation follows)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Находит наиболее похожую категорию из списка доступных"""
        # ... (implementation follows)

    async def classify(self, description: str) -> Dict[str, Any]:
        """Основной метод для классификации вакансии/услуги"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Запускаем рабочий процесс
        result = await self.workflow.ainvoke(initial_state)
        
        # Формируем итоговый ответ в формате JSON
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Демонстрация работы агента"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "Требуется Python разработчик для создания веб-приложения на Django. Постоянная работа.",
        "Ищу заказы на создание логотипов и фирменного стиля. Работаю в Adobe Illustrator.",
        "Нужен 3D-аниматор для краткосрочного проекта создания рекламного ролика.",
        "Резюме: опытный маркетолог, ищу удаленную работу в сфере digital-маркетинга",
        "Ищем фронтенд-разработчика React в нашу команду на постоянную основе"
    ]
    
    print("🤖 Демонстрация работы агента классификации вакансий\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Тест {i}: ---")
        print(f"Описание: {description}")
        try:
            result = await agent.classify(description)
            print("Результат классификации:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"❌ Ошибка: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

</code></pre>
<p>*(...остальная часть кода с реализацией методов была представлена в статье...)*</p>
<h3>Ключевые преимущества архитектуры</h3>
<ol>
<li><strong>Модульность</strong> — каждый узел решает одну задачу, легко тестировать и улучшать отдельно</li>
<li><strong>Расширяемость</strong> — можно добавлять новые узлы анализа без изменения существующих</li>
<li><strong>Прозрачность</strong> — весь процесс принятия решений документирован и отслеживаем</li>
<li><strong>Производительность</strong> — асинхронная обработка множественных запросов</li>
<li><strong>Надёжность</strong> — fallback-механизмы и обработка ошибок</li>
</ol>
<h3>Реальная польза</h3>
<p>Такой агент может использоваться в:</p>
<ul>
<li><strong>HR-платформах</strong> для автоматической категоризации резюме и вакансий</li>
<li><strong>Биржах фриланса</strong> для улучшения поиска и рекомендаций</li>
<li><strong>Внутренних системах</strong> компаний для обработки заявок и проектов</li>
<li><strong>Аналитических решениях</strong> для исследования рынка труда</li>
</ul>
<h3>МСР в действии: создаём агента с файловой системой и веб-поиском</h3>
<p>После того как мы разобрались с базовыми принципами LangGraph и создали простого агента-классификатора, давайте расширим его возможности, подключив к внешним миром через МСР.</p>
<p>Сейчас мы создадим полноценного ИИ-помощника, который сможет:</p>
<ul>
<li>Работать с файловой системой (читать, создавать, изменять файлы)</li>
<li>Искать актуальную информацию в интернете</li>
<li>Запоминать контекст диалога</li>
<li>Обрабатывать ошибки и восстанавливаться после сбоев</li>
</ul>
<h4>От теории к реальным инструментам</h4>
<p>Помните, как в начале статьи мы говорили о том, что <strong>МСР — это мост между нейросетью и её окружением</strong>? Сейчас вы увидите это на практике. Наш агент получит доступ к <strong>реальным инструментам</strong>:</p>
<pre class="line-numbers"><code class="language-text"># Инструменты файловой системы
- read_file — чтение файлов
- write_file — запись и создание файлов
- list_directory — просмотр содержимого папок
- create_directory — создание папок

# Инструменты веб-поиска
- brave_web_search — поиск в интернете
- get_web_content — получение содержимого страниц
</code></pre>
<p>Это уже не «игрушечный» агент — это <strong>рабочий инструмент</strong>, который может решать реальные задачи.</p>
<h4>📈 Архитектура: от простого к сложному</h4>
<p><strong>1. Конфигурация как основа стабильности</strong></p>
<pre class="line-numbers"><code class="language-python">from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Упрощенная конфигурация АІ-агента"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Валидация конфигурации"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Путь не существует: {self.filesystem_path}")
</code></pre>
<p><strong>Почему это важно?</strong> В отличие от примера с классификацией, здесь агент взаимодействует с внешними системами. Одна ошибка в пути к файлам или отсутствующий АРІ-ключ — и весь агент перестаёт работать. <strong>Валидация на старте</strong> экономит часы отладки.</p>
<p><strong>2. Фабрика моделей: гибкость выбора</strong></p>
<pre class="line-numbers"><code class="language-python">def create_model(config: AgentConfig):
    """Создает модель согласно конфигурации"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... другие провайдеры
</code></pre>
<p>Один код — множество моделей. Хотите бесплатную локальную модель? Используйте Ollama. Нужна максимальная точность? Переключитесь на GPT-4. Важна скорость? Попробуйте DeepSeek. Код остаётся тем же.</p>
<p><strong>3. МСР-интеграция: подключение к реальному миру</strong></p>
<pre class="line-numbers"><code class="language-python">async def _init_mcp_client(self):
    """Инициализация МСР клиента"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
</code></pre>
<p>Здесь происходит ключевая работа МСР: мы подключаем к агенту внешние МСР-серверы, которые предоставляют набор инструментов и функций. Агент при этом получает не просто отдельные функции, а полноценное контекстное понимание того, как работать с файловой системой и интернетом.</p>
<h4>Устойчивость к ошибкам</h4>
<p>В реальном мире всё ломается: сеть недоступна, файлы заблокированы, АРІ-ключи просрочены. Наш агент готов к этому:</p>
<pre class="line-numbers"><code class="language-python">@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
</code></pre>
<p>Декоратор <code>@retry_on_failure</code> автоматически повторяет операции при временных сбоях. Пользователь даже не заметит, что что-то пошло не так.</p>
<h3>Итоги: от теории к практике ИИ-агентов</h3>
<p>Сегодня мы прошли путь от базовых концепций до создания работающих ИИ-агентов. Давайте подведём итоги того, что мы изучили и чего достигли.</p>
<p><strong>Что мы освоили</strong></p>
<p><strong>1. Фундаментальные концепции</strong></p>
<ul>
<li>Разобрались с различием между чат-ботами и настоящими ИИ-агентами</li>
<li>Поняли роль <strong>МСР (Model Context Protocol)</strong> как моста между моделью и внешним миром</li>
<li>Изучили архитектуру <strong>LangGraph</strong> для построения сложной логики агентов</li>
</ul>
<p><strong>2. Практические навыки</strong></p>
<ul>
<li>Настроили рабочее окружение с поддержкой облачных и локальных моделей</li>
<li>Создали <strong>агента-классификатора</strong> с асинхронной архитектурой и управлением состояниями</li>
<li>Построили <strong>МСР-агента</strong> с доступом к файловой системе и веб-поиску</li>
</ul>
<p><strong>3. Архитектурные паттерны</strong></p>
<ul>
<li>Освоили модульную конфигурацию и фабрики моделей</li>
<li>Внедрили обработку ошибок и <strong>retry-механизмы</strong> для продакшн-готовых решений</li>
</ul>
<h3>Ключевые преимущества подхода</h3>
<p><strong>LangGraph + МСР</strong> дают нам:</p>
<ul>
<li><strong>Прозрачность</strong> — каждый шаг агента документирован и отслеживаем</li>
<li><strong>Расширяемость</strong> — новые возможности добавляются декларативно</li>
<li><strong>Надёжность</strong> — встроенная обработка ошибок и восстановление</li>
<li><strong>Гибкость</strong> — поддержка множества моделей и провайдеров из коробки</li>
</ul>
<h3>Заключение</h3>
<p>ИИ-агенты — это не футуристическая фантастика, а <strong>реальная технология сегодняшнего дня</strong>. С помощью LangGraph и MCP мы можем создавать системы, которые решают конкретные бизнес-задачи, автоматизируют рутину и открывают новые возможности.</p>
<p><strong>Главное — начать.</strong> Возьмите код из примеров, адаптируйте под свои задачи, экспериментируйте. Каждый проект — это новый опыт и шаг к мастерству в области ИИ-разработки.</p>
<p>Удачи в ваших проектах!</p>
<hr>
<p><em>Теги: python, ии, mcp, langchain, ии-ассистент, ollama, ии-агенты, local llm, langgraph, mcp-server</em><br>
<em>Хабы: Блог компании Amvera, Natural Language Processing, Искусственный интеллект, Python, Программирование</em><br>
<img src="https://habr.com/ru/companies/amvera/articles/929568/" alt="habr"></em></p>