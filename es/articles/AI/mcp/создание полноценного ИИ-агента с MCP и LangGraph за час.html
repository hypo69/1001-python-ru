<h1>C√≥mo ense√±ar a una red neuronal a trabajar con sus manos: creaci√≥n de un agente de IA completo con MCP y LangGraph en una hora</h1>
<p>¬°Saludos, amigos! Espero que me hayan echado de menos.</p>
<p>Durante los √∫ltimos dos meses, me he sumergido profundamente en la investigaci√≥n de la integraci√≥n de agentes de IA en mis propios proyectos de Python. En el proceso, he acumulado una gran cantidad de conocimientos pr√°cticos y observaciones que ser√≠a un pecado no compartir. As√≠ que hoy, regreso a Habr, con un nuevo tema, una perspectiva fresca y la intenci√≥n de escribir m√°s a menudo.</p>
<p>En la agenda de hoy est√°n LangGraph y MCP: herramientas con las que se pueden crear agentes de IA realmente √∫tiles.</p>
<p>Si antes discut√≠amos sobre qu√© red neuronal respond√≠a mejor en ruso, hoy el campo de batalla se ha desplazado hacia tareas m√°s aplicadas: ¬øqui√©n maneja mejor el papel de un agente de IA? ¬øQu√© frameworks realmente simplifican el desarrollo? ¬øY c√≥mo integrar todo esto en un proyecto real?</p>
<p>Pero antes de sumergirnos en la pr√°ctica y el c√≥digo, comprendamos los conceptos b√°sicos. Especialmente los dos clave: <strong>agentes de IA y MCP</strong>. Sin ellos, la conversaci√≥n sobre LangGraph estar√° incompleta.</p>
<h3>Agentes de IA en t√©rminos sencillos</h3>
<p>Los agentes de IA no son solo chatbots "mejorados". Son entidades m√°s complejas y aut√≥nomas que poseen dos caracter√≠sticas cruciales:</p>
<ol>
<li><strong>Capacidad de interactuar y coordinarse</strong></li>
</ol>
<p>Los agentes modernos son capaces de dividir tareas en subtareas, llamar a otros agentes, solicitar datos externos y trabajar en equipo. Ya no es un asistente solitario, sino un sistema distribuido donde cada componente puede contribuir.</p>
<ol start="2">
<li><strong>Acceso a recursos externos</strong></li>
</ol>
<p>Un agente de IA ya no est√° limitado por los l√≠mites de un di√°logo. Puede acceder a bases de datos, realizar llamadas a la API, interactuar con archivos locales, bases de conocimiento vectoriales e incluso ejecutar comandos en la terminal. Todo esto fue posible gracias a la aparici√≥n de MCP, un nuevo nivel de integraci√≥n entre el modelo y el entorno.</p>
<hr>
<p>En pocas palabras: <strong>MCP es un puente entre una red neuronal y su entorno</strong>. Permite que el modelo "comprenda" el contexto de una tarea, acceda a datos, realice llamadas y forme acciones razonadas, en lugar de simplemente generar respuestas de texto.</p>
<p><strong>Imaginemos una analog√≠a:</strong></p>
<ul>
<li>Tienes una <strong>red neuronal</strong> ‚Äî puede razonar y generar textos.</li>
<li>Hay <strong>datos y herramientas</strong> ‚Äî documentos, APIs, bases de conocimiento, terminal, c√≥digo.</li>
<li>Y hay <strong>MCP</strong> ‚Äî es una interfaz que permite que el modelo interact√∫e con estas fuentes externas como si fueran parte de su mundo interno.</li>
</ul>
<p><strong>Sin MCP:</strong></p>
<p>El modelo ‚Äî es un motor de di√°logo aislado. Le das texto ‚Äî responde. Y eso es todo.</p>
<p><strong>Con MCP:</strong></p>
<p>El modelo se convierte en un <strong>ejecutor de tareas</strong> completo:</p>
<ul>
<li>obtiene acceso a estructuras de datos y APIs;</li>
<li>llama a funciones externas;</li>
<li>navega por el estado actual del proyecto o aplicaci√≥n;</li>
<li>puede recordar, rastrear y cambiar el contexto durante un di√°logo;</li>
<li>utiliza extensiones como herramientas de b√∫squeda, ejecutores de c√≥digo, bases de datos de incrustaciones vectoriales, etc.</li>
</ul>
<p>En un sentido t√©cnico, <strong>MCP es un protocolo de interacci√≥n entre un LLM y su entorno</strong>, donde el contexto se proporciona como objetos estructurados (en lugar de texto "en bruto"), y las llamadas se formatean como operaciones interactivas (por ejemplo, llamadas a funciones, uso de herramientas o acciones de agente). Esto es lo que transforma un modelo ordinario en un <strong>verdadero agente de IA</strong>, capaz de hacer m√°s que simplemente "hablar".</p>
<h3>¬°Y ahora, al grano!</h3>
<p>Ahora que hemos cubierto los conceptos b√°sicos, es l√≥gico preguntar: "¬øC√≥mo implementamos todo esto en la pr√°ctica en Python?"</p>
<p>Aqu√≠ es donde entra en juego <strong>LangGraph</strong> ‚Äî un potente framework para construir grafos de estado, comportamiento de agentes y cadenas de pensamiento. Permite "unir" la l√≥gica de interacci√≥n entre agentes, herramientas y el usuario, creando una arquitectura de IA viva que se adapta a las tareas.</p>
<p>En las siguientes secciones, veremos c√≥mo:</p>
<ul>
<li>se construye un agente desde cero;</li>
<li>se crean estados, transiciones y eventos;</li>
<li>se integran funciones y herramientas;</li>
<li>y c√≥mo funciona todo este ecosistema en un proyecto real.</li>
</ul>
<h3>Un poco de teor√≠a: ¬øqu√© es LangGraph?</h3>
<p>Antes de pasar a la pr√°ctica, unas palabras sobre el framework en s√≠.</p>
<p><strong>LangGraph</strong> ‚Äî es un proyecto del equipo de <strong>LangChain</strong>, los mismos que propusieron por primera vez el concepto de ¬´cadenas¬ª (chains) de interacci√≥n con LLMs. Si antes el enfoque principal estaba en pipelines lineales o condicionalmente ramificados (langchain.chains), ahora los desarrolladores apuestan por un <strong>modelo de grafo</strong>, y LangGraph es lo que recomiendan como el nuevo ¬´n√∫cleo¬ª para construir sistemas de IA complejos.</p>
<p><strong>LangGraph</strong> ‚Äî es un framework para construir m√°quinas de estados finitos y grafos de estados, donde cada <strong>nodo</strong> representa una parte de la l√≥gica del agente: una llamada a un modelo, una herramienta externa, una condici√≥n, una entrada de usuario, etc.</p>
<h3>C√≥mo funciona: grafos y nodos</h3>
<p>Conceptualmente, LangGraph se basa en las siguientes ideas:</p>
<ul>
<li><strong>Grafo</strong> ‚Äî es una estructura que describe posibles rutas para ejecutar la l√≥gica. Puedes pensarlo como un mapa: de un punto puedes moverte a otro dependiendo de las condiciones o el resultado de la ejecuci√≥n.</li>
<li><strong>Nodos</strong> ‚Äî son pasos espec√≠ficos dentro del grafo. Cada nodo realiza alguna funci√≥n: llama a un modelo, llama a una API externa, verifica una condici√≥n o simplemente actualiza el estado interno.</li>
<li><strong>Transiciones entre nodos</strong> ‚Äî es la l√≥gica de enrutamiento: si el resultado del paso anterior es tal, entonces ve all√≠.</li>
<li><strong>Estado</strong> ‚Äî se pasa entre nodos y acumula todo lo necesario: historial, conclusiones intermedias, entrada de usuario, el resultado de las operaciones de la herramienta, etc.</li>
</ul>
<p>As√≠, obtenemos un <strong>mecanismo flexible para gestionar la l√≥gica del agente</strong>, en el que se pueden describir escenarios tanto simples como muy complejos: bucles, condiciones, acciones paralelas, llamadas anidadas y mucho m√°s.</p>
<h3>¬øPor qu√© es conveniente?</h3>
<p>LangGraph te permite construir una <strong>l√≥gica transparente, reproducible y extensible</strong>:</p>
<ul>
<li>f√°cil de depurar;</li>
<li>f√°cil de visualizar;</li>
<li>f√°cil de escalar para nuevas tareas;</li>
<li>f√°cil de integrar herramientas externas y protocolos MCP.</li>
</ul>
<p>En esencia, LangGraph es el <strong>"cerebro" del agente</strong>, donde cada paso est√° documentado, controlable y puede modificarse sin caos ni "magia".</p>
<h3>¬°Bueno, ya basta de teor√≠a!</h3>
<p>Podr√≠amos hablar mucho tiempo sobre grafos, estados, composici√≥n l√≥gica y las ventajas de LangGraph sobre los pipelines cl√°sicos. Pero, como muestra la pr√°ctica, es mejor verlo una vez en el c√≥digo.</p>
<p><strong>Es hora de pasar a la pr√°ctica.</strong> A continuaci√≥n, un ejemplo en Python: crearemos un agente de IA simple pero √∫til basado en LangGraph que utilizar√° herramientas externas, memoria y tomar√° decisiones por s√≠ mismo.</p>
<h3>Preparaci√≥n: redes neuronales en la nube y locales</h3>
<p>Para empezar a crear agentes de IA, primero necesitamos un <strong>cerebro</strong>, un modelo de lenguaje. Aqu√≠ hay dos enfoques:</p>
<ul>
<li><strong>utilizar soluciones en la nube</strong>, donde todo est√° listo "de f√°brica";</li>
<li>o <strong>levantar el modelo localmente</strong> ‚Äî para una autonom√≠a y confidencialidad completas.</li>
</ul>
<p>Consideremos ambas opciones.</p>
<h4>Servicios en la nube: r√°pidos y convenientes</h4>
<p>La forma m√°s sencilla es utilizar el poder de los grandes proveedores: OpenAI, Anthropic, y utilizar...</p>
<h3>D√≥nde obtener claves y tokens:</h3>
<ul>
<li><strong>OpenAI</strong> ‚Äî ChatGPT y otros productos;</li>
<li><strong>Anthropic</strong> ‚Äî Claude;</li>
<li><strong>OpenRouter.ai</strong> ‚Äî docenas de modelos (un token ‚Äî muchos modelos a trav√©s de una API compatible con OpenAI);</li>
<li><strong>Amvera Cloud</strong> ‚Äî capacidad de conectar LLAMA con pago en rublos y proxy integrado a OpenAI y Anthropic.</li>
</ul>
<p>Este camino es conveniente, especialmente si:</p>
<ul>
<li>no quieres configurar la infraestructura;</li>
<li>desarrollas con un enfoque en la velocidad;</li>
<li>trabajas con recursos limitados.</li>
</ul>
<h4>Modelos locales: control total</h4>
<p>Si la <strong>privacidad, el trabajo sin conexi√≥n</strong> son importantes para ti, o quieres construir <strong>agentes completamente aut√≥nomos</strong>, entonces tiene sentido desplegar la red neuronal localmente.</p>
<p><strong>Principales ventajas:</strong></p>
<ul>
<li><strong>Confidencialidad</strong> ‚Äî los datos permanecen contigo;</li>
<li><strong>Trabajo sin conexi√≥n</strong> ‚Äî √∫til en redes aisladas;</li>
<li><strong>Sin suscripciones ni tokens</strong> ‚Äî gratis despu√©s de la configuraci√≥n.</li>
</ul>
<p><strong>Las desventajas son obvias:</strong></p>
<ul>
<li>Requisitos de recursos (especialmente para la memoria de video);</li>
<li>La configuraci√≥n puede llevar tiempo;</li>
<li>Algunos modelos son dif√≠ciles de desplegar sin experiencia.</li>
</ul>
<p>Sin embargo, existen herramientas que facilitan el lanzamiento local. Una de las mejores hoy en d√≠a es <strong>Ollama</strong>.</p>
<h3>Despliegue de LLM local a trav√©s de Ollama + Docker</h3>
<p>Prepararemos un lanzamiento local del modelo Qwen 2.5 (qwen2.5:32b) utilizando un contenedor Docker y el sistema Ollama. Esto permitir√° integrar la red neuronal con MCP y usarla en tus propios agentes basados en LangGraph.</p>
<p>Si los recursos inform√°ticos de tu computadora o servidor son insuficientes para trabajar con esta versi√≥n del modelo, siempre puedes elegir una red neuronal menos "hambrienta de recursos"; el proceso de instalaci√≥n y lanzamiento seguir√° siendo similar.</p>
<p><strong>Instalaci√≥n r√°pida (resumen de pasos)</strong></p>
<ol>
<li><strong>Instala Docker + Docker Compose</strong></li>
<li><strong>Crea la estructura del proyecto:</strong>
<pre class="line-numbers"><code class="language-bash">mkdir qwen-local && cd qwen-local
</code></pre>
</li>
<li><strong>Crea <code>docker-compose.yml</code></strong>
(opci√≥n universal, la GPU se detecta autom√°ticamente)
<pre class="line-numbers"><code class="language-yaml">services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
</code></pre>
</li>
<li><strong>Inicia el contenedor:</strong>
<pre class="line-numbers"><code class="language-bash">docker compose up -d
</code></pre>
</li>
<li><strong>Descarga el modelo:</strong>
<pre class="line-numbers"><code class="language-bash">docker exec -it ollama_qwen ollama pull qwen2.5:32b
</code></pre>
</li>
<li><strong>Verifica el funcionamiento a trav√©s de la API:</strong>
<pre class="line-numbers"><code class="language-bash">curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "¬°Hola!", "stream": false}'
</code></pre>
<p>*(Imagen con el resultado de la ejecuci√≥n del comando curl)*</p>
</li>
<li><strong>Integraci√≥n con Python:</strong>
<pre class="line-numbers"><code class="language-python">import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Explica el entrelazamiento cu√°ntico"))
</code></pre>
<p>Ahora tienes un LLM local completo, listo para trabajar con MCP y LangGraph.</p>
</li>
</ol>
<p><strong>¬øQu√© sigue?</strong></p>
<p>Tenemos la opci√≥n de elegir entre modelos en la nube y locales, y hemos aprendido a conectar ambos. La parte m√°s interesante est√° por venir: <strong>crear agentes de IA en LangGraph</strong>, que utilicen el modelo seleccionado, la memoria, las herramientas y su propia l√≥gica.</p>
<p><strong>¬°Pasemos a la parte m√°s emocionante: el c√≥digo y la pr√°ctica!</strong></p>
<hr>
<p>Antes de pasar a la pr√°ctica, es importante preparar el entorno de trabajo. Asumo que ya est√°s familiarizado con los conceptos b√°sicos de Python, sabes qu√© son las bibliotecas y las dependencias, y entiendes por qu√© usar un entorno virtual.</p>
<p>Si todo esto es nuevo para ti, te recomiendo que primero tomes un curso corto o una gu√≠a sobre los conceptos b√°sicos de Python, y luego regreses al art√≠culo.</p>
<h4>Paso 1: Creaci√≥n de un entorno virtual</h4>
<p>Crea un nuevo entorno virtual en la carpeta del proyecto:</p>
<pre class="line-numbers"><code class="language-bash">python -m venv venv
source venv/bin/activate  # para Linux/macOS
venc\Scripts\activate   # para Windows
</code></pre>
<h4>Paso 2: Instalaci√≥n de dependencias</h4>
<p>Crea un archivo <code>requirements.txt</code> y a√±ade las siguientes l√≠neas:</p>
<pre class="line-numbers"><code class="language-text">langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
</code></pre>
<blockquote>
<p>‚ö†Ô∏è <strong>Las versiones actuales se indican a partir del 21 de julio de 2025.</strong> Desde la publicaci√≥n, pueden haber cambiado; <strong>verifica la relevancia antes de la instalaci√≥n.</strong></p>
</blockquote>
<p>Luego instala las dependencias:</p>
<pre class="line-numbers"><code class="language-bash">pip install -r requirements.txt</code></pre>
<h4>Paso 3: Configuraci√≥n de variables de entorno</h4>
<p>Crea un archivo <code>.env</code> en la ra√≠z del proyecto y a√±ade las claves API necesarias:</p>
<pre class="line-numbers"><code class="language-text">OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
</code></pre>
<p><strong>Prop√≥sito de las variables:</strong></p>
<ul>
<li><strong>OPENAI_API_KEY</strong> ‚Äî clave para acceder a los modelos GPT de OpenAI;</li>
<li><strong>DEEPSEEK_API_KEY</strong> ‚Äî clave para usar los modelos Deepseek;</li>
<li><strong>OPENROUTER_API_KEY</strong> ‚Äî clave √∫nica para acceder a m√∫ltiples modelos a trav√©s de OpenRouter</li>
</ul>
<hr>
<p>Algunas herramientas MCP (por ejemplo, <code>brave-web-search</code>) requieren una clave para funcionar. Sin ella, simplemente no se activar√°n.</p>
<p><strong>¬øQu√© pasa si no tienes claves API?</strong></p>
<p>No hay problema. Puedes comenzar el desarrollo con un modelo local (por ejemplo, a trav√©s de Ollama) sin conectar ning√∫n servicio externo. En este caso, el archivo <code>.env</code> se puede omitir por completo.</p>
<p>¬°Listo! Ahora tenemos todo lo que necesitamos para empezar: un entorno aislado, dependencias y, si es necesario, acceso a redes neuronales en la nube e integraciones MCP.</p>
<p>A continuaci√≥n, lanzaremos nuestro agente LLM de diferentes maneras.</p>
<h3>Lanzamiento simple de agentes LLM a trav√©s de LangGraph: integraci√≥n b√°sica</h3>
<p>Comencemos con lo m√°s simple: c√≥mo ¬´conectar el cerebro¬ª al futuro agente. Analizaremos las formas b√°sicas de lanzar modelos de lenguaje (LLM) usando LangChain, para que en el siguiente paso podamos pasar a la integraci√≥n con LangGraph y la construcci√≥n de un agente de IA completo.</p>
<h4>Importaciones</h4>
<pre class="line-numbers"><code class="language-python">import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
</code></pre>
<ul>
<li><code>os</code> y <code>load_dotenv()</code> ‚Äî para cargar variables del archivo <code>.env</code>.</li>
<li><code>ChatOpenAI</code>, <code>ChatOllama</code>, <code>ChatDeepSeek</code> ‚Äî envoltorios para conectar modelos de lenguaje a trav√©s de LangChain.</li>
</ul>
<blockquote>
<p>üí° Si utilizas enfoques alternativos para trabajar con configuraciones (por ejemplo, Pydantic Settings), puedes reemplazar <code>load_dotenv()</code> con tu m√©todo habitual.</p>
</blockquote>
<h4>Carga de variables de entorno</h4>
<pre class="line-numbers"><code class="language-python">load_dotenv()
</code></pre>
<p>Esto cargar√° todas las variables de <code>.env</code>, incluidas las claves para acceder a las API de OpenAI, DeepSeek, OpenRouter y otras.</p>
<h4>Funciones simples para obtener LLM</h4>
<p><strong>OpenAI</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
</code></pre>
<p>Si la variable <code>OPENAI_API_KEY</code> est√° configurada correctamente, LangChain la sustituir√° autom√°ticamente; la especificaci√≥n expl√≠cita de <code>api_key=...</code> es opcional aqu√≠.</p>
<p><strong>DeepSeek</strong></p>
<pre class="line-numbers"><code class="language-python">def get_deepseek_llm():
    # ...
</code></pre>
<p>De manera similar, pero usamos el envoltorio <code>ChatDeepSeek</code>.</p>
<p><strong>OpenRouter (y otras API compatibles)</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
</code></pre>
<p><strong>Caracter√≠sticas:</strong></p>
<ul>
<li><code>ChatOpenAI</code> se utiliza, aunque el modelo no sea de OpenAI ‚Äî porque OpenRouter utiliza el mismo protocolo.</li>
<li><code>base_url</code> es obligatorio: apunta a la API de OpenRouter.</li>
<li>El modelo <code>moonshotai/kimi-k2:free</code> fue elegido como una de las opciones m√°s equilibradas en t√©rminos de calidad y velocidad en el momento de escribir este art√≠culo.</li>
<li>La clave API de <code>OpenRouter</code> debe pasarse expl√≠citamente; la sustituci√≥n autom√°tica no funciona aqu√≠.</li>
</ul>
<h4>Mini-prueba: comprobaci√≥n del funcionamiento del modelo</h4>
<pre class="line-numbers"><code class="language-python">if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("¬øQui√©n eres?")
    print(response.content)
</code></pre>
<p>*(Imagen con el resultado de la ejecuci√≥n del comando curl: <code>Soy un asistente de IA creado por Moonshot AI...</code>)*</p>
<p>Si todo est√° configurado correctamente, recibir√°s una respuesta significativa del modelo. ¬°Felicidades, el primer paso est√° hecho!</p>
<h3>Pero esto a√∫n no es un agente</h3>
<p>En esta etapa, hemos conectado el LLM y hemos realizado una llamada simple. Esto se parece m√°s a un chatbot de consola que a un agente de IA.</p>
<p><strong>¬øPor qu√©?</strong></p>
<ul>
<li>Escribimos <strong>c√≥digo s√≠ncrono y lineal</strong> sin l√≥gica de estado ni objetivos.</li>
<li>El agente no toma decisiones, no recuerda el contexto y no utiliza herramientas.</li>
<li>MCP y LangGraph a√∫n no est√°n involucrados.</li>
</ul>
<p><strong>¬øQu√© sigue?</strong></p>
<p>A continuaci√≥n, implementaremos un <strong>agente de IA completo</strong> utilizando <strong>LangGraph</strong> ‚Äî primero sin MCP, para centrarnos en la arquitectura, los estados y la l√≥gica del propio agente.</p>
<p>Sumerj√°monos en la verdadera mec√°nica de los agentes. ¬°Vamos!</p>
<h3>Agente de clasificaci√≥n de vacantes: de la teor√≠a a la pr√°ctica</h3>
<p>...conceptos de LangGraph en la pr√°ctica y crear una herramienta √∫til para plataformas de RRHH y bolsas de trabajo freelance.</p>
<h4>Tarea del agente</h4>
<p>Nuestro agente toma como entrada una descripci√≥n de texto de una vacante o servicio y realiza una clasificaci√≥n de tres niveles:</p>
<ol>
<li><strong>Tipo de trabajo</strong>: trabajo por proyecto o vacante permanente</li>
<li><strong>Categor√≠a profesional</strong>: de m√°s de 45 especialidades predefinidas</li>
<li><strong>Tipo de b√∫squeda</strong>: si la persona busca trabajo o busca un ejecutante</li>
</ol>
<p>El resultado se devuelve en un formato JSON estructurado con una puntuaci√≥n de confianza para cada clasificaci√≥n.</p>
<h4>üìà Arquitectura del agente en LangGraph</h4>
<p>Siguiendo los principios de LangGraph, creamos un <strong>grafo de estados</strong> de cuatro nodos:</p>
<ul>
<li>Descripci√≥n de entrada</li>
<li>‚Üì</li>
<li>Nodo de clasificaci√≥n del tipo de trabajo</li>
<li>‚Üì</li>
<li>Nodo de clasificaci√≥n de categor√≠a</li>
<li>‚Üì</li>
<li>Nodo de determinaci√≥n del tipo de b√∫squeda</li>
<li>‚Üì</li>
<li>Nodo de c√°lculo de confianza</li>
<li>‚Üì</li>
<li>Resultado JSON</li>
</ul>
<p>Cada nodo es una <strong>funci√≥n especializada</strong> que:</p>
<ul>
<li>Recibe el estado actual del agente</li>
<li>Realiza su parte del an√°lisis</li>
<li>Actualiza el estado y lo pasa</li>
</ul>
<h4>Gesti√≥n de estado</h4>
<p>Definimos la <strong>estructura de memoria del agente</strong> a trav√©s de <code>TypedDict</code>:</p>
<pre class="line-numbers"><code class="language-python">from typing import TypedDict, Dict

class State(TypedDict):
    """Estado del agente para almacenar informaci√≥n sobre el proceso de clasificaci√≥n"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
</code></pre>
<p>Esta es la <strong>memoria de trabajo del agente</strong> ‚Äî todo lo que recuerda y acumula durante el proceso de an√°lisis. Similar a c√≥mo un experto humano mantiene el contexto de la tarea en mente al analizar un documento.</p>
<p>Veamos el c√≥digo completo y luego nos centraremos en los puntos principales.</p>
<pre class="line-numbers"><code class="language-python">import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Categor√≠as profesionales
CATEGORIES = [
    "2D-animador", "3D-animador", "3D-modelador",
    "Analista de negocios", "Desarrollador Blockchain", ...
]

class JobType(Enum):
    PROJECT = "trabajo por proyecto"
    PERMANENT = "trabajo permanente"

class SearchType(Enum):
    LOOKING_FOR_WORK = "buscando trabajo"
    LOOKING_FOR_PERFORMER = "buscando ejecutante"

class State(TypedDict):
    """Estado del agente para almacenar informaci√≥n sobre el proceso de clasificaci√≥n"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Agente as√≠ncrono para clasificar vacantes y servicios"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Inicializaci√≥n del agente"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Crea un flujo de trabajo de agente basado en LangGraph"""
        workflow = StateGraph(State)

        # A√±adir nodos al grafo
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)

        # Definir la secuencia de ejecuci√≥n de nodos
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)

        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """Nodo para determinar el tipo de trabajo: por proyecto o permanente"""
        # ... (la implementaci√≥n sigue)

    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """Nodo para determinar la categor√≠a profesional"""
        # ... (la implementaci√≥n sigue)

    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """Nodo para determinar el tipo de b√∫squeda"""
        # ... (la implementaci√≥n sigue)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """Nodo para calcular el nivel de confianza en la clasificaci√≥n"""
        # ... (la implementaci√≥n sigue)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Encuentra la categor√≠a m√°s cercana de la lista disponible"""
        # ... (la implementaci√≥n sigue)

    async def classify(self, description: str) -> Dict[str, Any]:
        """M√©todo principal para clasificar vacantes/servicios"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }

        # Ejecutar el flujo de trabajo
        result = await self.workflow.ainvoke(initial_state)

        # Formar la respuesta final en formato JSON
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Demostraci√≥n del funcionamiento del agente"""
    agent = VacancyClassificationAgent()

    test_cases = [
        "Se busca desarrollador Python para crear una aplicaci√≥n web en Django. Trabajo permanente.",
        "Busco pedidos para crear logotipos e identidad corporativa. Trabajo en Adobe Illustrator.",
        "Se necesita animador 3D para un proyecto a corto plazo de creaci√≥n de un anuncio.",
        "Curr√≠culum: comercializador experimentado, busco trabajo remoto en marketing digital",
        "Buscamos un desarrollador frontend React para nuestro equipo de forma permanente"
    ]

    print("ü§ñ Demostraci√≥n del funcionamiento del agente de clasificaci√≥n de vacantes\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Prueba {i}: ---")
        print(f"Descripci√≥n: {description}")
        try:
            result = await agent.classify(description)
            print("Resultado de la clasificaci√≥n:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå Error: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())
</code></pre>
<p>*(...el resto del c√≥digo con la implementaci√≥n de los m√©todos se present√≥ en el art√≠culo...)*</p>
<h3>Ventajas clave de la arquitectura</h3>
<ol>
<li><strong>Modularidad</strong> ‚Äî cada nodo resuelve una tarea, f√°cil de probar y mejorar por separado</li>
<li><strong>Extensibilidad</strong> ‚Äî se a√±aden nuevos nodos de an√°lisis sin modificar los existentes</li>
<li><strong>Transparencia</strong> ‚Äî todo el proceso de toma de decisiones est√° documentado y es rastreable</li>
<li><strong>Rendimiento</strong> ‚Äî procesamiento as√≠ncrono de m√∫ltiples solicitudes</li>
<li><strong>Fiabilidad</strong> ‚Äî mecanismos de reserva incorporados y manejo de errores</li>
</ol>
<h3>Beneficios reales</h3>
<p>Un agente as√≠ puede utilizarse en:</p>
<ul>
<li><strong>Plataformas de RRHH</strong> para la categorizaci√≥n autom√°tica de curr√≠culums y vacantes</li>
<li><strong>Bolsas de trabajo freelance</strong> para mejorar la b√∫squeda y las recomendaciones</li>
<li><strong>Sistemas internos</strong> de empresas para el procesamiento de solicitudes y proyectos</li>
<li><strong>Soluciones anal√≠ticas</strong> para la investigaci√≥n del mercado laboral</li>
</ul>
<h3>MCP en acci√≥n: creaci√≥n de un agente con sistema de archivos y b√∫squeda web</h3>
<p>Despu√©s de haber abordado los principios b√°sicos de LangGraph y haber creado un agente clasificador simple, ampliemos sus capacidades conect√°ndolo al mundo exterior a trav√©s de MCP.</p>
<p>Ahora crearemos un asistente de IA completo que podr√°:</p>
<ul>
<li>Trabajar con el sistema de archivos (leer, crear, modificar archivos)</li>
<li>Buscar informaci√≥n relevante en Internet</li>
<li>Recordar el contexto del di√°logo</li>
<li>Manejar errores y recuperarse de fallos</li>
</ul>
<h4>De la teor√≠a a las herramientas reales</h4>
<p>¬øRecuerdas c√≥mo al principio del art√≠culo hablamos de que <strong>MCP es un puente entre una red neuronal y su entorno</strong>? Ahora lo ver√°s en la pr√°ctica. Nuestro agente tendr√° acceso a <strong>herramientas reales</strong>:</p>
<pre class="line-numbers"><code class="language-text"># Herramientas del sistema de archivos
- read_file ‚Äî lectura de archivos
- write_file ‚Äî escritura y creaci√≥n de archivos
- list_directory ‚Äî visualizaci√≥n del contenido de carpetas
- create_directory ‚Äî creaci√≥n de carpetas

# Herramientas de b√∫squeda web
- brave_web_search ‚Äî b√∫squeda en Internet
- get_web_content ‚Äî obtenci√≥n del contenido de p√°ginas
</code></pre>
<p>Este ya no es un agente "de juguete", es una <strong>herramienta de trabajo</strong> que puede resolver problemas reales.</p>
<h4>üìà Arquitectura: de lo simple a lo complejo</h4>
<p><strong>1. Configuraci√≥n como base de la estabilidad</strong></p>
<pre class="line-numbers"><code class="language-python">from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Configuraci√≥n simplificada del agente de IA"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Validaci√≥n de la configuraci√≥n"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"La ruta no existe: {self.filesystem_path}")
</code></pre>
<p><strong>¬øPor qu√© es importante?</strong> A diferencia del ejemplo de clasificaci√≥n, aqu√≠ el agente interact√∫a con sistemas externos. Un error en la ruta del archivo o una clave API faltante, y todo el agente deja de funcionar. La <strong>validaci√≥n al inicio</strong> ahorra horas de depuraci√≥n.</p>
<p><strong>2. F√°brica de modelos: flexibilidad de elecci√≥n</strong></p>
<pre class="line-numbers"><code class="language-python">def create_model(config: AgentConfig):
    """Crea un modelo seg√∫n la configuraci√≥n"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... otros proveedores
</code></pre>
<p>Un c√≥digo, muchos modelos. ¬øQuieres un modelo local gratuito? Usa Ollama. ¬øNecesitas m√°xima precisi√≥n? Cambia a GPT-4. ¬øNecesitas velocidad? Prueba DeepSeek. El c√≥digo sigue siendo el mismo.</p>
<p><strong>3. Integraci√≥n MCP: conexi√≥n con el mundo real</strong></p>
<pre class="line-numbers"><code class="language-python">async def _init_mcp_client(self):
    """Inicializaci√≥n del cliente MCP"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
</code></pre>
<p>Aqu√≠ tiene lugar el trabajo clave de MCP: conectamos servidores MCP externos al agente, que proporcionan un conjunto de herramientas y funciones. El agente, a su vez, recibe no solo funciones individuales, sino una comprensi√≥n contextual completa de c√≥mo trabajar con el sistema de archivos e Internet.</p>
<h4>Resistencia a errores</h4>
<p>En el mundo real, todo falla: la red no est√° disponible, los archivos est√°n bloqueados, las claves API caducan. Nuestro agente est√° preparado para esto:</p>
<pre class="line-numbers"><code class="language-python">@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
</code></pre>
<p>El decorador <code>@retry_on_failure</code> reintenta autom√°ticamente las operaciones en caso de fallos temporales. El usuario ni siquiera notar√° que algo sali√≥ mal.</p>
<h3>Conclusiones: de la teor√≠a a la pr√°ctica de los agentes de IA</h3>
<p>Hoy hemos recorrido un largo camino desde los conceptos b√°sicos hasta la creaci√≥n de agentes de IA funcionales. Resumamos lo que hemos aprendido y logrado.</p>
<p><strong>Lo que hemos dominado</strong></p>
<p><strong>1. Conceptos fundamentales</strong></p>
<ul>
<li>Comprendimos la diferencia entre chatbots y agentes de IA reales</li>
<li>Comprendimos el papel de <strong>MCP (Model Context Protocol)</strong> como puente entre el modelo y el mundo exterior</li>
<li>Estudiamos la arquitectura de <strong>LangGraph</strong> para construir l√≥gica de agente compleja</li>
</ul>
<p><strong>2. Habilidades pr√°cticas</strong></p>
<ul>
<li>Configuramos un entorno de trabajo con soporte para modelos en la nube y locales</li>
<li>Creamos un <strong>agente clasificador</strong> con una arquitectura as√≠ncrona y gesti√≥n de estados</li>
<li>Construimos un <strong>agente MCP</strong> con acceso al sistema de archivos y b√∫squeda web</li>
</ul>
<p><strong>3. Patrones arquitect√≥nicos</strong></p>
<ul>
<li>Dominamos la configuraci√≥n modular y las f√°bricas de modelos</li>
<li>Implementamos el manejo de errores y <strong>mecanismos de reintento</strong> para soluciones listas para producci√≥n</li>
</ul>
<h3>Ventajas clave del enfoque</h3>
<p><strong>LangGraph + MCP</strong> nos brindan:</p>
<ul>
<li><strong>Transparencia</strong> ‚Äî cada paso del agente est√° documentado y es rastreable</li>
<li><strong>Extensibilidad</strong> ‚Äî se a√±aden nuevas funcionalidades de forma declarativa</li>
<li><strong>Fiabilidad</strong> ‚Äî manejo de errores y recuperaci√≥n integrados</li>
<li><strong>Flexibilidad</strong> ‚Äî soporte para m√∫ltiples modelos y proveedores de forma predeterminada</li>
</ul>
<h3>Conclusi√≥n</h3>
<p>Los agentes de IA no son una fantas√≠a futurista, sino una <strong>tecnolog√≠a real de hoy</strong>. Con LangGraph y MCP, podemos crear sistemas que resuelvan problemas de negocio espec√≠ficos, automaticen rutinas y abran nuevas posibilidades.</p>
<p><strong>Lo principal es empezar.</strong> Toma el c√≥digo de los ejemplos, ad√°ptalo a tus tareas, experimenta. Cada proyecto es una nueva experiencia y un paso hacia la maestr√≠a en el campo del desarrollo de IA.</p>
<p>¬°Mucha suerte con tus proyectos!</p>
<hr>
<p><em>Etiquetas: python, ia, mcp, langchain, asistente de ia, ollama, agentes de ia, llm local, langgraph, mcp-server</em><br>
<em>Hubs: Blog de la empresa Amvera, Procesamiento del lenguaje natural, Inteligencia artificial, Python, Programaci√≥n</em><br>
<img src="https://habr.com/ru/companies/amvera/articles/929568/" alt="habr"></p>
