# C√≥mo ense√±ar a una red neuronal a trabajar con sus manos: creaci√≥n de un agente de IA completo con MCP y LangGraph en una hora

¬°Amigos, saludos! Espero que me hayan extra√±ado.

Durante los √∫ltimos dos meses, me he sumergido profundamente en la investigaci√≥n de la integraci√≥n de agentes de IA en mis propios proyectos de Python. En el proceso, he acumulado una gran cantidad de conocimientos pr√°cticos y observaciones que ser√≠a un pecado no compartir. As√≠ que hoy regreso a Habr, con un nuevo tema, una perspectiva fresca y la intenci√≥n de escribir m√°s a menudo.

En la agenda est√°n LangGraph y MCP: herramientas con las que puedes crear agentes de IA realmente √∫tiles.

Si antes discut√≠amos sobre qu√© red neuronal responde mejor en ruso, hoy el campo de batalla se ha desplazado hacia tareas m√°s aplicadas: ¬øqui√©n se desenvuelve mejor en el papel de un agente de IA? ¬øQu√© frameworks realmente simplifican el desarrollo? ¬øY c√≥mo integrar todo esto en un proyecto real?

Pero antes de sumergirnos en la pr√°ctica y el c√≥digo, aclaremos los conceptos b√°sicos. Especialmente dos clave: **agentes de IA y MCP**. Sin ellos, la conversaci√≥n sobre LangGraph estar√° incompleta.

### Agentes de IA en t√©rminos sencillos

Los agentes de IA no son solo chatbots "mejorados". Representan entidades m√°s complejas y aut√≥nomas que poseen dos caracter√≠sticas cruciales:

1.  **Capacidad de interactuar y coordinarse**

    Los agentes modernos pueden dividir tareas en subtareas, llamar a otros agentes, solicitar datos externos, trabajar en equipo. Ya no es un asistente solitario, sino un sistema distribuido donde cada componente puede contribuir.

2.  **Acceso a recursos externos**

    Un agente de IA ya no est√° limitado por los l√≠mites de un di√°logo. Puede acceder a bases de datos, realizar llamadas a API, interactuar con archivos locales, bases de conocimiento vectoriales e incluso ejecutar comandos en el terminal. Todo esto fue posible gracias a la aparici√≥n de MCP, un nuevo nivel de integraci√≥n entre el modelo y el entorno.

---

En pocas palabras: **MCP es un puente entre una red neuronal y su entorno**. Permite que el modelo "comprenda" el contexto de la tarea, acceda a los datos, realice llamadas y forme acciones razonadas, en lugar de simplemente generar respuestas de texto.

**Imaginemos una analog√≠a:**

*   Tienes una **red neuronal** ‚Äî puede razonar y generar textos.
*   Hay **datos y herramientas** ‚Äî documentos, API, bases de conocimiento, terminal, c√≥digo.
*   Y hay **MCP** ‚Äî es una interfaz que permite al modelo interactuar con estas fuentes externas como si fueran parte de su mundo interno.

**Sin MCP:**

El modelo ‚Äî es un motor de di√°logo aislado. Le das texto ‚Äî responde. Y eso es todo.

**Con MCP:**

El modelo se convierte en un **ejecutor de tareas** completo:

*   obtiene acceso a estructuras de datos y API;
*   llama a funciones externas;
*   navega por el estado actual del proyecto o aplicaci√≥n;
*   puede recordar, rastrear y cambiar el contexto a medida que avanza el di√°logo;
*   utiliza extensiones como herramientas de b√∫squeda, ejecutores de c√≥digo, bases de datos de incrustaciones vectoriales, etc.

En un sentido t√©cnico, **MCP es un protocolo para la interacci√≥n entre un LLM y su entorno**, donde el contexto se proporciona como objetos estructurados (en lugar de texto "en bruto"), y las llamadas se formatean como operaciones interactivas (por ejemplo, llamadas a funciones, uso de herramientas o acciones de agente). Esto es lo que convierte un modelo ordinario en un **verdadero agente de IA**, capaz de hacer m√°s que solo "hablar".

### ¬°Y ahora, al grano!

Ahora que hemos cubierto los conceptos b√°sicos, es l√≥gico preguntar: "¬øC√≥mo implementamos todo esto en la pr√°ctica en Python?"

Aqu√≠ es donde entra en juego **LangGraph**, un potente framework para construir gr√°ficos de estado, comportamientos de agente y cadenas de pensamiento. Te permite "unir" la l√≥gica de interacci√≥n entre agentes, herramientas y el usuario, creando una arquitectura de IA viva que se adapta a las tareas.

En las siguientes secciones, veremos c√≥mo:

*   construir un agente desde cero;
*   crear estados, transiciones y eventos;
*   integrar funciones y herramientas;
*   y c√≥mo funciona todo este ecosistema en un proyecto real.

### Un poco de teor√≠a: ¬øqu√© es LangGraph?

Antes de sumergirnos en la pr√°ctica, unas palabras sobre el framework en s√≠.

**LangGraph** es un proyecto del equipo de **LangChain**, los mismos que propusieron por primera vez el concepto de "cadenas" de interacci√≥n con los LLM. Si antes el enfoque principal estaba en las tuber√≠as lineales o condicionalmente ramificadas (langchain.chains), ahora los desarrolladores apuestan por un **modelo de grafo**, y LangGraph es lo que recomiendan como el nuevo "n√∫cleo" para construir sistemas de IA complejos.

**LangGraph** es un framework para construir m√°quinas de estados finitos y grafos de estados, donde cada **nodo** representa una parte de la l√≥gica del agente: una llamada a un modelo, una herramienta externa, una condici√≥n, una entrada de usuario, etc.

### C√≥mo funciona: grafos y nodos

Conceptualmente, LangGraph se basa en las siguientes ideas:

*   **Grafo** ‚Äî es una estructura que describe las posibles rutas de ejecuci√≥n de la l√≥gica. Puedes pensarlo como un mapa: de un punto puedes moverte a otro dependiendo de las condiciones o el resultado de la ejecuci√≥n.
*   **Nodos** ‚Äî son pasos espec√≠ficos dentro del grafo. Cada nodo realiza alguna funci√≥n: llama a un modelo, llama a una API externa, verifica una condici√≥n o simplemente actualiza el estado interno.
*   **Transiciones entre nodos** ‚Äî es la l√≥gica de enrutamiento: si el resultado del paso anterior es tal y cual, entonces ve all√≠.
*   **Estado** ‚Äî se pasa entre nodos y acumula todo lo necesario: historial, conclusiones intermedias, entrada de usuario, resultados de operaciones de herramientas, etc.

As√≠, obtenemos un **mecanismo flexible para controlar la l√≥gica del agente**, en el que se pueden describir escenarios tanto simples como muy complejos: bucles, condiciones, acciones paralelas, llamadas anidadas y mucho m√°s.

### ¬øPor qu√© es conveniente?

LangGraph te permite construir una **l√≥gica transparente, reproducible y extensible**:

*   f√°cil de depurar;
*   f√°cil de visualizar;
*   f√°cil de escalar para nuevas tareas;
*   f√°cil de integrar herramientas externas y protocolos MCP.

En esencia, LangGraph es el **"cerebro" del agente**, donde cada paso est√° documentado, controlable y puede modificarse sin caos ni "magia".

### ¬°Y ahora, basta de teor√≠a!

Podr√≠amos hablar mucho tiempo sobre grafos, estados, composici√≥n l√≥gica y las ventajas de LangGraph sobre las tuber√≠as cl√°sicas. Pero, como muestra la pr√°ctica, es mejor verlo una vez en el c√≥digo.

**Es hora de pasar a la pr√°ctica.** A continuaci√≥n, un ejemplo en Python: crearemos un agente de IA simple pero √∫til basado en LangGraph que utilizar√° herramientas externas, memoria y tomar√° sus propias decisiones.

### Preparaci√≥n: redes neuronales en la nube y locales

Para empezar a crear agentes de IA, primero necesitamos un **cerebro**, un modelo de lenguaje. Aqu√≠ hay dos enfoques:

*   **usar soluciones en la nube**, donde todo est√° listo "de f√°brica";
*   o **levantar el modelo localmente** ‚Äî para una autonom√≠a y confidencialidad completas.

Consideremos ambas opciones.

#### Servicios en la nube: r√°pidos y convenientes

La forma m√°s sencilla es utilizar el poder de los grandes proveedores: OpenAI, Anthropic, y utilizar...

### D√≥nde obtener claves y tokens:

*   **OpenAI** ‚Äî ChatGPT y otros productos;
*   **Anthropic** ‚Äî Claude;
*   **OpenRouter.ai** ‚Äî docenas de modelos (un token ‚Äî muchos modelos a trav√©s de una API compatible con OpenAI);
*   **Amvera Cloud** ‚Äî capacidad de conectar LLAMA con pago en rublos y proxy integrado a OpenAI y Anthropic.

Este camino es conveniente, especialmente si:

*   no quieres configurar la infraestructura;
*   desarrollas con un enfoque en la velocidad;
*   trabajas con recursos limitados.

### Modelos locales: control total

Si la **privacidad, el trabajo sin conexi√≥n** son importantes para ti, o quieres construir **agentes completamente aut√≥nomos**, entonces tiene sentido desplegar la red neuronal localmente.

**Principales ventajas:**

*   **Confidencialidad** ‚Äî los datos permanecen contigo;
*   **Trabajo sin conexi√≥n** ‚Äî √∫til en redes aisladas;
*   **Sin suscripciones ni tokens** ‚Äî gratis despu√©s de la configuraci√≥n.

**Las desventajas son obvias:**

*   Requisitos de recursos (especialmente para la memoria de video);
*   La configuraci√≥n puede llevar tiempo;
*   Algunos modelos son dif√≠ciles de desplegar sin experiencia.

Sin embargo, existen herramientas que facilitan el lanzamiento local. Una de las mejores hoy en d√≠a es **Ollama**.

### Despliegue de un LLM local a trav√©s de Ollama + Docker

Prepararemos un lanzamiento local del modelo Qwen 2.5 (qwen2.5:32b) utilizando un contenedor Docker y el sistema Ollama. Esto permitir√° integrar la red neuronal con MCP y usarla en tus propios agentes basados en LangGraph.

Si los recursos inform√°ticos de tu ordenador o servidor son insuficientes para trabajar con esta versi√≥n del modelo, siempre puedes elegir una red neuronal menos "hambrienta" de recursos; el proceso de instalaci√≥n y lanzamiento seguir√° siendo similar.

**Instalaci√≥n r√°pida (resumen de pasos)**

1.  **Instala Docker + Docker Compose**
2.  **Crea la estructura del proyecto:**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **Crea `docker-compose.yml`**
(opci√≥n universal, GPU detectada autom√°ticamente)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **Inicia el contenedor:**
```bash
docker compose up -d
```

5.  **Descarga el modelo:**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **Verifica el funcionamiento a trav√©s de la API:**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "¬°Hola!", "stream": false}'
```
*(Imagen con el resultado de la ejecuci√≥n del comando curl)*

7.  **Integraci√≥n con Python:**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Explica el entrelazamiento cu√°ntico"))
```
Ahora tienes un LLM local completo, listo para trabajar con MCP y LangGraph.

**¬øQu√© sigue?**

Tenemos la opci√≥n de elegir entre modelos en la nube y locales, y hemos aprendido a conectar ambos. Lo m√°s interesante est√° por venir: **la creaci√≥n de agentes de IA en LangGraph**, que utilizan el modelo seleccionado, la memoria, las herramientas y su propia l√≥gica.

**¬°Pasemos a la parte m√°s sabrosa: el c√≥digo y la pr√°ctica!**

---

Antes de pasar a la pr√°ctica, es importante preparar el entorno de trabajo. Asumo que ya est√°s familiarizado con los conceptos b√°sicos de Python, sabes qu√© son las bibliotecas y las dependencias, y entiendes por qu√© usar un entorno virtual.

Si todo esto es nuevo para ti, te recomiendo que primero tomes un curso corto o una gu√≠a sobre los conceptos b√°sicos de Python, y luego regreses al art√≠culo.

#### Paso 1: Creaci√≥n de un entorno virtual

Crea un nuevo entorno virtual en la carpeta del proyecto:
```bash
python -m venv venv
source venv/bin/activate  # para Linux/macOS
venv\Scripts\activate   # para Windows
```

#### Paso 2: Instalaci√≥n de dependencias

Crea un archivo `requirements.txt` y a√±ade las siguientes l√≠neas:
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ‚ö†Ô∏è **Las versiones actuales son a 21 de julio de 2025.** Es posible que hayan cambiado desde la publicaci√≥n; **verifica la relevancia antes de la instalaci√≥n.**

Luego instala las dependencias:
```bash
pip install -r requirements.txt```

#### Paso 3: Configuraci√≥n de variables de entorno

Crea un archivo `.env` en la ra√≠z del proyecto y a√±ade las claves API necesarias:
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Prop√≥sito de las variables:**

*   **OPENAI_API_KEY** ‚Äî clave para acceder a los modelos GPT de OpenAI;
*   **DEEPSEEK_API_KEY** ‚Äî clave para usar los modelos Deepseek;
*   **OPENROUTER_API_KEY** ‚Äî clave √∫nica para acceder a muchos modelos a trav√©s de OpenRouter

---
Algunas herramientas MCP (por ejemplo, `brave-web-search`) requieren una clave para funcionar. Sin ella, simplemente no se activar√°n.

**¬øQu√© pasa si no tienes claves API?**

No hay problema. Puedes comenzar el desarrollo con un modelo local (por ejemplo, a trav√©s de Ollama), sin conectar ning√∫n servicio externo. En este caso, no es necesario crear el archivo `.env` en absoluto.

¬°Listo! Ahora tenemos todo lo necesario para empezar: un entorno aislado, dependencias y, si es necesario, acceso a redes neuronales en la nube e integraciones MCP.

A continuaci√≥n, ejecutaremos nuestro agente LLM de diferentes maneras.

### Lanzamiento simple de agentes LLM a trav√©s de LangGraph: integraci√≥n b√°sica

Comencemos con lo m√°s simple: c√≥mo "conectar el cerebro" al futuro agente. Analizaremos las formas b√°sicas de lanzar modelos de lenguaje (LLM) usando LangChain, para que en el siguiente paso podamos pasar a la integraci√≥n con LangGraph y construir un agente de IA completo.

#### Importaciones
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` y `load_dotenv()` ‚Äî para cargar variables del archivo `.env`.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` ‚Äî envoltorios para conectar modelos de lenguaje a trav√©s de LangChain.

> üí° Si utilizas enfoques alternativos para trabajar con configuraciones (por ejemplo, Pydantic Settings), puedes reemplazar `load_dotenv()` por tu m√©todo habitual.

#### Carga de variables de entorno
```python
load_dotenv()
```
Esto cargar√° todas las variables de `.env`, incluidas las claves para acceder a las API de OpenAI, DeepSeek, OpenRouter y otras.

#### Funciones simples para obtener LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
Si la variable `OPENAI_API_KEY` est√° configurada correctamente, LangChain la sustituir√° autom√°ticamente; la especificaci√≥n expl√≠cita de `api_key=...` aqu√≠ es opcional.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
De manera similar, pero usamos el envoltorio `ChatDeepSeek`.

**OpenRouter (y otras API compatibles)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Caracter√≠sticas:**

*   Se utiliza `ChatOpenAI`, aunque el modelo no sea de OpenAI, porque OpenRouter utiliza el mismo protocolo.
*   `base_url` es obligatorio: apunta a la API de OpenRouter.
*   El modelo `moonshotai/kimi-k2:free` fue elegido como una de las opciones m√°s equilibradas en t√©rminos de calidad y velocidad en el momento de escribir este art√≠culo.
*   La clave API de `OpenRouter` debe pasarse expl√≠citamente; la sustituci√≥n autom√°tica no funciona aqu√≠.

#### Mini-prueba: comprobaci√≥n del funcionamiento del modelo
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("¬øQui√©n eres?")
    print(response.content)
```
*(Imagen con el resultado de la ejecuci√≥n del comando)*

Si todo est√° configurado correctamente, recibir√°s una respuesta significativa del modelo. ¬°Felicidades, el primer paso est√° hecho!

### Pero esto a√∫n no es un agente

En la etapa actual, hemos conectado LLM y hemos realizado una llamada simple. Esto se parece m√°s a un chatbot de consola que a un agente de IA.

**¬øPor qu√©?**

*   Escribimos **c√≥digo s√≠ncrono y lineal** sin l√≥gica de estado ni objetivos.
*   El agente no toma decisiones, no recuerda el contexto y no utiliza herramientas.
*   MCP y LangGraph a√∫n no est√°n involucrados.

**¬øQu√© sigue?**

A continuaci√≥n, implementaremos un **agente de IA completo** utilizando **LangGraph**, primero sin MCP, para centrarnos en la arquitectura, los estados y la l√≥gica del propio agente.

¬°Sum√©rgete en la verdadera mec√°nica de los agentes! ¬°Vamos!

### Agente de clasificaci√≥n de vacantes: de la teor√≠a a la pr√°ctica

...conceptos de LangGraph en la pr√°ctica y crear una herramienta √∫til para plataformas de RRHH e intercambios de freelancers.

#### Tarea del agente

Nuestro agente toma como entrada una descripci√≥n de texto de una vacante o servicio y realiza una clasificaci√≥n de tres niveles:

1.  **Tipo de trabajo**: trabajo por proyecto o vacante permanente
2.  **Categor√≠a profesional**: de m√°s de 45 especialidades predefinidas
3.  **Tipo de b√∫squeda**: si una persona busca trabajo o busca un int√©rprete

El resultado se devuelve en un formato JSON estructurado con una puntuaci√≥n de confianza para cada clasificaci√≥n.

#### üìà Arquitectura del agente en LangGraph

Siguiendo los principios de LangGraph, creamos un **grafo de estados** de cuatro nodos:

- Descripci√≥n de entrada
- ‚Üì
- Nodo de clasificaci√≥n del tipo de trabajo
- ‚Üì
- Nodo de clasificaci√≥n de categor√≠a
- ‚Üì
- Nodo de determinaci√≥n del tipo de b√∫squeda
- ‚Üì
- Nodo de c√°lculo de confianza
- ‚Üì
- Resultado JSON

Cada nodo es una **funci√≥n especializada** que:

*   Recibe el estado actual del agente
*   Realiza su parte del an√°lisis
*   Actualiza el estado y lo pasa

#### Gesti√≥n de estados

Definimos la **estructura de memoria del agente** a trav√©s de `TypedDict`:

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """Estado del agente para almacenar informaci√≥n sobre el proceso de clasificaci√≥n"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

Esta es la **memoria de trabajo del agente**, todo lo que recuerda y acumula durante el an√°lisis. Similar a c√≥mo un experto humano mantiene el contexto de la tarea en mente al analizar un documento.

Veamos el c√≥digo completo y luego nos centraremos en los puntos principales.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Categor√≠as profesionales
CATEGORIES = [
    "Animador 2D", "Animador 3D", "Modelador 3D",
    "Analista de negocios", "Desarrollador Blockchain", ...
]

class JobType(Enum):
    PROJECT = "trabajo por proyecto"
    PERMANENT = "trabajo permanente"

class SearchType(Enum):
    LOOKING_FOR_WORK = "buscando trabajo"
    LOOKING_FOR_PERFORMER = "buscando un int√©rprete"

class State(TypedDict):
    """Estado del agente para almacenar informaci√≥n sobre el proceso de clasificaci√≥n"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Agente as√≠ncrono para clasificar vacantes y servicios"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Inicializaci√≥n del agente"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Crea el flujo de trabajo del agente basado en LangGraph"""
        workflow = StateGraph(State)
        
        # A√±adir nodos al grafo
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # Definir la secuencia de ejecuci√≥n de nodos
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """Nodo para determinar el tipo de trabajo: por proyecto o permanente"""
        # ... (la implementaci√≥n sigue)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """Nodo para determinar la categor√≠a profesional"""
        # ... (la implementaci√≥n sigue)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """Nodo para determinar el tipo de b√∫squeda"""
        # ... (la implementaci√≥n sigue)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """Nodo para calcular el nivel de confianza en la clasificaci√≥n"""
        # ... (la implementaci√≥n sigue)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Encuentra la categor√≠a m√°s cercana de la lista de disponibles"""
        # ... (la implementaci√≥n sigue)

    async def classify(self, description: str) -> Dict[str, Any]:
        """M√©todo principal para clasificar vacantes/servicios"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Iniciar el flujo de trabajo
        result = await self.workflow.ainvoke(initial_state)
        
        # Formar la respuesta JSON final
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Demostraci√≥n del funcionamiento del agente"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "Se requiere desarrollador Python para crear una aplicaci√≥n web en Django. Trabajo permanente.",
        "Busco pedidos para crear logotipos e identidad corporativa. Trabajo en Adobe Illustrator.",
        "Se necesita un animador 3D para un proyecto a corto plazo de creaci√≥n de un spot publicitario.",
        "Curr√≠culum: comercializador experimentado, busco trabajo remoto en marketing digital",
        "Buscamos un desarrollador frontend React para nuestro equipo de forma permanente"
    ]
    
    print("ü§ñ Demostraci√≥n del funcionamiento del agente de clasificaci√≥n de vacantes\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Prueba {i}: ---")
        print(f"Descripci√≥n: {description}")
        try:
            result = await agent.classify(description)
            print("Resultado de la clasificaci√≥n:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå Error: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...el resto del c√≥digo con la implementaci√≥n de los m√©todos se present√≥ en el art√≠culo...)*

### Ventajas clave de la arquitectura
1.  **Modularidad** ‚Äî cada nodo resuelve una tarea, f√°cil de probar y mejorar por separado
2.  **Extensibilidad** ‚Äî se pueden a√±adir nuevos nodos de an√°lisis sin modificar los existentes
3.  **Transparencia** ‚Äî todo el proceso de toma de decisiones est√° documentado y es rastreable
4.  **Rendimiento** ‚Äî procesamiento as√≠ncrono de m√∫ltiples solicitudes
5.  **Fiabilidad** ‚Äî mecanismos de respaldo y manejo de errores

### Beneficios reales
Un agente as√≠ puede utilizarse en:
*   **Plataformas de RRHH** para la categorizaci√≥n autom√°tica de curr√≠culums y vacantes
*   **Bolsas de trabajo freelance** para mejorar la b√∫squeda y las recomendaciones
*   **Sistemas internos** de empresas para procesar solicitudes y proyectos
*   **Soluciones anal√≠ticas** para la investigaci√≥n del mercado laboral

### MCP en acci√≥n: creaci√≥n de un agente con sistema de archivos y b√∫squeda web
Una vez que hemos comprendido los principios b√°sicos de LangGraph y hemos creado un agente clasificador simple, ampliemos sus capacidades conect√°ndolo al mundo exterior a trav√©s de MCP.

Ahora crearemos un asistente de IA completo que podr√°:
*   Trabajar con el sistema de archivos (leer, crear, modificar archivos)
*   Buscar informaci√≥n relevante en Internet
*   Recordar el contexto del di√°logo
*   Manejar errores y recuperarse de fallos

#### De la teor√≠a a las herramientas reales
¬øRecuerdas c√≥mo al principio del art√≠culo hablamos de que **MCP es un puente entre una red neuronal y su entorno**? Ahora lo ver√°s en la pr√°ctica. Nuestro agente tendr√° acceso a **herramientas reales**:
```
# Herramientas del sistema de archivos
- read_file ‚Äî leer archivos
- write_file ‚Äî escribir y crear archivos
- list_directory ‚Äî ver el contenido de las carpetas
- create_directory ‚Äî crear carpetas

# Herramientas de b√∫squeda web
- brave_web_search ‚Äî buscar en Internet
- get_web_content ‚Äî obtener el contenido de las p√°ginas
```
Este ya no es un agente de "juguete", es una **herramienta de trabajo** que puede resolver problemas reales.

#### üìà Arquitectura: de lo simple a lo complejo

**1. Configuraci√≥n como base de la estabilidad**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Configuraci√≥n simplificada del agente de IA"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Validaci√≥n de la configuraci√≥n"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"La ruta no existe: {self.filesystem_path}")
```
**¬øPor qu√© es importante?** A diferencia del ejemplo de clasificaci√≥n, aqu√≠ el agente interact√∫a con sistemas externos. Un error en la ruta del archivo o una clave API faltante, y todo el agente deja de funcionar. La **validaci√≥n al inicio** ahorra horas de depuraci√≥n.

**2. F√°brica de modelos: elecci√≥n flexible**
```python
def create_model(config: AgentConfig):
    """Crea un modelo seg√∫n la configuraci√≥n"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... otros proveedores
```
Un c√≥digo, muchos modelos. ¬øQuieres un modelo local gratuito? Usa Ollama. ¬øNecesitas la m√°xima precisi√≥n? Cambia a GPT-4. ¬øLa velocidad es importante? Prueba DeepSeek. El c√≥digo sigue siendo el mismo.

**3. Integraci√≥n MCP: conexi√≥n con el mundo real**
```python
async def _init_mcp_client(self):
    """Inicializaci√≥n del cliente MCP"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
Aqu√≠ ocurre el trabajo clave de MCP: conectamos servidores MCP externos al agente, que proporcionan un conjunto de herramientas y funciones. El agente no solo recibe funciones individuales, sino una comprensi√≥n contextual completa de c√≥mo trabajar con el sistema de archivos e Internet.

#### Resistencia a errores
En el mundo real, todo falla: la red no est√° disponible, los archivos est√°n bloqueados, las claves API han caducado. Nuestro agente est√° preparado para esto:
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
El decorador `@retry_on_failure` reintenta autom√°ticamente las operaciones en caso de fallos temporales. El usuario ni siquiera notar√° que algo sali√≥ mal.

### Resultados: de la teor√≠a a la pr√°ctica de los agentes de IA

Hoy hemos recorrido el camino desde los conceptos b√°sicos hasta la creaci√≥n de agentes de IA funcionales. Resumamos lo que hemos aprendido y logrado.

**Lo que hemos dominado**

**1. Conceptos fundamentales**
*   Comprendimos la diferencia entre chatbots y verdaderos agentes de IA
*   Entendimos el papel de **MCP (Protocolo de Contexto del Modelo)** como puente entre el modelo y el mundo exterior
*   Estudiamos la arquitectura de **LangGraph** para construir una l√≥gica de agente compleja

**2. Habilidades pr√°cticas**
*   Configuramos un entorno de trabajo con soporte para modelos en la nube y locales
*   Creamos un **agente clasificador** con arquitectura as√≠ncrona y gesti√≥n de estados
*   Construimos un **agente MCP** con acceso al sistema de archivos y b√∫squeda web

**3. Patrones arquitect√≥nicos**
*   Dominamos la configuraci√≥n modular y las f√°bricas de modelos
*   Implementamos el manejo de errores y los **mecanismos de reintento** para soluciones listas para producci√≥n

### Ventajas clave del enfoque
**LangGraph + MCP** nos brindan:
*   **Transparencia** ‚Äî cada paso del agente est√° documentado y es rastreable
*   **Extensibilidad** ‚Äî se a√±aden nuevas funciones de forma declarativa
*   **Fiabilidad** ‚Äî manejo de errores y recuperaci√≥n integrados
*   **Flexibilidad** ‚Äî soporte para m√∫ltiples modelos y proveedores de forma predeterminada

### Conclusi√≥n

Los agentes de IA no son una ficci√≥n futurista, sino una **tecnolog√≠a real de hoy**. Con LangGraph y MCP, podemos crear sistemas que resuelven problemas de negocio espec√≠ficos, automatizan rutinas y abren nuevas posibilidades.

**Lo principal es empezar.** Toma el c√≥digo de los ejemplos, ad√°ptalo a tus tareas, experimenta. Cada proyecto es una nueva experiencia y un paso hacia la maestr√≠a en el campo del desarrollo de IA.

¬°Mucha suerte con tus proyectos!

---
*Etiquetas: python, ia, mcp, langchain, asistente de ia, ollama, agentes de ia, llm local, langgraph, mcp-server*
*Hubs: Blog de la empresa Amvera, Procesamiento del lenguaje natural, Inteligencia artificial, Python, Programaci√≥n*
