### **Цикл «НЕ Selenium». Интро**

Те, кто занимается веб-скрапингом, тестированием и автоматизацией, знакомы с Selenium, более современным Playwright и/или фреймворком Crawlee. Они мощные, они могут почти все, и они... не всегда нужны. Более того, во многих случаях использование этих инструментов — забивание гвоздей микроскопом: работа, конечно, будет сделана, но ценой неоправданных затрат — скорости, системных ресурсов и сложности настройки.

Добро пожаловать в цикл статей «НЕ Selenium». Здесь я покажу другие способы (не всегда очевидные) взаимодействия с содержимым интернета.

#### Парадигма №1: Прямое общение. HTTP-клиенты

*   **`Requests`** — Формирует и отправляет сетевой запрос к целевому адресу (URL), точно так же, как это делает ваш браузер в самый первый момент загрузки страницы, но без самого браузера. В этот запрос он упаковывает метод (например, `GET`, чтобы получить данные), заголовки (`Headers`), которые представляются сайту (например, `User-Agent: "я-браузер"`), и другие параметры. В ответ от сервера он получает сырые данные — чаще всего, это исходный HTML-код страницы или строка в формате JSON, а также код статуса (например, `200 OK`).

*   **`HTTPX`** — это современный наследник `Requests`. На фундаментальном уровне он делает всё то же самое: отправляет те же самые HTTP-запросы с теми же заголовками и получает те же самые ответы. Но есть ключевое отличие: `Requests` работает **синхронно** — отправил запрос, сидит и ждет ответа, получил ответ, отправил следующий. `HTTPX` же умеет работать **асинхронно** — он может "забросить" сразу сотню запросов, не дожидаясь ответов, и затем эффективно обрабатывать их по мере поступления.

Отлично подходят для сбора данных со статических сайтов, работы с API, парсинга тысяч страниц, где не требуется выполнение JavaScript.

*   **Преимущества:** **Скорость и эффективность.** Благодаря асинхронности `HTTPX`, там, где `Requests` будет последовательно делать 100 запросов несколько минут, `HTTPX` справится за несколько секунд.
*   **Недостатки:** Не подходят для сайтов, где контент генерируется с помощью JavaScript.

#### Парадигма №2: Chrome DevTools Protocol (CDP)

Что делать, если сайт динамический и контент генерируется с помощью JavaScript? Современные браузеры (Chrome, Chromium, Edge) имеют встроенный протокол для отладки и управления — **Chrome DevTools Protocol (CDP)**. Он позволяет отдавать команды браузеру напрямую, минуя громоздкую прослойку в виде WebDriver, которую использует Selenium.

*   **Инструменты:** Основным представителем этого подхода сегодня является `Pydoll`, который пришел на смену некогда популярному, но ныне не поддерживаемому `pyppeteer`.
*   **Когда использовать:** Когда нужен рендеринг JavaScript, но хочется сохранить высокую скорость и избежать сложностей с драйверами.
*   **Преимущества:** **Баланс.** Вы получаете мощь настоящего браузера, но с гораздо меньшими накладными расходами и зачастую со встроенными механизмами обхода защит.
*   **Недостатки:** Может быть сложнее в отладке, чем Playwright, и требует более глубокого понимания работы браузера.

#### Парадигма №3: Автономные LLM-агенты

Это самый передовой рубеж. Что, если вместо того, чтобы писать код, который говорит "кликни сюда, введи это", мы просто дадим задачу на естественном языке? "Найди мне всех поставщиков на этом сайте и собери их категории товаров".

Именно эту задачу решают LLM-агенты. Используя "мозг" в виде большой языковой модели (GPT, Gemini) и "руки" в виде набора инструментов (браузер, поиск Google), эти агенты могут самостоятельно планировать и выполнять сложные задачи в вебе.

*   **Инструменты:** Связки вроде `LangChain` + `Pydoll` или кастомные решения, как в `simple_browser.py`, который мы разберем позже.
*   **Когда использовать:** Для сложных исследовательских задач, где шаги заранее неизвестны и требуется адаптация в реальном времени.
*   **Преимущества:** **Интеллект.** Способность решать неструктурированные задачи и адаптироваться к изменениям на лету.
*   **Недостатки:** "Недетерминированность" (результат может меняться от запуска к запуску), стоимость API-вызовов к LLM, более низкая скорость по сравнению с прямым кодом.

#### Парадигма №4: Скрапинг без кода

Иногда задача настолько проста, что писать код — это излишество. Нужно быстро вытащить таблицу с одной страницы? Для этого существуют элегантные решения, не требующие программирования.

*   **Инструменты:** Функции Google Sheets (`IMPORTXML`, `IMPORTHTML`), браузерные расширения.
*   **Когда использовать:** Для одноразовых задач, быстрого прототипирования или когда вы просто не хотите писать код.
*   **Преимущества:** **Простота.** Открыл, указал, что нужно собрать, — получил результат.
*   **Недостатки:** Ограниченная функциональность, не подходят для сложных задач или больших объемов данных.

### Что дальше?

Эта статья — лишь введение. В следующих выпусках нашего цикла «НЕ Selenium» мы перейдем от теории к жесткой практике. Мы глубоко погрузимся в каждую из этих парадигм и покажем, как они работают на реальных примерах:

*   Разберем **Pydoll** и посмотрим, как он обходит Cloudflare.
*   Устроим битву **JavaScript против Python** за звание лучшего языка для скрапинга.
*   Научимся выжимать максимум скорости из парсинга с помощью **lxml**.
*   Напишем скрипт, который собирает данные с **Amazon** и сохраняет их в **Excel**.
*   Покажем, как **Google Sheets** может стать вашим первым скрапером.
*   И, конечно же, детально разберем, как создать и использовать **автономного LLM-агента** для управления браузером.

Приготовьтесь изменить свой взгляд на автоматизацию и сбор данных в вебе. Будет быстро, эффективно и очень интересно. Подписывайтесь

