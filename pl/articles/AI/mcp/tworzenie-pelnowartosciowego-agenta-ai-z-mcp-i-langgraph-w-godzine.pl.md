## Jak nauczyÄ‡ sieÄ‡ neuronowÄ… pracowaÄ‡ rÄ™kami: tworzenie peÅ‚nowartoÅ›ciowego agenta AI z MCP i LangGraph w godzinÄ™


Przyjaciele, witam! Mam nadziejÄ™, Å¼e stÄ™skniliÅ›cie siÄ™.

Przez ostatnie kilka miesiÄ™cy byÅ‚em gÅ‚Ä™boko zaangaÅ¼owany w badanie integracji agentÃ³w AI we wÅ‚asnych projektach Python. W tym procesie zgromadziÅ‚em sporo praktycznej wiedzy i obserwacji, ktÃ³rymi po prostu grzechem byÅ‚oby siÄ™ nie podzieliÄ‡. Dlatego dziÅ› wracam na Habr â€“ z nowym tematem, Å›wieÅ¼ym spojrzeniem i zamiarem czÄ™stszego pisania.

Na porzÄ…dku dziennym sÄ… LangGraph i MCP: narzÄ™dzia, za pomocÄ… ktÃ³rych moÅ¼na tworzyÄ‡ naprawdÄ™ uÅ¼yteczne agenty AI.

JeÅ›li wczeÅ›niej spieraliÅ›my siÄ™ o to, ktÃ³ra sieÄ‡ neuronowa lepiej odpowiada po rosyjsku, to dziÅ› pole bitwy przesunÄ™Å‚o siÄ™ w stronÄ™ bardziej praktycznych zadaÅ„: kto lepiej radzi sobie z rolÄ… agenta AI? Jakie frameworki naprawdÄ™ upraszczajÄ… rozwÃ³j? I jak zintegrowaÄ‡ to wszystko w prawdziwym projekcie?

Ale zanim zanurkujemy w praktykÄ™ i kod, przyjrzyjmy siÄ™ podstawowym pojÄ™ciom. ZwÅ‚aszcza dwÃ³m kluczowym: **agentom AI i MCP**. Bez nich rozmowa o LangGraph bÄ™dzie niekompletna.

### Agenty AI w prostych sÅ‚owach

Agenty AI to nie tylko â€ulepszoneâ€ chatboty. SÄ… to bardziej zÅ‚oÅ¼one, autonomiczne byty, ktÃ³re posiadajÄ… dwie najwaÅ¼niejsze cechy:

1.  **UmiejÄ™tnoÅ›Ä‡ interakcji i koordynacji**

    WspÃ³Å‚czesne agenty potrafiÄ… dzieliÄ‡ zadania na podzadania, wywoÅ‚ywaÄ‡ inne agenty, Å¼Ä…daÄ‡ danych zewnÄ™trznych, pracowaÄ‡ w zespole. To juÅ¼ nie samotny asystent, ale rozproszony system, w ktÃ³rym kaÅ¼dy komponent moÅ¼e wnieÅ›Ä‡ swÃ³j wkÅ‚ad.

2.  **DostÄ™p do zasobÃ³w zewnÄ™trznych**

    Agent AI nie jest juÅ¼ ograniczony ramami dialogu. MoÅ¼e uzyskiwaÄ‡ dostÄ™p do baz danych, wykonywaÄ‡ wywoÅ‚ania API, wchodziÄ‡ w interakcje z plikami lokalnymi, wektorowymi bazami wiedzy, a nawet uruchamiaÄ‡ polecenia w terminalu. Wszystko to staÅ‚o siÄ™ moÅ¼liwe dziÄ™ki pojawieniu siÄ™ MCP â€“ nowego poziomu integracji miÄ™dzy modelem a Å›rodowiskiem.

---

JeÅ›li mÃ³wiÄ‡ prosto: **MCP to most miÄ™dzy sieciÄ… neuronowÄ… a jej otoczeniem**. Pozwala modelowi â€rozumieÄ‡â€ kontekst zadania, uzyskiwaÄ‡ dostÄ™p do danych, wykonywaÄ‡ wywoÅ‚ania i formuÅ‚owaÄ‡ uzasadnione dziaÅ‚ania, a nie tylko wydawaÄ‡ odpowiedzi tekstowe.

**WyobraÅºmy sobie analogiÄ™:**

*   Masz **sieÄ‡ neuronowÄ…** â€“ potrafi rozumowaÄ‡ i generowaÄ‡ teksty.
*   SÄ… **dane i narzÄ™dzia** â€“ dokumenty, API, bazy wiedzy, terminal, kod.
*   I jest **MCP** â€“ to interfejs, ktÃ³ry pozwala modelowi wchodziÄ‡ w interakcje z tymi zewnÄ™trznymi ÅºrÃ³dÅ‚ami tak, jakby byÅ‚y czÄ™Å›ciÄ… jego wewnÄ™trznego Å›wiata.

**Bez MCP:**

Model to izolowany silnik dialogowy. Podajesz mu tekst â€“ odpowiada. I tyle.

**Z MCP:**

Model staje siÄ™ peÅ‚noprawnym **wykonawcÄ… zadaÅ„**:

*   uzyskuje dostÄ™p do struktur danych i API;
*   wywoÅ‚uje funkcje zewnÄ™trzne;
*   orientuje siÄ™ w bieÅ¼Ä…cym stanie projektu lub aplikacji;
*   moÅ¼e zapamiÄ™tywaÄ‡, Å›ledziÄ‡ i zmieniaÄ‡ kontekst w trakcie dialogu;
*   wykorzystuje rozszerzenia, takie jak narzÄ™dzia wyszukiwania, uruchamiacze kodu, bazÄ™ osadzeÅ„ wektorowych itp.

Technicznie rzecz biorÄ…c, **MCP to protokÃ³Å‚ interakcji miÄ™dzy LLM a jego otoczeniem**, gdzie kontekst jest dostarczany w postaci ustrukturyzowanych obiektÃ³w (zamiast â€surowegoâ€ tekstu), a wywoÅ‚ania sÄ… formalizowane jako operacje interaktywne (np. wywoÅ‚ywanie funkcji, uÅ¼ycie narzÄ™dzi lub akcje agenta). To wÅ‚aÅ›nie to przeksztaÅ‚ca zwykÅ‚y model w **prawdziwego agenta AI**, zdolnego do zrobienia czegoÅ› wiÄ™cej niÅ¼ tylko â€rozmowyâ€.

### A teraz â€“ do dzieÅ‚a!

Teraz, gdy omÃ³wiliÅ›my podstawowe pojÄ™cia, logiczne jest zadaÄ‡ sobie pytanie: â€Jak to wszystko zaimplementowaÄ‡ w praktyce w Pythonie?â€

Tutaj wkracza **LangGraph** â€“ potÄ™Å¼ny framework do budowania grafÃ³w stanÃ³w, zachowaÅ„ agentÃ³w i Å‚aÅ„cuchÃ³w myÅ›lowych. Pozwala on â€zszyÄ‡â€ logikÄ™ interakcji miÄ™dzy agentami, narzÄ™dziami i uÅ¼ytkownikiem, tworzÄ…c Å¼ywÄ… architekturÄ™ AI, ktÃ³ra dostosowuje siÄ™ do zadaÅ„.

W kolejnych sekcjach przyjrzymy siÄ™, jak:

*   buduje siÄ™ agenta od podstaw;
*   tworzy siÄ™ stany, przejÅ›cia i zdarzenia;
*   integruje siÄ™ funkcje i narzÄ™dzia;
*   i jak caÅ‚y ten ekosystem dziaÅ‚a w prawdziwym projekcie.

### TrochÄ™ teorii: co to jest LangGraph

Zanim przejdziemy do praktyki, trzeba powiedzieÄ‡ kilka sÅ‚Ã³w o samym frameworku.

**LangGraph** to projekt zespoÅ‚u **LangChain**, tych samych, ktÃ³rzy jako pierwsi zaproponowali koncepcjÄ™ â€Å‚aÅ„cuchÃ³wâ€ (chains) interakcji z LLM. JeÅ›li wczeÅ›niej gÅ‚Ã³wny nacisk kÅ‚adziono na liniowe lub warunkowo rozgaÅ‚Ä™zione potoki (langchain.chains), to teraz deweloperzy stawiajÄ… na **model grafowy**, i to wÅ‚aÅ›nie LangGraph polecajÄ… jako nowe â€jÄ…droâ€ do budowania zÅ‚oÅ¼onych systemÃ³w AI.

**LangGraph** to framework do budowania skoÅ„czonych automatÃ³w stanÃ³w i grafÃ³w stanÃ³w, w ktÃ³rych kaÅ¼dy **wÄ™zeÅ‚** reprezentuje czÄ™Å›Ä‡ logiki agenta: wywoÅ‚anie modelu, narzÄ™dzie zewnÄ™trzne, warunek, dane wejÅ›ciowe uÅ¼ytkownika itp.

### Jak to dziaÅ‚a: grafy i wÄ™zÅ‚y

Konceptualnie LangGraph opiera siÄ™ na nastÄ™pujÄ…cych ideach:

*   **Graf** â€“ to struktura, ktÃ³ra opisuje moÅ¼liwe Å›cieÅ¼ki wykonania logiki. MoÅ¼na o nim myÅ›leÄ‡ jak o mapie: z jednego punktu moÅ¼na przejÅ›Ä‡ do drugiego w zaleÅ¼noÅ›ci od warunkÃ³w lub wyniku wykonania.
*   **WÄ™zÅ‚y** â€“ to konkretne kroki w grafie. KaÅ¼dy wÄ™zeÅ‚ wykonuje jakÄ…Å› funkcjÄ™: wywoÅ‚uje model, wywoÅ‚uje zewnÄ™trzne API, sprawdza warunek lub po prostu aktualizuje stan wewnÄ™trzny.
*   **PrzejÅ›cia miÄ™dzy wÄ™zÅ‚ami** â€“ to logika routingu: jeÅ›li wynik poprzedniego kroku jest taki, to idziemy tam.
*   **Stan** â€“ jest przekazywany miÄ™dzy wÄ™zÅ‚ami i gromadzi wszystko, co potrzebne: historiÄ™, poÅ›rednie wnioski, dane wejÅ›ciowe uÅ¼ytkownika, wyniki dziaÅ‚ania narzÄ™dzi itp.

W ten sposÃ³b uzyskujemy **elastyczny mechanizm sterowania logikÄ… agenta**, w ktÃ³rym moÅ¼na opisywaÄ‡ zarÃ³wno proste, jak i bardzo zÅ‚oÅ¼one scenariusze: pÄ™tle, warunki, dziaÅ‚ania rÃ³wnolegÅ‚e, zagnieÅ¼dÅ¼one wywoÅ‚ania i wiele innych.

### Dlaczego to jest wygodne?

LangGraph pozwala budowaÄ‡ **przejrzystÄ…, odtwarzalnÄ… i rozszerzalnÄ… logikÄ™**:

*   Å‚atwe do debugowania;
*   Å‚atwe do wizualizacji;
*   Å‚atwe do skalowania do nowych zadaÅ„;
*   Å‚atwe do integracji narzÄ™dzi zewnÄ™trznych i protokoÅ‚Ã³w MCP.

Zasadniczo LangGraph to **â€mÃ³zgâ€ agenta**, gdzie kaÅ¼dy krok jest udokumentowany, kontrolowany i moÅ¼e byÄ‡ zmieniony bez chaosu i â€magiiâ€.

### No dobrze, wystarczy teorii!

MoÅ¼na jeszcze dÅ‚ugo opowiadaÄ‡ o grafach, stanach, kompozycji logiki i zaletach LangGraph nad klasycznymi potokami. Ale, jak pokazuje praktyka, lepiej raz zobaczyÄ‡ w kodzie.

**Czas przejÅ›Ä‡ do praktyki.** Dalej â€“ przykÅ‚ad w Pythonie: stworzymy prostego, ale uÅ¼ytecznego agenta AI opartego na LangGraph, ktÃ³ry bÄ™dzie uÅ¼ywaÅ‚ narzÄ™dzi zewnÄ™trznych, pamiÄ™ci i sam podejmowaÅ‚ decyzje.

### Przygotowanie: sieci neuronowe w chmurze i lokalne

Aby rozpoczÄ…Ä‡ tworzenie agentÃ³w AI, potrzebujemy przede wszystkim **mÃ³zgu** â€“ modelu jÄ™zykowego. Tutaj sÄ… dwa podejÅ›cia:

*   **uÅ¼ywaÄ‡ rozwiÄ…zaÅ„ chmurowych**, gdzie wszystko jest gotowe â€od razuâ€;
*   lub **podnieÅ›Ä‡ model lokalnie** â€“ dla peÅ‚nej autonomii i poufnoÅ›ci.

RozwaÅ¼my obie opcje.

#### UsÅ‚ugi chmurowe: szybko i wygodnie

Najprostszym sposobem jest wykorzystanie mocy duÅ¼ych dostawcÃ³w: OpenAI, Anthropic i uÅ¼ycie...

### Gdzie zdobyÄ‡ klucze i tokeny:

*   **OpenAI** â€“ ChatGPT i inne produkty;
*   **Anthropic** â€“ Claude;
*   **OpenRouter.ai** â€“ dziesiÄ…tki modeli (jeden token â€“ wiele modeli przez API kompatybilne z OpenAI);
*   **Amvera Cloud** â€“ moÅ¼liwoÅ›Ä‡ podÅ‚Ä…czenia LLAMA z pÅ‚atnoÅ›ciÄ… w rublach i wbudowanym proxy do OpenAI i Anthropic.

Ta Å›cieÅ¼ka jest wygodna, zwÅ‚aszcza jeÅ›li:

*   nie chcesz konfigurowaÄ‡ infrastruktury;
*   rozwijasz z naciskiem na szybkoÅ›Ä‡;
*   pracujesz z ograniczonymi zasobami.

### Modele lokalne: peÅ‚na kontrola

JeÅ›li waÅ¼na jest dla Ciebie **prywatnoÅ›Ä‡, praca offline** lub chcesz budowaÄ‡ **w peÅ‚ni autonomiczne agenty**, to warto wdroÅ¼yÄ‡ sieÄ‡ neuronowÄ… lokalnie.

**GÅ‚Ã³wne zalety:**

*   **PoufnoÅ›Ä‡** â€“ dane pozostajÄ… u Ciebie;
*   **Praca offline** â€“ przydatne w izolowanych sieciach;
*   **Brak subskrypcji i tokenÃ³w** â€“ bezpÅ‚atnie po konfiguracji.

**Wady sÄ… oczywiste:**

*   Wymagania dotyczÄ…ce zasobÃ³w (zwÅ‚aszcza pamiÄ™ci wideo);
*   Konfiguracja moÅ¼e zajÄ…Ä‡ trochÄ™ czasu;
*   NiektÃ³re modele sÄ… trudne do wdroÅ¼enia bez doÅ›wiadczenia.

Niemniej jednak istniejÄ… narzÄ™dzia, ktÃ³re uÅ‚atwiajÄ… lokalne uruchamianie. Jednym z najlepszych obecnie jest **Ollama**.

### WdraÅ¼anie lokalnego LLM przez Ollama + Docker

Przygotujemy lokalne uruchomienie modelu Qwen 2.5 (qwen2.5:32b) za pomocÄ… kontenera Docker i systemu Ollama. Pozwoli to na integracjÄ™ sieci neuronowej z MCP i wykorzystanie jej we wÅ‚asnych agentach opartych na LangGraph.

JeÅ›li zasoby obliczeniowe Twojego komputera lub serwera okaÅ¼Ä… siÄ™ niewystarczajÄ…ce do pracy z tÄ… wersjÄ… modelu, zawsze moÅ¼esz wybraÄ‡ mniej â€zasoboÅ¼ernÄ…â€ sieÄ‡ neuronowÄ… â€“ proces instalacji i uruchamiania pozostanie podobny.

**Szybka instalacja (podsumowanie krokÃ³w)**

1.  **Zainstaluj Docker + Docker Compose**
2.  **UtwÃ³rz strukturÄ™ projektu:**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **UtwÃ³rz `docker-compose.yml`**
(opcja uniwersalna, GPU jest wykrywane automatycznie)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **Uruchom kontener:**
```bash
docker compose up -d
```

5.  **Pobierz model:**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **SprawdÅº dziaÅ‚anie przez API:**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Witaj!", "stream": false}'
```
*(Obraz z wynikiem wykonania polecenia curl)*

7.  **Integracja z Pythonem:**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("WyjaÅ›nij splÄ…tanie kwantowe"))
```
Teraz masz peÅ‚noprawnego lokalnego LLM, gotowego do pracy z MCP i LangGraph.

**Co dalej?**

MoÅ¼emy wybieraÄ‡ miÄ™dzy modelami chmurowymi a lokalnymi, a takÅ¼e nauczyliÅ›my siÄ™, jak podÅ‚Ä…czyÄ‡ oba. Najciekawsze przed nami â€“ **tworzenie agentÃ³w AI na LangGraph**, ktÃ³rzy wykorzystujÄ… wybrany model, pamiÄ™Ä‡, narzÄ™dzia i wÅ‚asnÄ… logikÄ™.

**PrzejdÅºmy do najsmaczniejszej czÄ™Å›ci â€“ kodu i praktyki!**

---

Zanim przejdziemy do praktyki, waÅ¼ne jest, aby przygotowaÄ‡ Å›rodowisko pracy. ZakÅ‚adam, Å¼e znasz juÅ¼ podstawy Pythona, wiesz, czym sÄ… biblioteki i zaleÅ¼noÅ›ci, oraz rozumiesz, dlaczego warto uÅ¼ywaÄ‡ Å›rodowiska wirtualnego.

JeÅ›li to wszystko jest dla Ciebie nowoÅ›ciÄ… â€“ polecam najpierw przejÅ›Ä‡ krÃ³tki kurs lub przewodnik po podstawach Pythona, a nastÄ™pnie wrÃ³ciÄ‡ do artykuÅ‚u.

#### Krok 1: Tworzenie Å›rodowiska wirtualnego

UtwÃ³rz nowe Å›rodowisko wirtualne w folderze projektu:
```bash
python -m venv venv
source venv/bin/activate  # dla Linuxa/macOS
venv\Scripts\activate   # dla Windows
```

#### Krok 2: Instalacja zaleÅ¼noÅ›ci

UtwÃ³rz plik `requirements.txt` i dodaj do niego nastÄ™pujÄ…ce wiersze:
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> âš ï¸ **Aktualne wersje sÄ… podane na dzieÅ„ 21 lipca 2025 r.** MogÅ‚y siÄ™ zmieniÄ‡ od czasu publikacji â€“ **sprawdÅº aktualnoÅ›Ä‡ przed instalacjÄ….**

NastÄ™pnie zainstaluj zaleÅ¼noÅ›ci:
```bash
pip install -r requirements.txt```

#### Krok 3: Konfiguracja zmiennych Å›rodowiskowych

UtwÃ³rz w katalogu gÅ‚Ã³wnym projektu plik `.env` i dodaj do niego potrzebne klucze API:
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Przeznaczenie zmiennych:**

*   **OPENAI_API_KEY** â€“ klucz do dostÄ™pu do modeli GPT od OpenAI;
*   **DEEPSEEK_API_KEY** â€“ klucz do uÅ¼ywania modeli Deepseek;
*   **OPENROUTER_API_KEY** â€“ pojedynczy klucz do dostÄ™pu do wielu modeli przez OpenRouter

---
NiektÃ³re narzÄ™dzia MCP (np. `brave-web-search`) wymagajÄ… klucza do dziaÅ‚ania. Bez niego po prostu siÄ™ nie aktywujÄ….

**A jeÅ›li nie masz kluczy API?**

Nie ma problemu. MoÅ¼esz rozpoczÄ…Ä‡ rozwÃ³j z modelem lokalnym (np. przez Ollama), bez podÅ‚Ä…czania Å¼adnych zewnÄ™trznych usÅ‚ug. W takim przypadku plik `.env` moÅ¼na w ogÃ³le nie tworzyÄ‡.

Gotowe! Teraz mamy wszystko, czego potrzebujemy, aby zaczÄ…Ä‡ â€“ izolowane Å›rodowisko, zaleÅ¼noÅ›ci i, w razie potrzeby, dostÄ™p do chmurowych sieci neuronowych i integracji MCP.

NastÄ™pnie â€“ uruchomimy naszego agenta LLM na rÃ³Å¼ne sposoby.

### Proste uruchamianie agentÃ³w LLM przez LangGraph: podstawowa integracja

Zacznijmy od najprostszego: jak â€podÅ‚Ä…czyÄ‡ mÃ³zgâ€ do przyszÅ‚ego agenta. Przeanalizujemy podstawowe sposoby uruchamiania modeli jÄ™zykowych (LLM) za pomocÄ… LangChain, aby w nastÄ™pnym kroku przejÅ›Ä‡ do integracji z LangGraph i budowania peÅ‚noprawnego agenta AI.

#### Importy
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` i `load_dotenv()` â€“ do Å‚adowania zmiennych z pliku `.env`.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` â€“ opakowania do Å‚Ä…czenia modeli jÄ™zykowych przez LangChain.

> ğŸ’¡ JeÅ›li uÅ¼ywasz alternatywnych podejÅ›Ä‡ do pracy z konfiguracjami (np. Pydantic Settings), moÅ¼esz zastÄ…piÄ‡ `load_dotenv()` swojÄ… zwykÅ‚Ä… metodÄ….

#### Åadowanie zmiennych Å›rodowiskowych
```python
load_dotenv()
```
Spowoduje to zaÅ‚adowanie wszystkich zmiennych z `.env`, w tym kluczy dostÄ™pu do OpenAI, DeepSeek, OpenRouter i innych API.

#### Proste funkcje do pobierania LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
JeÅ›li zmienna `OPENAI_API_KEY` jest poprawnie ustawiona, LangChain automatycznie jÄ… podstawi â€“ jawne okreÅ›lenie `api_key=...` jest tutaj opcjonalne.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
Podobnie, ale uÅ¼ywajÄ…c opakowania `ChatDeepSeek`.

**OpenRouter (i inne kompatybilne API)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Cechy:**

*   `ChatOpenAI` jest uÅ¼ywany, mimo Å¼e model nie pochodzi z OpenAI â€“ poniewaÅ¼ OpenRouter uÅ¼ywa tego samego protokoÅ‚u.
*   `base_url` jest obowiÄ…zkowy: wskazuje na API OpenRouter.
*   Model `moonshotai/kimi-k2:free` zostaÅ‚ wybrany jako jedna z najbardziej zrÃ³wnowaÅ¼onych opcji pod wzglÄ™dem jakoÅ›ci i szybkoÅ›ci w momencie pisania.
*   Klucz API `OpenRouter` musi byÄ‡ przekazany jawnie â€“ automatyczne podstawienie tutaj nie dziaÅ‚a.

#### Mini-test: sprawdzanie dziaÅ‚ania modelu
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Kim jesteÅ›?")
    print(response.content)
```
*(Obraz z wynikiem wykonania: `Jestem asystentem AI stworzonym przez firmÄ™ Moonshot AI...`)*

JeÅ›li wszystko jest poprawnie skonfigurowane, otrzymasz sensownÄ… odpowiedÅº od modelu. Gratulacje â€“ pierwszy krok zostaÅ‚ wykonany!

### Ale to jeszcze nie agent

Na obecnym etapie podÅ‚Ä…czyliÅ›my LLM i wykonaliÅ›my proste wywoÅ‚anie. To bardziej przypomina chatbota konsolowego niÅ¼ agenta AI.

**Dlaczego?**

*   Pisujemy **synchroniczny, liniowy kod** bez logiki stanu ani celÃ³w.
*   Agent nie podejmuje decyzji, nie pamiÄ™ta kontekstu i nie uÅ¼ywa narzÄ™dzi.
*   MCP i LangGraph nie sÄ… jeszcze zaangaÅ¼owane.

**Co dalej?**

NastÄ™pnie zaimplementujemy **peÅ‚noprawnego agenta AI** za pomocÄ… **LangGraph** â€“ najpierw bez MCP, aby skupiÄ‡ siÄ™ na architekturze, stanach i logice samego agenta.

Zanurzmy siÄ™ w prawdziwÄ… mechanikÄ™ agentÃ³w. Zaczynamy!

### Agent klasyfikacji ofert pracy: od teorii do praktyki

...koncepcje LangGraph w praktyce i stworzyÄ‡ uÅ¼yteczne narzÄ™dzie dla platform HR i gieÅ‚d freelancerÃ³w.

#### Zadanie agenta

Nasz agent przyjmuje jako dane wejÅ›ciowe tekstowy opis oferty pracy lub usÅ‚ugi i wykonuje trÃ³jpoziomowÄ… klasyfikacjÄ™:

1.  **Typ pracy**: praca projektowa lub staÅ‚a oferta pracy
2.  **Kategoria zawodu**: z ponad 45 predefiniowanych specjalnoÅ›ci
3.  **Typ wyszukiwania**: czy osoba szuka pracy, czy szuka wykonawcy

Wynik jest zwracany w ustrukturyzowanym formacie JSON z ocenÄ… pewnoÅ›ci dla kaÅ¼dej klasyfikacji.

#### ğŸ“ˆ Architektura agenta na LangGraph

Zgodnie z zasadami LangGraph tworzymy **graf stanÃ³w** z czterech wÄ™zÅ‚Ã³w:

- Opis wejÅ›ciowy
- â†“
- WÄ™zeÅ‚ klasyfikacji typu pracy
- â†“
- WÄ™zeÅ‚ klasyfikacji kategorii
- â†“
- WÄ™zeÅ‚ okreÅ›lania typu wyszukiwania
- â†“
- WÄ™zeÅ‚ obliczania pewnoÅ›ci
- â†“
- Wynik JSON

KaÅ¼dy wÄ™zeÅ‚ to **wyspecjalizowana funkcja**, ktÃ³ra:

*   Odbiera bieÅ¼Ä…cy stan agenta
*   Wykonuje swojÄ… czÄ™Å›Ä‡ analizy
*   Aktualizuje stan i przekazuje go dalej

#### ZarzÄ…dzanie stanem

Definiujemy **strukturÄ™ pamiÄ™ci agenta** za pomocÄ… `TypedDict`:

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """Stan agenta do przechowywania informacji o procesie klasyfikacji"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

To jest **pamiÄ™Ä‡ robocza agenta** â€“ wszystko, co pamiÄ™ta i gromadzi podczas analizy. Podobnie jak ekspert ludzki, ktÃ³ry podczas analizy dokumentu ma na uwadze kontekst zadania.

Przyjrzyjmy siÄ™ peÅ‚nemu kodowi, a nastÄ™pnie skupmy siÄ™ na gÅ‚Ã³wnych punktach.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Kategorie zawodÃ³w
CATEGORIES = [
    "Animator 2D", "Animator 3D", "Modelarz 3D",
    "Analityk biznesowy", "Deweloper Blockchain", ...
]

class JobType(Enum):
    PROJECT = "praca projektowa"
    PERMANENT = "praca staÅ‚a"

class SearchType(Enum):
    LOOKING_FOR_WORK = "szukanie pracy"
    LOOKING_FOR_PERFORMER = "szukanie wykonawcy"

class State(TypedDict):
    """Stan agenta do przechowywania informacji o procesie klasyfikacji"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Asynchroniczny agent do klasyfikacji ofert pracy i usÅ‚ug"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Inicjalizacja agenta"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Tworzy przepÅ‚yw pracy agenta oparty na LangGraph"""
        workflow = StateGraph(State)
        
        # Dodaj wÄ™zÅ‚y do grafu
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # Zdefiniuj sekwencjÄ™ wykonywania wÄ™zÅ‚Ã³w
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """WÄ™zeÅ‚ do okreÅ›lania typu pracy: projektowa lub staÅ‚a"""
        # ... (implementacja poniÅ¼ej)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """WÄ™zeÅ‚ do okreÅ›lania kategorii zawodu"""
        # ... (implementacja poniÅ¼ej)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """WÄ™zeÅ‚ do okreÅ›lania typu wyszukiwania"""
        # ... (implementacja poniÅ¼ej)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """WÄ™zeÅ‚ do obliczania poziomu pewnoÅ›ci w klasyfikacji"""
        # ... (implementacja poniÅ¼ej)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Znajduje najbliÅ¼szÄ… kategoriÄ™ z listy dostÄ™pnych"""
        # ... (implementacja poniÅ¼ej)

    async def classify(self, description: str) -> Dict[str, Any]:
        """GÅ‚Ã³wna metoda do klasyfikacji ofert pracy/usÅ‚ug"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Uruchom przepÅ‚yw pracy
        result = await self.workflow.ainvoke(initial_state)
        
        # SformuÅ‚uj koÅ„cowÄ… odpowiedÅº JSON
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Demonstracja dziaÅ‚ania agenta"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "Wymagany programista Python do tworzenia aplikacji webowej w Django. Praca staÅ‚a.",
        "Szukam zleceÅ„ na tworzenie logo i identyfikacji wizualnej. PracujÄ™ w Adobe Illustrator.",
        "Potrzebny animator 3D do krÃ³tkoterminowego projektu tworzenia reklamy.",
        "CV: doÅ›wiadczony marketer, szukam pracy zdalnej w dziedzinie marketingu cyfrowego",
        "Szukamy programisty frontend React do naszego zespoÅ‚u na staÅ‚e"
    ]
    
    print("ğŸ¤– Demonstracja dziaÅ‚ania agenta klasyfikacji ofert pracy\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Test {i}: ---")
        print(f"Opis: {description}")
        try:
            result = await agent.classify(description)
            print("Wynik klasyfikacji:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...reszta kodu z implementacjÄ… metod zostaÅ‚a przedstawiona w artykule...)*

### Kluczowe zalety architektury
1.  **ModuÅ‚owoÅ›Ä‡** â€“ kaÅ¼dy wÄ™zeÅ‚ rozwiÄ…zuje jedno zadanie, Å‚atwe do testowania i ulepszania oddzielnie
2.  **RozszerzalnoÅ›Ä‡** â€“ nowe funkcje sÄ… dodawane deklaratywnie
3.  **PrzejrzystoÅ›Ä‡** â€“ caÅ‚y proces podejmowania decyzji jest udokumentowany i moÅ¼liwy do Å›ledzenia
4.  **WydajnoÅ›Ä‡** â€“ asynchroniczne przetwarzanie wielu Å¼Ä…daÅ„
5.  **NiezawodnoÅ›Ä‡** â€“ wbudowana obsÅ‚uga bÅ‚Ä™dÃ³w i odzyskiwanie

### Rzeczywiste korzyÅ›ci
Taki agent moÅ¼e byÄ‡ uÅ¼ywany w:
*   **Platformach HR** do automatycznej kategoryzacji CV i ofert pracy
*   **GieÅ‚dach freelancerÃ³w** do poprawy wyszukiwania i rekomendacji
*   **Systemach wewnÄ™trznych** firm do przetwarzania wnioskÃ³w i projektÃ³w
*   **RozwiÄ…zaniach analitycznych** do badania rynku pracy

### MCP w akcji: tworzenie agenta z systemem plikÃ³w i wyszukiwaniem w sieci
Po tym, jak omÃ³wiliÅ›my podstawowe zasady LangGraph i stworzyliÅ›my prostego agenta klasyfikujÄ…cego, rozszerzmy jego moÅ¼liwoÅ›ci, Å‚Ä…czÄ…c go ze Å›wiatem zewnÄ™trznym za poÅ›rednictwem MCP.

Teraz stworzymy peÅ‚noprawnego asystenta AI, ktÃ³ry bÄ™dzie mÃ³gÅ‚:
*   PracowaÄ‡ z systemem plikÃ³w (czytaÄ‡, tworzyÄ‡, modyfikowaÄ‡ pliki)
*   WyszukiwaÄ‡ aktualne informacje w Internecie
*   ZapamiÄ™tywaÄ‡ kontekst dialogu
*   ObsÅ‚ugiwaÄ‡ bÅ‚Ä™dy i odzyskiwaÄ‡ po awariach

#### Od teorii do prawdziwych narzÄ™dzi
PamiÄ™tasz, jak na poczÄ…tku artykuÅ‚u mÃ³wiliÅ›my o tym, Å¼e **MCP jest mostem miÄ™dzy sieciÄ… neuronowÄ… a jej otoczeniem**? Teraz zobaczysz to w praktyce. Nasz agent uzyska dostÄ™p do **prawdziwych narzÄ™dzi**:
```
# NarzÄ™dzia systemu plikÃ³w
- read_file â€“ czytanie plikÃ³w
- write_file â€“ zapisywanie i tworzenie plikÃ³w
- list_directory â€“ przeglÄ…danie zawartoÅ›ci folderÃ³w
- create_directory â€“ tworzenie folderÃ³w

# NarzÄ™dzia wyszukiwania w sieci
- brave_web_search â€“ wyszukiwanie w Internecie
- get_web_content â€“ pobieranie zawartoÅ›ci stron
```
To juÅ¼ nie jest agent â€zabawkaâ€ â€“ to **narzÄ™dzie pracy**, ktÃ³re moÅ¼e rozwiÄ…zywaÄ‡ prawdziwe problemy.

#### ğŸ“ˆ Architektura: od prostego do zÅ‚oÅ¼onego

**1. Konfiguracja jako podstawa stabilnoÅ›ci**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Uproszczona konfiguracja agenta AI"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Walidacja konfiguracji"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"ÅšcieÅ¼ka nie istnieje: {self.filesystem_path}")
```
**Dlaczego to jest waÅ¼ne?** W przeciwieÅ„stwie do przykÅ‚adu klasyfikacji, tutaj agent wchodzi w interakcje z systemami zewnÄ™trznymi. Jeden bÅ‚Ä…d w Å›cieÅ¼ce pliku lub brakujÄ…cy klucz API â€“ i caÅ‚y agent przestaje dziaÅ‚aÄ‡. **Walidacja na starcie** oszczÄ™dza godziny debugowania.

**2. Fabryka modeli: elastycznoÅ›Ä‡ wyboru**
```python
def create_model(config: AgentConfig):
    """Tworzy model zgodnie z konfiguracjÄ…"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... inni dostawcy
```
Jeden kod â€“ wiele modeli. Chcesz darmowy model lokalny? UÅ¼yj Ollamy. Potrzebujesz maksymalnej dokÅ‚adnoÅ›ci? PrzeÅ‚Ä…cz siÄ™ na GPT-4. WaÅ¼na jest szybkoÅ›Ä‡? WyprÃ³buj DeepSeek. Kod pozostaje ten sam.

**3. Integracja MCP: poÅ‚Ä…czenie z prawdziwym Å›wiatem**
```python
async def _init_mcp_client(self):
    """Inicjalizacja klienta MCP"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
Tutaj odbywa siÄ™ kluczowa praca MCP: podÅ‚Ä…czamy zewnÄ™trzne serwery MCP do agenta, ktÃ³re zapewniajÄ… zestaw narzÄ™dzi i funkcji. Agent z kolei otrzymuje nie tylko pojedyncze funkcje, ale peÅ‚ne kontekstowe zrozumienie, jak pracowaÄ‡ z systemem plikÃ³w i Internetem.

#### OdpornoÅ›Ä‡ na bÅ‚Ä™dy
W prawdziwym Å›wiecie wszystko siÄ™ psuje: sieÄ‡ jest niedostÄ™pna, pliki sÄ… zablokowane, klucze API wygasÅ‚y. Nasz agent jest na to gotowy:
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
Dekorator `@retry_on_failure` automatycznie ponawia operacje w przypadku tymczasowych awarii. UÅ¼ytkownik nawet nie zauwaÅ¼y, Å¼e coÅ› poszÅ‚o nie tak.

### Podsumowanie: od teorii do praktyki agentÃ³w AI

DziÅ› przeszliÅ›my drogÄ™ od podstawowych koncepcji do tworzenia dziaÅ‚ajÄ…cych agentÃ³w AI. Podsumujmy, czego siÄ™ nauczyliÅ›my i co osiÄ…gnÄ™liÅ›my.

**Co opanowaliÅ›my**

**1. Podstawowe koncepcje**
*   ZrozumieliÅ›my rÃ³Å¼nicÄ™ miÄ™dzy chatbotami a prawdziwymi agentami AI
*   ZrozumieliÅ›my rolÄ™ **MCP (Model Context Protocol)** jako mostu miÄ™dzy modelem a Å›wiatem zewnÄ™trznym
*   PoznaliÅ›my architekturÄ™ **LangGraph** do budowania zÅ‚oÅ¼onej logiki agentÃ³w

**2. UmiejÄ™tnoÅ›ci praktyczne**
*   SkonfigurowaliÅ›my Å›rodowisko pracy z obsÅ‚ugÄ… modeli chmurowych i lokalnych
*   StworzyliÅ›my **agenta klasyfikujÄ…cego** z asynchronicznÄ… architekturÄ… i zarzÄ…dzaniem stanem
*   ZbudowaliÅ›my **agenta MCP** z dostÄ™pem do systemu plikÃ³w i wyszukiwania w sieci

**3. Wzorce architektoniczne**
*   OpanowaliÅ›my moduÅ‚owÄ… konfiguracjÄ™ i fabryki modeli
*   WdroÅ¼yliÅ›my obsÅ‚ugÄ™ bÅ‚Ä™dÃ³w i **mechanizmy ponawiania prÃ³b** dla rozwiÄ…zaÅ„ gotowych do produkcji

### Kluczowe zalety podejÅ›cia
**LangGraph + MCP** dajÄ… nam:
*   **PrzejrzystoÅ›Ä‡** â€“ kaÅ¼dy krok agenta jest udokumentowany i moÅ¼liwy do Å›ledzenia
*   **RozszerzalnoÅ›Ä‡** â€“ nowe funkcje sÄ… dodawane deklaratywnie
*   **NiezawodnoÅ›Ä‡** â€“ wbudowana obsÅ‚uga bÅ‚Ä™dÃ³w i odzyskiwanie
*   **ElastycznoÅ›Ä‡** â€“ obsÅ‚uga wielu modeli i dostawcÃ³w od razu

### Wniosek

Agenty AI to nie futurystyczna fantazja, ale **prawdziwa technologia dzisiejszych czasÃ³w**. DziÄ™ki LangGraph i MCP moÅ¼emy tworzyÄ‡ systemy, ktÃ³re rozwiÄ…zujÄ… konkretne problemy biznesowe, automatyzujÄ… rutynÄ™ i otwierajÄ… nowe moÅ¼liwoÅ›ci.

**NajwaÅ¼niejsze â€“ zaczÄ…Ä‡.** WeÅº kod z przykÅ‚adÃ³w, dostosuj go do swoich zadaÅ„, eksperymentuj. KaÅ¼dy projekt to nowe doÅ›wiadczenie i krok w kierunku mistrzostwa w dziedzinie rozwoju AI.

Powodzenia w Twoich projektach!

---
*Tagi: python, ai, mcp, langchain, ai-assistant, ollama, ai-agents, local llm, langgraph, mcp-server*
*Huby: Blog firmy Amvera, Przetwarzanie jÄ™zyka naturalnego, Sztuczna inteligencja, Python, Programowanie*
