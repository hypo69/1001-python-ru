<h2>Ściągawka. Personalizacja LLM: prompty, dostrajanie modeli, przykłady kodu.</h2>
<p>W tym artykule:</p>
<ol>
<li>Jak powstaje "efekt pamięci" w LLM (krótki przegląd).</li>
<li>Dlaczego i kiedy potrzebne jest dostrajanie (Fine-tuning) modelu.</li>
<li>Kiedy dostrajanie nie jest najlepszym rozwiązaniem.</li>
<li>Przygotowanie danych.</li>
<li>Przykłady dostrajania dla <strong>OpenAI (GPT)</strong>, <strong>Google (Gemini)</strong> i <strong>Anthropic (Claude)</strong> (różni się).</li>
</ol>
<h3>1. Jak LLM "pamięta" i "dostosowuje się": Iluzja kontekstu</h3>
<p>Zanim przejdziemy do dostrajania, ważne jest, aby zrozumieć, jak LLM w ogóle udaje się stworzyć poczucie personalizacji.
Jest to ważne, aby nie rzucać się w kosztowne dostrajanie, jeśli zadanie można rozwiązać prostszymi metodami:</p>
<ul>
<li>Poprzez <strong>Okno kontekstowe (Pamięć krótkotrwała):</strong> W ramach jednego dialogu wysyłasz modelowi nie tylko nowe pytanie, ale także <strong>całą lub część poprzedniej korespondencji</strong>. Model przetwarza cały ten tekst jako jeden "kontekst". Dzięki temu "pamięta" poprzednie wypowiedzi i kontynuuje myśl. Ograniczeniem jest tutaj długość okna kontekstowego (liczba tokenów).</li>
<li>Tworzenie <strong>Instrukcji systemowych (System Prompt):</strong> Możesz ustawić rolę, ton i zasady zachowania modelu na początku każdego dialogu. Na przykład: "Jesteś ekspertem Pythona, odpowiadaj zwięźle".</li>
<li>Włączenie do zapytania kilku przykładów pożądanego zachowania <strong>Uczenie z kilku przykładów (Few-Shot Learning):</strong> (pary wejście/wyjście) pozwala modelowi "nauczyć się" tego wzorca bezpośrednio w ramach bieżącego zapytania.</li>
<li><strong>Zarządzanie stanem po stronie aplikacji:</strong> Najpotężniejszy sposób. Aplikacja (która odwołuje się do API) może przechowywać informacje o użytkowniku (preferencje, historię, dane profilu) i dynamicznie dodawać je do promptu przed wysłaniem do modelu.</li>
</ul>
<h3>2.</h3>
<p>Dostrajanie to proces dalszego szkolenia już przygotowanego bazowego LLM na własnym, specyficznym zestawie danych. Pozwala to modelowi na:</p>
<ul>
<li><strong>Dostosowanie stylu i tonu:</strong> Model będzie mówił "Twoim językiem" – czy to surowym naukowym, przyjaznym marketingowym, czy slangiem określonej społeczności.</li>
<li><strong>Przestrzeganie specyficznych instrukcji i formatów:</strong> Jeśli potrzebujesz odpowiedzi w ściśle określonej strukturze JSON lub zawsze z określonym zestawem pól.
</li>
<li><strong>Zrozumienie języka specyficznego dla domeny:</strong> Szkolenie na Twojej wewnętrznej dokumentacji lub tekstach branżowych pomoże modelowi lepiej radzić sobie z terminologią Twojej niszy.</li>
<li><strong>Poprawę wydajności w wąskich zadaniach:</strong> W przypadku niektórych typów zapytań (np. klasyfikacja sentymentu, generowanie kodu w określonym frameworku) dostrajanie może zapewnić dokładniejsze i bardziej trafne odpowiedzi niż model bazowy.</li>
<li><strong>Skrócenie długości promptów:</strong> Jeśli model "zna" już pożądane zachowanie dzięki dostrojeniu, nie musisz mu o tym przypominać za każdym razem w prompcie, co oszczędza tokeny i zmniejsza opóźnienia.</li>
</ul>
<h3>3.</h3>
<p>Dostrajanie to potężne, ale nie uniwersalne narzędzie. Nie należy go używać, jeśli:</p>
<ul>
<li><strong>Model musi mieć dostęp do nowej wiedzy:</strong> Dostrajanie zmienia wagi modelu, ale nie "ładuje" do niego nowych faktów w czasie rzeczywistym. Jeśli Twoim zadaniem jest odpowiadanie na pytania na podstawie stale zmieniającej się bazy wiedzy (dokumenty firmowe, najnowsze wiadomości), lepiej użyć <strong>Generacji Rozszerzonej o Pobieranie (RAG)</strong>. Tutaj model bazowy uzyskuje kontekst z Twojej bazy danych <em>podczas wykonywania zapytania</em>.</li>
<li><strong>Proste zadanie można rozwiązać za pomocą inżynierii promptów:</strong> Zawsze zaczynaj od najbardziej efektywnej inżynierii promptów. Jeśli zadanie można rozwiązać za pomocą prostych instrukcji i kilku przykładów, dostrajanie jest zbędne i bardziej kosztowne.</li>
<li><strong>Nie masz wystarczającej ilości wysokiej jakości danych:</strong> Złe dane = źle dostrojony model.</li>
</ul>
<h3>4. Przygotowanie danych.</h3>
<p>Jakość i ilość Twoich danych są kluczowe. Model uczy się na Twoich przykładach, dlatego muszą być one dokładne, różnorodne i spójne.</p>
<ul>
<li><strong>Format:</strong> Najczęściej JSON Lines (<code>.jsonl</code>) lub CSV (<code>.csv</code>).</li>
<li><strong>Struktura danych:</strong> Zależy od zadania.
<ul>
<li><strong>Dostrajanie instrukcji (Instruction Tuning - Instrukcja-Odpowiedź):</strong> Nadaje się do uogólnionych zadań, takich jak pytania-odpowiedzi, przeformułowywanie, podsumowywanie.
<pre class="line-numbers"><code class="language-json">{"input_text": "Przeformułuj zdanie: 'Technologia AI szybko się rozwija.'", "output_text": "Sztuczna inteligencja wykazuje szybki postęp."} 
{"input_text": "Podaj stolicę Francji.", "output_text": "Stolicą Francji jest Paryż."} 
</code></pre>
</li>
<li><strong>Dostrajanie czatu (Chat Tuning - Czat):</strong> Idealne do szkolenia modelu w prowadzeniu dialogu w określonej roli lub stylu.
<pre class="line-numbers"><code class="language-json">{"messages": [{"author": "user", "content": "Cześć! Co polecasz na obiad?"}, {"author": "model", "content": "Dobry wieczór! Dziś jest świetny dzień na makaron Carbonara, lub, jeśli wolisz coś lekkiego, sałatkę Cezar."}]} 
{"messages": [{"author": "user", "content": "Opowiedz mi o nowych funkcjach w Pythonie 3.12."}, {"author": "model", "content": "W Pythonie 3.12 pojawiły się f-stringi z wcięciami, nowa składnia dla uniwersalnych generyków i ulepszona obsługa błędów Unicode."} ]} 
</code></pre>
</li>
</ul>
</li>
<li><strong>Ilość:</strong> Minimum 100-200 wysokiej jakości przykładów, ale im więcej, tym lepiej (tysiące dla poważnych zadań).</li>
<li><strong>Jakość:</strong> Dokładność, spójność stylu, tonu i formatu w całym zestawie danych.</li>
</ul>
<h3>5. Modele i platformy</h3>
<h4>5.1. OpenAI</h4>
<p>OpenAI zapewnia dość prosty i intuicyjny interfejs API do dostrajania.</p>
<p><strong>Proces:</strong></p>
<ol>
<li>Przygotuj dane w formacie JSONL, gdzie każdy wiersz to obiekt z tablicą <code>messages</code> (jak w Chat Completions API).</li>
<li>Prześlij plik danych za pośrednictwem API.</li>
<li>Utwórz zadanie dostrajania, określając przesłany plik i model bazowy.</li>
<li>Monitoruj postęp.</li>
<li>Użyj nowego, dostrojonego modelu po jego ID.</li>
</ol>
<p><strong>Przykładowe dane (plik <code>training_data.jsonl</code>):</strong></p>
<pre class="line-numbers"><code class="language-json">{"messages": [{"role": "system", "content": "Jesteś przyjaznym asystentem wsparcia klienta firmy IT."}, {"role": "user", "content": "Mój internet nie działa."}, {"role": "assistant", "content": "Och, przepraszam za niedogodności! Spróbujmy to rozgryźć. Jak jesteś podłączony do internetu – przez Wi-Fi czy przez kabel?"}]} 
{"messages": [{"role": "system", "content": "Jesteś przyjaznym asystentem wsparcza klienta firmy IT."}, {"role": "user", "content": "Jak mogę zresetować hasło?"}, {"role": "assistant", "content": "Oczywiście! Aby zresetować hasło, przejdź na stronę logowania i kliknij 'Zapomniałeś hasła?' pod polem wprowadzania. Następnie postępuj zgodnie z instrukcjami na ekranie."} ]} 
</code></pre>
<p><strong>Przykładowy kod Python:</strong></p>
<p>Zainstaluj wcześniej: <code>pip install openai</code></p>
<pre class="line-numbers"><code class="language-python">import openai
from openai import OpenAI
import os

# Ustaw swój klucz API OpenAI. Zaleca się używanie zmiennej środowiskowej.
# os.environ["OPENAI_API_KEY"] = "sk-..."
client = OpenAI()

# 1. Przesyłanie pliku danych
try:
    file_response = client.files.create(
        file=open("training_data.jsonl", "rb"),
        purpose="fine-tune"
    )
    file_id = file_response.id
    print(f"Plik przesłany pomyślnie. ID pliku: {file_id}")
except openai.APIStatusError as e:
    print(f"Błąd przesyłania pliku: {e.status_code} - {e.response}")
    exit()

# 2. Tworzenie zadania dostrajania
try:
    ft_job_response = client.fine_tuning.jobs.create(
        training_file=file_id,
        model="gpt-3.5-turbo" # Możesz określić konkretną wersję, np. "gpt-3.5-turbo-0125"
    )
    job_id = ft_job_response.id
    print(f"Zadanie dostrajania utworzone. ID zadania: {job_id}")
    print("Monitoruj status zadania za pośrednictwem API lub w OpenAI Playground.")
except openai.APIStatusError as e:
    print(f"Błąd tworzenia zadania: {e.status_code} - {e.response}")
    exit()

# Przykład monitorowania statusu i pobierania nazwy modelu (uruchom po utworzeniu zadania):
# # job_id = "ftjob-..." # Zastąp swoim ID zadania
# # job_status = client.fine_tuning.jobs.retrieve(job_id)
# # print(f"Aktualny status zadania: {job_status.status}")
# # if job_status.status == "succeeded":
# #     fine_tuned_model_name = job_status.fine_tuned_model
# #     print(f"Nazwa dostrojonego modelu: {fine_tuned_model_name}")

# 3. Używanie dostrojonego modelu (po jego gotowości)
# # Zastąp rzeczywistą nazwą swojego modelu, uzyskaną po pomyślnym dostrojeniu
# # fine_tuned_model_name = "ft:gpt-3.5-turbo-0125:my-org::abcd123"

# # if 'fine_tuned_model_name' in locals() and fine_tuned_model_name:
# #     try:
# #         response = client.chat.completions.create(
# #             model=fine_tuned_model_name,
# #             messages=[
# #                 {"role": "user", "content": "Mam problem z logowaniem."}
# #             ]
# #         )
# #         print("\nOdpowiedź dostrojonego modelu:")
# #         print(response.choices[0].message.content)
# #     except openai.APIStatusError as e:
# #         print(f"Błąd podczas używania modelu: {e.status_code} - {e.response}")
</code></pre>
<h4>5.2. Anthropic</h4>
<p>Anthropic <strong>nie udostępnia publicznego API do dostrajania swoich modeli Claude 3 (Opus, Sonnet, Haiku) w tym samym sensie, co OpenAI czy Google.</strong></p>
<p>Anthropic koncentruje się na tworzeniu bardzo potężnych modeli bazowych, które, jak twierdzą, doskonale współpracują z zaawansowaną inżynierią promptów i wzorcami RAG, minimalizując potrzebę dostrajania w większości przypadków.
For large enterprise clients or partners, there may be programs for creating "custom" models or specialized integrations, but this is not a publicly available fine-tuning function via API.</p>
<p>If you are working with Claude 3, your primary focus should be on:</p>
<ul>
<li><strong>Wysokiej jakości inżynieria promptów:</strong> Eksperymentuj z instrukcjami systemowymi, przykładami few-shot, wyraźnym formatowaniem zapytań. Claude jest znany z umiejętności ścisłego przestrzegania instrukcji, zwłaszcza w tagach XML.</li>
<li><strong>Systemy RAG:</strong> Używaj zewnętrznych baz wiedzy, aby dostarczyć modelowi odpowiedni kontekst.</li>
</ul>
<h4>5.3. Google (Gemini)</h4>
<p>Google aktywnie rozwija możliwości dostrajania za pośrednictwem swojej platformy <strong>Google Cloud Vertex AI</strong>.
Jest to pełnoprawna platforma ML, która zapewnia narzędzia do przygotowywania danych, uruchamiania zadań szkoleniowych i wdrażania modeli.
    Dostrajanie jest dostępne dla modeli z rodziny Gemini.</p>
<p><strong>Proces:</strong></p>
<ol>
<li>Przygotuj dane (JSONL lub CSV) w formacie <code>input_text</code>/<code>output_text</code> (do dostrajania instrukcji) lub <code>messages</code> (do dostrajania czatu).</li>
<li>Prześlij dane do Google Cloud Storage (GCS).</li>
<li>Utwórz i uruchom zadanie dostrajania za pośrednictwem konsoli Vertex AI lub SDK.</li>
<li>Wdróż dostrojony model na punkcie końcowym (Endpoint).</li>
<li>Użyj dostrojonego modelu za pośrednictwem tego punktu końcowego.</li>
</ol>
<p><strong>Przykładowe dane (plik <code>gemini_tuning_data.jsonl</code>):</strong></p>
<pre class="line-numbers"><code class="language-json">{"input_text": "Podsumuj główne idee tej książki: 'Książka opowiada o podróży bohatera, który pokonuje przeszkody i odnajduje siebie.'", "output_text": "Główny bohater książki wyrusza w transformacyjną podróż, stawiając czoła wyzwaniom i osiągając samopoznanie."} 
{"input_text": "Wyjaśnij zasadę działania reaktora termojądrowego w prostych słowach.", "output_text": "Reaktor termojądrowy próbuje odtworzyć proces zachodzący na Słońcu: fuzję lekkich jąder atomowych w bardzo wysokich temperaturach, uwalniając ogromne ilości energii."} 
</code></pre>
<p><strong>Przykładowy kod Python (wymaga <code>google-cloud-aiplatform</code>):</strong></p>
<p>Zainstaluj wcześniej: <code>pip install google-cloud-aiplatform</code> i <code>pip install google-cloud-storage</code></p>
<pre class="line-numbers"><code class="language-python">import os
from google.cloud import aiplatform
from google.cloud import storage

# --- Ustawienia ---
# ZASTĄP swoimi wartościami:
PROJECT_ID = "your-gcp-project-id"
REGION = "us-central1"               # Wybierz region obsługujący Gemini i Vertex AI
BUCKET_NAME = "your-gcs-bucket-for-tuning" # Nazwa Twojego bucketa GCS (musi być utworzony wcześniej)
DATA_FILE_LOCAL_PATH = "gemini_tuning_data.jsonl"
GCS_DATA_URI = f"gs://{BUCKET_NAME}/{DATA_FILE_LOCAL_PATH}"
TUNED_MODEL_DISPLAY_NAME = "my-tuned-gemini-model"
# --- Koniec ustawień ---

# Inicjalizacja Vertex AI
aiplatform.init(project=PROJECT_ID, location=REGION)

# 1. Tworzenie pliku danych (jeśli nie istnieje)
with open(DATA_FILE_LOCAL_PATH, "w", encoding="utf-8") as f:
    f.write('{"input_text": "Podsumuj główne idee tej książki: \'Książka opowiada o podróży bohatera, który pokonuje przeszkody i odnajduje siebie.\'", "output_text": "Główny bohater książki wyrusza w transformacyjną podróż, stawiając czoła wyzwaniom i osiągając samopoznanie."}')
    f.write('{"input_text": "Wyjaśnij zasadę działania reaktora termojądrowego w prostych słowach.", "output_text": "Reaktor termojądrowy próbuje odtworzyć proces zachodzący na Słońcu: fuzję lekkich jąder atomowych w bardzo wysokich temperaturach, uwalniając ogromne ilości energii."}')
print(f"Plik danych '{DATA_FILE_LOCAL_PATH}' utworzony.")


# 2. Przesyłanie danych do Google Cloud Storage
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """Przesyła plik do bucketa GCS."""
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"Plik '{source_file_name}' przesłany do 'gs://{bucket_name}/{destination_blob_name}'.")

try:
    upload_blob(BUCKET_NAME, DATA_FILE_LOCAL_PATH, DATA_FILE_LOCAL_PATH)
except Exception as e:
    print(f"Błąd przesyłania pliku do GCS. Upewnij się, że bucket istnieje i masz uprawnienia: {e}")
    exit()

# 3. Tworzenie i uruchamianie zadania dostrajania
print(f"\nRozpoczynanie dostrajania modelu '{TUNED_MODEL_DISPLAY_NAME}'...")
try:
    # `tune_model` uruchamia zadanie i zwraca dostrojony model po zakończeniu
    tuned_model = aiplatform.Model.tune_model(
        model_display_name=TUNED_MODEL_DISPLAY_NAME,
        source_model_name="gemini-1.0-pro-001", # Bazowy model Gemini Pro
        training_data_path=GCS_DATA_URI,
        tuning_method="SUPERVISED_TUNING",
        train_steps=100, # Liczba kroków treningowych. Optymalna wartość zależy od rozmiaru danych.
        # batch_size=16, # Można określić
        # learning_rate_multiplier=1.0 # Można określić
    )
    print(f"Model '{TUNED_MODEL_DISPLAY_NAME}' dostrojony pomyślnie. ID modelu: {tuned_model.name}")
    print("Proces dostrajania może zająć sporo czasu.")
except Exception as e:
    print(f"Błąd dostrajania. Sprawdź logi w konsoli Vertex AI: {e}")
    exit()

# 4. Wdrażanie dostrojonego modelu (do użytku)
print(f"\nWdrażanie dostrojonego modelu '{TUNED_MODEL_DISPLAY_NAME}' na punkt końcowy...")
try:
    endpoint = tuned_model.deploy(
        machine_type="n1-standard-4", # Typ maszyny dla punktu końcowego. Wybierz odpowiedni.
        min_replica_count=1,
        max_replica_count=1
    )
    print(f"Model wdrożony na punkt końcowy: {endpoint.name}")
    print("Wdrażanie również może zająć kilka minut.")
except Exception as e:
    print(f"Błąd wdrażania modelu: {e}")
    exit()

# 5. Używanie dostrojonego modelu
print("\nTestowanie dostrojonego modelu...")
prompt = "Opowiedz mi o swoich możliwościach po treningu."
instances = [{"prompt": prompt}] # Do dostrajania instrukcji. Jeśli dostrajanie czatu, to {"messages": [...]} 

try:
    response = endpoint.predict(instances=instances)
    print("\nOdpowiedź dostrojonego modelu:")
    print(response.predictions[0])
except Exception as e:
    print(f"Błąd podczas używania dostrojonego modelu: {e}")

# Po zakończeniu pracy, nie zapomnij usunąć punktu końcowego i modelu, aby uniknąć niepotrzebnych kosztów:
# endpoint.delete()
# tuned_model.delete()
</code></pre>
<h3>6. Ogólne rekomendacje</h3>
<ul>
<li><strong>Zacznij od małego:</strong> Nie próbuj od razu szkolić modelu na tysiącach przykładów. Zacznij od małego, ale wysokiej jakości zestawu danych.</li>
<li><strong>Iteruj:</strong> Dostrajanie to proces iteracyjny. Szkol, oceniaj, dostosuj dane lub hiperparametry, powtarzaj.</li>
<li><strong>Monitorowanie:</strong> Dokładnie monitoruj metryki szkoleniowe (stratę) i używaj zestawu danych walidacyjnych, aby uniknąć przetrenowania.</li>
<li><strong>Ocena:</strong> Zawsze testuj dostrojony model na danych, których <em>nigdy nie widział</em> podczas szkolenia, aby ocenić jego zdolność do generalizacji.</li>
<li><strong>Koszt:</strong> Pamiętaj, że dostrajanie i wdrażanie punktów końcowych są płatne. Uwzględnij to w swoim budżecie.</li>
<li><strong>Dokumentacja:</strong> Zawsze odwołuj się do oficjalnej dokumentacji dostawcy LLM. Interfejsy API i możliwości stale ewoluują.</li>
</ul>
