# Fine Tuning

Файн-тюнинг стоит делать не "как можно чаще", а **когда это действительно необходимо и оправдано** с точки зрения наличия новых данных, ожидаемого улучшения качества и затрат ресурсов (времени и денег). Вместо частого файн-тюнинга на малых объемах данных иногда эффективнее использовать другие подходы, например, Retrieval-Augmented Generation (RAG), где модель получает актуальную информацию в момент запроса из внешней базы знаний.
Технически, **нет строгих ограничений со стороны платформ** (как Google Vertex AI, например) на то, как часто вы можете запускать процесс файн-тюнинга (fine-tuning), кроме общих квот на использование ресурсов и вашего бюджета. Вы можете запустить новый процесс файн-тюнинга хоть через час после предыдущего, если есть такая необходимость и ресурсы.

Однако, **с практической точки зрения**, частота файн-тюнинга определяется следующими факторами:

1.  **Наличие новых данных:** Самая главная причина для повторного файн-тюнинга – появление **значительного объема новых, качественных данных**, которые могут улучшить производительность модели для вашей задачи. Файн-тюнинг на небольшом количестве новых данных может не дать заметного эффекта или даже ухудшить модель (переобучение на новых данных).
2.  **Стоимость:** Каждый процесс файн-тюнинга требует вычислительных ресурсов (GPU/TPU) и, соответственно, стоит денег. Частый файн-тюнинг может быть весьма затратным.
3.  **Время:** Процесс файн-тюнинга занимает время – от часов до дней, в зависимости от размера модели, объема данных и конфигурации. Плюс время на подготовку данных и оценку результата.
4.  **Необходимость:** Действительно ли нужно обновлять модель?
    *   Изменилась ли задача, для которой вы используете модель?
    *   Ухудшилось ли качество ответов существующей дообученной модели со временем (например, из-за изменений во внешнем мире, которые отражены в новых данных)?
    *   Даст ли добавление новых данных ощутимый прирост качества, оправдывающий затраты?
5.  **Оценка (Evaluation):** После каждого файн-тюнинга необходимо проводить тщательную оценку модели на тестовом наборе данных, чтобы убедиться, что новая версия действительно лучше предыдущей и не произошло "катастрофического забывания" (когда модель ухудшает свои способности в областях, на которых ее не дообучали в этот раз).

**Когда обычно делают повторный файн-тюнинг:**

*   При накоплении **существенного** блока новых релевантных данных (например, данные за последний квартал, месяц и т.д.).
*   При **значительных изменениях** в предметной области или требованиях к задаче.
*   Когда **мониторинг производительности** показывает заметное снижение качества текущей модели.
*   Периодически (например, раз в квартал или полгода), если данные поступают непрерывно и есть бюджет/ресурсы на поддержание модели в актуальном состоянии.

Структура файла для файн-тюнинга (fine-tuning) зависит от **платформы**, которую вы используете (например, Google Vertex AI, OpenAI API, Hugging Face, etc.) и **типа задачи**, для которой вы дообучаете модель.

Однако, наиболее распространенным форматом является **JSON Lines (JSONL)**. В этом формате каждая строка файла представляет собой отдельный, валидный JSON-объект, содержащий один пример для обучения.

**Общая идея структуры для генеративных моделей (как Gemini):**

Каждый JSON-объект (каждая строка в файле .jsonl) обычно содержит пары "вход" -> "ожидаемый выход".

**Примеры форматов данных для файн-тюнинга:**

**1. Задачи инструктивного типа / чата / вопрос-ответ:**

* **Вариант 1 (Простой):**

```jsonl
{"prompt": "Переведи на английский: Как дела?", "completion": "How are you?"}
{"prompt": "Напиши короткое стихотворение о весне.", "completion": "Звенит капель, ручьи бегут,\nПодснежник первый тут как тут.\nПроснулся лес, проснулся луг,\nВесна пришла, мой милый друг!"}
{"prompt": "Столица Франции?", "completion": "Париж"}
{"instruction": "Напиши код на Python для вычисления факториала числа 5.", "output_text": "def factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\nprint(factorial(5))"}
{"instruction": "Суммируй числа от 1 до 10.", "output_text": "55"}
{"input_text": "Объясни концепцию квантовой запутанности простым языком.", "output_text": "Представь две монеты, которые каким-то образом связаны. Если одна выпадает орлом, другая автоматически выпадает решкой, независимо от расстояния между ними."}
```

* **Вариант 2 (Более структурированный, для чатов):**

```jsonl
{"messages": [{"role": "user", "content": "Привет! Как дела?"}, {"role": "assistant", "content": "Привет! У меня всё отлично. Чем могу помочь?"}]}
{"messages": [{"role": "user", "content": "Объясни теорию относительности простыми словами."}, {"role": "assistant", "content": "Представь, что время и пространство - это как эластичная ткань..."}]}
{"messages": [{"role": "user", "content": "Расскажи анекдот про программистов."}, {"role": "assistant": "Почему программисты путают Рождество и Хэллоуин? Потому что Dec 25 == Oct 31"}]}
{"messages":[{"role":"user","content":"Какой сегодня день?"},{"role":"assistant","content":"Сегодня [дата]"} ]}
```

**2. Задачи генерации текста (продолжение текста):**

* **Вариант 1 (Объединенный):**

```jsonl
{"text": "Однажды зимним вечером, когда снег падал крупными хлопьями, и ветер выл, как голодный волк,  в маленьком домике на опушке леса сидела старушка и вязала.  Вдруг раздался стук в дверь…  Это был…"}
{"text": "Рецепт яблочного пирога. Нам понадобятся: 200 г муки, 150 г сахара, 2 яйца, 100 г сливочного масла, 3 средних яблока… Способ приготовления: 1. Разогреть духовку до 180 градусов. 2. Взбить яйца с сахаром…  Затем…"}
{"text": "В начале было Слово, и Слово было у Бога, и Слово было Бог. Оно было в начале у Бога. Все чрез Него начало быть, и без Него ничто не начало быть, что начало быть. В Нем была жизнь, и жизнь была свет человеков... "}
```

* **Вариант 2 (Префикс/Продолжение):**

```jsonl
{"prompt": "Рецепт борща. Ингредиенты:", "completion": " свекла, капуста, картофель, морковь, лук, томатная паста..."}
{"prompt": "The quick brown fox jumps over the lazy dog. This sentence is a pangram because it ", "completion": "contains every letter of the alphabet."}
{"prompt":"Once upon a time, in a land far, far away, there lived a princess who...", "completion": "was known for her kindness and her love of adventure."}
```

**3. Задачи классификации:**

```jsonl
{"text": "Этот ресторан предлагает превосходную кухню и отличное обслуживание.", "label": "положительный"}
{"text": "К сожалению, заказ был доставлен с опозданием на два часа.", "label": "отрицательный"}
{"text": "Продукт соответствует описанию, но цена немного высоковата.", "label": "нейтральный"}
{"text": "Фильм скучный и неинтересный, я бы не рекомендовала его к просмотру.", "label": "отрицательный"}
{"text": "Книга написана хорошим языком и имеет увлекательный сюжет, рекомендую к прочтению.", "label": "положительный"}
{"text":"Этот отзыв очень полезен, спасибо автору!","label":"положительный"}
```

**Важные моменты:**

1.  **Обязательно смотрите документацию той платформы, где вы проводите файн-тюнинг!** Названия полей (`input_text`, `output_text`, `prompt`, `completion`, `text`, `label` и т.д.) могут отличаться. Например, Google Vertex AI часто использует `input_text` и `output_text` для supervised tuning.
2.  **Формат Файла:** Убедитесь, что это именно JSON Lines (.jsonl), а не обычный JSON-массив. Каждая строка - отдельный JSON.
3.  **Кодировка:** Обычно требуется UTF-8.
4.  **Самое важное – это качество и релевантность ваших данных**. Они должны быть чистыми, точными и соответствовать той задаче, которую вы хотите, чтобы модель решала.
5.  **Размер Данных:** Количество примеров влияет на качество файн-тюнинга. Обычно требуются сотни или тысячи примеров, в зависимости от сложности задачи и модели.

**Пример файла `my_finetuning_data.jsonl` для Google Vertex AI (инструктивный тюнинг):**

```jsonl
{"input_text": "Напиши слоган для кофейни 'Утренний Луч'", "output_text": "Утренний Луч: Начни свой день с идеальной чашки!"}
{"input_text": "Кратко перескажи сюжет 'Гамлета'", "output_text": "Датский принц Гамлет мстит своему дяде Клавдию за убийство отца и узурпацию трона, что приводит к трагической гибели большинства главных героев."}
{"input_text": "Составь список из 5 идей для летнего отпуска в России", "output_text": "1. Поход по Алтаю\n2. Отдых на Байкале\n3. Путешествие по Золотому Кольцу\n4. Пляжный отдых в Сочи или Крыму\n5. Сплав по рекам Карелии"}
```
