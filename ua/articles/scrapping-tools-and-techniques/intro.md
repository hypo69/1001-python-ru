### **Цикл «НЕ Selenium». Інтро**

Ті, хто займається веб-скрапінгом, тестуванням та автоматизацією, знайомі з Selenium, більш сучасним Playwright та/або фреймворком Crawlee. Вони потужні, вони можуть майже все, і вони... не завжди потрібні. Більше того, у багатьох випадках використання цих інструментів — забивання цвяхів мікроскопом: робота, звичайно, буде зроблена, але ціною невиправданих витрат — швидкості, системних ресурсів та складності налаштування.

Ласкаво просимо до циклу статей «НЕ Selenium». Тут я покажу інші способи (не завжди очевидні) взаємодії з вмістом інтернету.

#### Парадигма №1: Пряме спілкування. HTTP-клієнти

*   **`Requests`** — Формує та надсилає мережевий запит до цільової адреси (URL), точно так само, як це робить ваш браузер у найперший момент завантаження сторінки, але без самого браузера. У цей запит він упаковує метод (наприклад, `GET`, щоб отримати дані), заголовки (`Headers`), які представляються сайту (наприклад, `User-Agent: "я-браузер"`), та інші параметри. У відповідь від сервера він отримує сирі дані — найчастіше, це вихідний HTML-код сторінки або рядок у форматі JSON, а також код статусу (наприклад, `200 OK`).

*   **`HTTPX`** — це сучасний спадкоємець `Requests`. На фундаментальному рівні він робить все те саме: надсилає ті ж самі HTTP-запити з тими ж заголовками та отримує ті ж самі відповіді. Але є ключова відмінність: `Requests` працює **синхронно** — надіслав запит, сидить і чекає відповіді, отримав відповідь, надіслав наступний. `HTTPX` же вміє працювати **асинхронно** — він може "закинути" одразу сотню запитів, не чекаючи відповідей, і потім ефективно обробляти їх у міру надходження.

Відмінно підходять для збору даних зі статичних сайтів, роботи з API, парсингу тисяч сторінок, де не потрібне виконання JavaScript.

*   **Переваги:** **Швидкість та ефективність.** Завдяки асинхронності `HTTPX`, там, де `Requests` буде послідовно робити 100 запитів кілька хвилин, `HTTPX` впорається за кілька секунд.
*   **Недоліки:** Не підходять для сайтів, де контент генерується за допомогою JavaScript.

#### Парадигма №2: Chrome DevTools Protocol (CDP)

Що робити, якщо сайт динамічний і контент генерується за допомогою JavaScript? Сучасні браузери (Chrome, Chromium, Edge) мають вбудований протокол для налагодження та керування — **Chrome DevTools Protocol (CDP)**. Він дозволяє віддавати команди браузеру безпосередньо, минаючи громіздкий прошарок у вигляді WebDriver, який використовує Selenium.

*   **Інструменти:** Основним представником цього підходу сьогодні є `Pydoll`, який прийшов на зміну колись популярному, але нині не підтримуваному `pyppeteer`.</li>
*   **Коли використовувати:** Коли потрібен рендеринг JavaScript, але хочеться зберегти високу швидкість та уникнути складнощів з драйверами.</li>
*   **Переваги:** **Баланс.** Ви отримуєте потужність справжнього браузера, але з набагато меншими накладними витратами і часто з вбудованими механізмами обходу захистів.</li>
*   **Недоліки:** Може бути складніше в налагодженні, ніж Playwright, і вимагає глибшого розуміння роботи браузера.</li>
</ul>
<h4>Парадигма №3: Автономні LLM-агенти</h4>
<p>Це найпередовіший рубіж. Що, якщо замість того, щоб писати код, який говорить "клікни сюди, введи це", ми просто дамо завдання природною мовою? "Знайди мені всіх постачальників на цьому сайті та збери їхні категорії товарів".</p>
<p>Саме це завдання вирішують LLM-агенти. Використовуючи "мозок" у вигляді великої мовної моделі (GPT, Gemini) та "руки" у вигляді набору інструментів (браузер, пошук Google), ці агенти можуть самостійно планувати та виконувати складні завдання в мережі.</p>
<ul>
<li><strong>Інструменти:</strong> Зв'язки на кшталт `LangChain` + `Pydoll` або кастомні рішення, як у `simple_browser.py`, який ми розберемо пізніше.</li>
<li><strong>Коли використовувати:</strong> Для складних дослідницьких завдань, де кроки заздалегідь невідомі та потрібна адаптація в реальному часі.</li>
<li><strong>Переваги:</strong> **Інтелект.** Здатність вирішувати неструктуровані завдання та адаптуватися до змін на льоту.</li>
<li><strong>Недоліки:</strong> "Недетермінованість" (результат може змінюватися від запуску до запуску), вартість API-викликів до LLM, нижча швидкість порівняно з прямим кодом.</li>
</ul>
<h4>Парадигма №4: Скрапінг без коду</h4>
<p>Іноді завдання настільки просте, що писати код — це надмірність. Потрібно швидко витягнути таблицю з однієї сторінки? Для цього існують елегантні рішення, які не вимагають програмування.</p>
<ul>
<li><strong>Інструменти:</strong> Функції Google Sheets (<code>IMPORTXML</code>, <code>IMPORTHTML</code>), розширення браузера.</li>
<li><strong>Коли використовувати:</strong> Для одноразових завдань, швидкого прототипування або коли ви просто не хочете писати код.</li>
<li><strong>Переваги:</strong> **Простота.** Відкрив, вказав, що потрібно зібрати, — отримав результат.</li>
<li><strong>Недоліки:</strong> Обмежена функціональність, не підходять для складних завдань або великих обсягів даних.</li>
</ul>
<h3>Що далі?</h3>
<p>Ця стаття — лише вступ. У наступних випусках нашого циклу «НЕ Selenium» ми перейдемо від теорії до жорсткої практики. Ми глибоко зануримося в кожну з цих парадигм і покажемо, як вони працюють на реальних прикладах:</p>
<ul>
<li>Розберемо <strong>Pydoll</strong> і подивимося, як він обходить Cloudflare.</li>
<li>Влаштуємо битву <strong>JavaScript проти Python</strong> за звання найкращої мови для веб-скрапінгу.</li>
<li>Навчимося вичавлювати максимум швидкості з парсингу за допомогою <strong>lxml</strong>.</li>
<li>Напишемо скрипт, який збирає дані з <strong>Amazon</strong> і зберігає їх у <strong>Excel</strong>.</li>
<li>Покажемо, як <strong>Google Sheets</strong> може стати вашим першим скрапером.</li>
<li>І, звичайно ж, детально розберемо, як створити та використовувати <strong>автономного LLM-агента</strong> для керування браузером.</li>
</ul>
<p>Приготуйтеся змінити свій погляд на автоматизацію та збір даних у мережі. Буде швидко, ефективно та дуже цікаво. Підписуйтесь</p>
