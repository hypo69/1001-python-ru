<h3><strong>Цикл «НЕ Selenium». Інтро</strong></h3>
<p>Ті, хто займається веб-скрапінгом, тестуванням та автоматизацією, знайомі з Selenium, більш сучасним Playwright та/або фреймворком Crawlee. Вони потужні, вони можуть майже все, і вони... не завжди потрібні. Більше того, у багатьох випадках використання цих інструментів — забивання цвяхів мікроскопом: робота, звичайно, буде зроблена, але ціною невиправданих витрат — швидкості, системних ресурсів та складності налаштування.</p>
<p>Ласкаво просимо до циклу статей «НЕ Selenium». Тут я покажу інші способи (не завжди очевидні) взаємодії з вмістом інтернету.</p>
<h4>Парадигма №1: Пряме спілкування. HTTP-клієнти</h4>
<ul>
<li><strong><code>Requests</code></strong> — Формує та надсилає мережевий запит до цільової адреси (URL), точно так само, як це робить ваш браузер у найперший момент завантаження сторінки, але без самого браузера. У цей запит він упаковує метод (наприклад, <code>GET</code>, щоб отримати дані), заголовки (<code>Headers</code>), які представляються сайту (наприклад, <code>User-Agent: &quot;я-браузер&quot;</code>), та інші параметри. У відповідь від сервера він отримує сирі дані — найчастіше, це вихідний HTML-код сторінки або рядок у форматі JSON, а також код статусу (наприклад, <code>200 OK</code>).</li>
<li><strong><code>HTTPX</code></strong> — це сучасний спадкоємець <code>Requests</code>. На фундаментальному рівні він робить все те саме: надсилає ті ж самі HTTP-запити з тими ж заголовками та отримує ті ж самі відповіді. Але є ключова відмінність: <code>Requests</code> працює <strong>синхронно</strong> — надіслав запит, сидить і чекає відповіді, отримав відповідь, надіслав наступний. <code>HTTPX</code> же вміє працювати <strong>асинхронно</strong> — він може &quot;закинути&quot; одразу сотню запитів, не чекаючи відповідей, і потім ефективно обробляти їх у міру надходження.</li>
</ul>
<p>Відмінно підходять для збору даних зі статичних сайтів, роботи з API, парсингу тисяч сторінок, де не потрібне виконання JavaScript.</p>
<ul>
<li><strong>Переваги:</strong> <strong>Швидкість та ефективність.</strong> Завдяки асинхронності <code>HTTPX</code>, там, де <code>Requests</code> буде послідовно робити 100 запитів кілька хвилин, <code>HTTPX</code> впорається за кілька секунд.</li>
<li><strong>Недоліки:</strong> Не підходять для сайтів, де контент генерується за допомогою JavaScript.</li>
</ul>
<h4>Парадигма №2: Chrome DevTools Protocol (CDP)</h4>
<p>Що робити, якщо сайт динамічний і контент генерується за допомогою JavaScript? Сучасні браузери (Chrome, Chromium, Edge) мають вбудований протокол для налагодження та керування — <strong>Chrome DevTools Protocol (CDP)</strong>. Він дозволяє віддавати команди браузеру безпосередньо, минаючи громіздкий прошарок у вигляді WebDriver, який використовує Selenium.</p>
<ul>
<li><strong>Інструменти:</strong> Основним представником цього підходу сьогодні є <code>Pydoll</code>, який прийшов на зміну колись популярному, але нині не підтримуваному <code>pyppeteer</code>.</li>
<li><strong>Коли використовувати:</strong> Коли потрібен рендеринг JavaScript, але хочеться зберегти високу швидкість та уникнути складнощів з драйверами.</li>
<li><strong>Переваги:</strong> <strong>Баланс.</strong> Ви отримуєте потужність справжнього браузера, але з набагато меншими накладними витратами і часто з вбудованими механізмами обходу захистів.</li>
<li><strong>Недоліки:</strong> Може бути складніше в налагодженні, ніж Playwright, і вимагає глибшого розуміння роботи браузера.</li>
</ul>
<h4>Парадигма №3: Автономні LLM-агенти</h4>
<p>Це найпередовіший рубіж. Що, якщо замість того, щоб писати код, який говорить &quot;клікни сюди, введи це&quot;, ми просто дамо завдання природною мовою? &quot;Знайди мені всіх постачальників на цьому сайті та збери їхні категорії товарів&quot;.</p>
<p>Саме це завдання вирішують LLM-агенти. Використовуючи &quot;мозок&quot; у вигляді великої мовної моделі (GPT, Gemini) та &quot;руки&quot; у вигляді набору інструментів (браузер, пошук Google), ці агенти можуть самостійно планувати та виконувати складні завдання в мережі.</p>
<ul>
<li><strong>Інструменти:</strong> Зв'язки на кшталт <code>LangChain</code> + <code>Pydoll</code> або кастомні рішення, як у <code>simple_browser.py</code>, який ми розберемо пізніше.</li>
<li><strong>Коли використовувати:</strong> Для складних дослідницьких завдань, де кроки заздалегідь невідомі та потрібна адаптація в реальному часі.</li>
<li><strong>Переваги:</strong> **Інтелект.** Здатність вирішувати неструктуровані завдання та адаптуватися до змін на льоту.</li>
<li><strong>Недоліки:</strong> &quot;Недетермінованість&quot; (результат може змінюватися від запуску до запуску), вартість API-викликів до LLM, нижча швидкість порівняно з прямим кодом.</li>
</ul>
<h4>Парадигма №4: Скрапінг без коду</h4>
<p>Іноді завдання настільки просте, що писати код — це надмірність. Потрібно швидко витягнути таблицю з однієї сторінки? Для цього існують елегантні рішення, які не вимагають програмування.</p>
<ul>
<li><strong>Інструменти:</strong> Функції Google Sheets (<code>IMPORTXML</code>, <code>IMPORTHTML</code>), розширення браузера.</li>
<li><strong>Коли використовувати:</strong> Для одноразових завдань, швидкого прототипування або коли ви просто не хочете писати код.</li>
<li><strong>Переваги:</strong> **Простота.** Відкрив, вказав, що потрібно зібрати, — отримав результат.</li>
<li><strong>Недоліки:</strong> Обмежена функціональність, не підходять для складних завдань або великих обсягів даних.</li>
</ul>
<h3>Що далі?</h3>
<p>Ця стаття — лише вступ. У наступних випусках нашого циклу «НЕ Selenium» ми перейдемо від теорії до жорсткої практики. Ми глибоко зануримося в кожну з цих парадигм і покажемо, як вони працюють на реальних прикладах:</p>
<ul>
<li>Розберемо <strong>Pydoll</strong> і подивимося, як він обходить Cloudflare.</li>
<li>Влаштуємо битву <strong>JavaScript проти Python</strong> за звання найкращої мови для веб-скрапінгу.</li>
<li>Навчимося вичавлювати максимум швидкості з парсингу за допомогою <strong>lxml</strong>.</li>
<li>Напишемо скрипт, який збирає дані з <strong>Amazon</strong> і зберігає їх у <strong>Excel</strong>.</li>
<li>Покажемо, як <strong>Google Sheets</strong> може стати вашим першим скрапером.</li>
<li>І, звичайно ж, детально розберемо, як створити та використовувати <strong>автономного LLM-агента</strong> для керування браузером.</li>
</ul>
<p>Приготуйтеся змінити свій погляд на автоматизацію та збір даних у мережі. Буде швидко, ефективно та дуже цікаво. Підписуйтесь</p>
