<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>&YAcy;&kcy; &ncy;&acy;&vcy;&chcy;&icy;&tcy;&icy; &ncy;&iecy;&jcy;&rcy;&ocy;&mcy;&iecy;&rcy;&iecy;&zhcy;&ucy; &pcy;&rcy;&acy;&tscy;&yucy;&vcy;&acy;&tcy;&icy; &rcy;&ucy;&kcy;&acy;&mcy;&icy;&colon; &scy;&tcy;&vcy;&ocy;&rcy;&iecy;&ncy;&ncy;&yacy; &pcy;&ocy;&vcy;&ncy;&ocy;&tscy;&iukcy;&ncy;&ncy;&ocy;&gcy;&ocy; &Iukcy;&Iukcy;-&acy;&gcy;&iecy;&ncy;&tcy;&acy; &zcy; MCP &tcy;&acy; LangGraph &zcy;&acy; &gcy;&ocy;&dcy;&icy;&ncy;&ucy;</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h2 id="як-навчити-нейромережу-працювати-руками-створення-повноцінного-іі-агента-з-mcp-та-langgraph-за-годину">Як навчити нейромережу працювати руками: створення повноцінного ІІ-агента з MCP та LangGraph за годину</h2>
<p>Друзі, вітаю! Сподіваюся, встигли скучити.</p>
<p>Останні пару місяців я з головою поринув у дослідження інтеграції ІІ-агентів у власні Python-проекти. У процесі накопичилося чимало практичних знань та спостережень, якими просто гріх не поділитися. Тому сьогодні я повертаюся на Хабр — з новою темою, свіжим поглядом та з наміром писати частіше.</p>
<p>На порядку денному — LangGraph та MCP: інструменти, за допомогою яких можна створювати дійсно корисних ІІ-агентів.</p>
<p>Якщо раніше ми сперечалися про те, яка нейромережа краще відповідає українською мовою, то сьогодні поле битви змістилося в бік більш прикладних завдань: хто краще справляється з роллю ІІ-агента? Які фреймворки дійсно спрощують розробку? І як інтегрувати все це добро в реальний проект?</p>
<p>Але перш ніж зануритися в практику та код, давайте розберемося з базовими поняттями. Особливо з двома ключовими: <strong>ІІ-агенти та MCP</strong>. Без них розмова про LangGraph буде неповною.</p>
<h3 id="іі-агенти-простими-словами">ІІ-агенти простими словами</h3>
<p>ІІ-агенти — це не просто «прокачані» чат-боти. Вони являють собою більш складні, автономні сутності, які володіють двома найважливішими особливостями:</p>
<ol>
<li>
<p><strong>Вміння взаємодіяти та координуватися</strong></p>
<p>Сучасні агенти здатні ділити завдання на підзавдання, викликати інших агентів, запитувати зовнішні дані, працювати в команді. Це вже не одиночний асистент, а розподілена система, де кожен компонент може вносити свій внесок.</p>
</li>
<li>
<p><strong>Доступ до зовнішніх ресурсів</strong></p>
<p>ІІ-агент більше не обмежений рамками діалогу. Він може звертатися до баз даних, виконувати виклики до API, взаємодіяти з локальними файлами, векторними сховищами знань і навіть запускати команди в терміналі. Все це стало можливим завдяки появі MCP — нового рівня інтеграції між моделлю та середовищем.</p>
</li>
</ol>
<hr>
<p>Якщо говорити просто: <strong>MCP — це міст між нейромережею та її оточенням</strong>. Він дозволяє моделі «розуміти» контекст завдання, отримувати доступ до даних, виконувати виклики та формувати обґрунтовані дії, а не просто видавати текстові відповіді.</p>
<p><strong>Уявімо аналогію:</strong></p>
<ul>
<li>У вас є <strong>нейромережа</strong> — вона вміє міркувати та генерувати тексти.</li>
<li>Є <strong>дані та інструменти</strong> — документи, API, бази знань, термінал, код.</li>
<li>І є <strong>MCP</strong> — це інтерфейс, який дозволяє моделі взаємодіяти з цими зовнішніми джерелами так, ніби вони є частиною її внутрішнього світу.</li>
</ul>
<p><strong>Без MCP:</strong></p>
<p>Модель — це ізольований діалоговий двигун. Ви подаєте їй текст — вона відповідає. І все.</p>
<p><strong>З MCP:</strong></p>
<p>Модель стає повноцінним <strong>виконавцем завдань</strong>:</p>
<ul>
<li>отримує доступ до структур даних та API;</li>
<li>викликає зовнішні функції;</li>
<li>орієнтується в поточному стані проекту або програми;</li>
<li>може запам'ятовувати, відстежувати та змінювати контекст по ходу діалогу;</li>
<li>використовує розширення, такі як інструменти пошуку, кодові ранери, базу векторних вбудовувань тощо.</li>
</ul>
<p>У технічному сенсі <strong>MCP — це протокол взаємодії між LLM та її оточенням</strong>, де контекст подається у вигляді структурованих об'єктів (замість «сирого» тексту), а виклики оформлюються як інтерактивні операції (наприклад, виклик функцій, використання інструментів або дії агента). Саме це перетворює звичайну модель на <strong>справжнього ІІ-агента</strong>, здатного робити більше, ніж просто «поговорити».</p>
<h3 id="а-тепер--до-справи">А тепер — до справи!</h3>
<p>Тепер, коли ми розібралися з базовими поняттями, логічно поставити запитання: «Як все це реалізувати на практиці в Python?»</p>
<p>Ось тут і вступає в гру <strong>LangGraph</strong> — потужний фреймворк для побудови графів станів, поведінки агентів та ланцюжків мислення. Він дозволяє «прошивати» логіку взаємодії між агентами, інструментами та користувачем, створюючи живу архітектуру ІІ, що адаптується до завдань.</p>
<p>У наступних розділах ми розглянемо, як:</p>
<ul>
<li>будується агент з нуля;</li>
<li>створюються стани, переходи та події;</li>
<li>інтегруються функції та інструменти;</li>
<li>і як вся ця екосистема працює в реальному проекті.</li>
</ul>
<h3 id="трохи-теорії-що-таке-langgraph">Трохи теорії: що таке LangGraph</h3>
<p>Перш ніж приступити до практики, потрібно сказати пару слів про сам фреймворк.</p>
<p><strong>LangGraph</strong> — це проект від команди <strong>LangChain</strong>, тих самих, хто першими запропонували концепцію «ланцюжків» (chains) взаємодії з LLM. Якщо раніше основний акцент робився на лінійні або умовно-розгалужені конвеєри (langchain.chains), то тепер розробники роблять ставку на <strong>графову модель</strong>, і саме LangGraph вони рекомендують як нове «ядро» для побудови складних ІІ-систем.</p>
<p><strong>LangGraph</strong> — це фреймворк для побудови кінцевих автоматів та графів станів, у яких кожен <strong>вузол</strong> представляє собою частину логіки агента: виклик моделі, зовнішній інструмент, умова, введення користувача тощо.</p>
<h3 id="як-це-працює-графи-та-вузли">Як це працює: графи та вузли</h3>
<p>Концептуально, LangGraph будується на таких ідеях:</p>
<ul>
<li><strong>Граф</strong> — це структура, яка описує можливі шляхи виконання логіки. Можна думати про нього як про карту: з однієї точки можна перейти в іншу залежно від умов або результату виконання.</li>
<li><strong>Вузли</strong> — це конкретні кроки всередині графа. Кожен вузол виконує якусь функцію: викликає модель, викликає зовнішній API, перевіряє умову або просто оновлює внутрішній стан.</li>
<li><strong>Переходи між вузлами</strong> — це логіка маршрутизації: якщо результат попереднього кроку такий-то, то йдемо туди-то.</li>
<li><strong>Стан</strong> — передається між вузлами та накопичує все, що потрібно: історію, проміжні висновки, введення користувача, результат роботи інструментів тощо.</li>
</ul>
<p>Таким чином, ми отримуємо <strong>гнучкий механізм керування логікою агента</strong>, в якому можна описувати як прості, так і дуже складні сценарії: цикли, умови, паралельні дії, вкладені виклики та багато іншого.</p>
<h3 id="чому-це-зручно">Чому це зручно?</h3>
<p>LangGraph дозволяє будувати <strong>прозору, відтворювану та розширювану логіку</strong>:</p>
<ul>
<li>легко налагоджувати;</li>
<li>легко візуалізувати;</li>
<li>легко масштабувати для нових завдань;</li>
<li>легко інтегрувати зовнішні інструменти та протоколи MCP.</li>
</ul>
<p>По суті, LangGraph — це <strong>«мозок» агента</strong>, де кожен крок задокументований, контрольований і може бути змінений без хаосу та «магії».</p>
<h3 id="ну-а-тепер--досить-теорії">Ну а тепер — досить теорії!</h3>
<p>Можна ще довго розповідати про графи, стани, композицію логіки та переваги LangGraph над класичними конвеєрами. Але, як показує практика, краще один раз побачити в коді.</p>
<p><strong>Час перейти до практики.</strong> Далі — приклад на Python: створимо простого, але корисного ІІ-агента на базі LangGraph, який використовуватиме зовнішні інструменти, пам'ять та прийматиме рішення сам.</p>
<h3 id="підготовка-хмарні-та-локальні-нейромережі">Підготовка: хмарні та локальні нейромережі</h3>
<p>Для того щоб приступити до створення ІІ-агентів, нам насамперед потрібен <strong>мозок</strong> — мовна модель. Тут є два підходи:</p>
<ul>
<li><strong>використовувати хмарні рішення</strong>, де все готово «з коробки»;</li>
<li>або <strong>підняти модель локально</strong> — для повної автономії та конфіденційності.</li>
</ul>
<p>Розглянемо обидва варіанти.</p>
<h4 id="хмарні-сервіси-швидко-та-зручно">Хмарні сервіси: швидко та зручно</h4>
<p>Найпростіший шлях — скористатися потужностями великих провайдерів: OpenAI, Anthropic, та використовувати...</p>
<h3 id="де-взяти-ключі-та-токени">Де взяти ключі та токени:</h3>
<ul>
<li><strong>OpenAI</strong> — ChatGPT та інші продукти;</li>
<li><strong>Anthropic</strong> — Claude;</li>
<li><strong><a href="http://OpenRouter.ai">OpenRouter.ai</a></strong> — десятки моделей (один токен — безліч моделей через OpenAI-сумісний API);</li>
<li><strong>Amvera Cloud</strong> — можливість підключити LLAMA з оплатою рублями та вбудованим проксіюванням до OpenAI та Anthropic.</li>
</ul>
<p>Цей шлях зручний, особливо якщо ви:</p>
<ul>
<li>не хочете налаштовувати інфраструктуру;</li>
<li>розробляєте з акцентом на швидкість;</li>
<li>працюєте з обмеженими ресурсами.</li>
</ul>
<h3 id="локальні-моделі-повний-контроль">Локальні моделі: повний контроль</h3>
<p>Якщо вам важлива <strong>конфіденційність, робота без інтернету</strong> або ви хочете будувати <strong>повністю автономні агенти</strong>, то має сенс розгорнути нейромережу локально.</p>
<p><strong>Основні переваги:</strong></p>
<ul>
<li><strong>Конфіденційність</strong> — дані залишаються у вас;</li>
<li><strong>Робота без інтернету</strong> — корисно в ізольованих мережах;</li>
<li><strong>Відсутність підписок та токенів</strong> — безкоштовно після налаштування.</li>
</ul>
<p><strong>Недоліки очевидні:</strong></p>
<ul>
<li>Вимоги до ресурсів (особливо до відеопам'яті);</li>
<li>Налаштування може зайняти час;</li>
<li>Деякі моделі складно розгорнути без досвіду.</li>
</ul>
<p>Тим не менш, є інструменти, які роблять локальний запуск простішим. Один з найкращих на сьогодні — це <strong>Ollama</strong>.</p>
<h3 id="розгортання-локальної-llm-через-ollama--docker">Розгортання локальної LLM через Ollama + Docker</h3>
<p>Ми підготуємо локальний запуск моделі Qwen 2.5 (qwen2.5:32b) за допомогою Docker-контейнера та системи Ollama. Це дозволить інтегрувати нейромережу з MCP та використовувати її у власних LangGraph-агентах.</p>
<p>Якщо обчислювальних ресурсів вашого комп'ютера або сервера виявиться недостатньо для роботи з цією версією моделі, ви завжди можете вибрати менш «ненажерливу» нейромережу — процес встановлення та запуску залишиться аналогічним.</p>
<p><strong>Швидка установка (короткий опис кроків)</strong></p>
<ol>
<li><strong>Встановіть Docker + Docker Compose</strong></li>
<li><strong>Створіть структуру проекту:</strong></li>
</ol>
<pre><code class="language-bash"><span class="hljs-built_in">mkdir</span> qwen-local &amp;&amp; <span class="hljs-built_in">cd</span> qwen-local
</code></pre>
<ol start="3">
<li><strong>Створіть <code>docker-compose.yml</code></strong>
(універсальний варіант, GPU визначається автоматично)</li>
</ol>
<pre><code class="language-yaml"><span class="hljs-attr">services:</span>
  <span class="hljs-attr">ollama:</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">ollama/ollama:latest</span>
    <span class="hljs-attr">container_name:</span> <span class="hljs-string">ollama_qwen</span>
    <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;11434:11434&quot;</span>
    <span class="hljs-attr">volumes:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">./ollama_data:/root/.ollama</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">/tmp:/tmp</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">OLLAMA_HOST=0.0.0.0</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">OLLAMA_ORIGINS=*</span>
    <span class="hljs-attr">restart:</span> <span class="hljs-string">unless-stopped</span>
</code></pre>
<ol start="4">
<li><strong>Запустіть контейнер:</strong></li>
</ol>
<pre><code class="language-bash">docker compose up -d
</code></pre>
<ol start="5">
<li><strong>Завантажте модель:</strong></li>
</ol>
<pre><code class="language-bash">docker <span class="hljs-built_in">exec</span> -it ollama_qwen ollama pull qwen2.5:32b
</code></pre>
<ol start="6">
<li><strong>Перевірте роботу через API:</strong></li>
</ol>
<pre><code class="language-bash">curl http://localhost:11434/api/generate \
  -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \
  -d <span class="hljs-string">&#x27;{&quot;model&quot;: &quot;qwen2.5:32b&quot;, &quot;prompt&quot;: &quot;Привіт!&quot;, &quot;stream&quot;: false}&#x27;</span>
</code></pre>
<p><em>(Зображення з результатом виконання команди curl)</em></p>
<ol start="7">
<li><strong>Інтеграція з Python:</strong></li>
</ol>
<pre><code class="language-python"><span class="hljs-keyword">import</span> requests

<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">prompt</span>):
    res = requests.post(<span class="hljs-string">&quot;http://localhost:11434/api/generate&quot;</span>, json={
        <span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;qwen2.5:32b&quot;</span>,
        <span class="hljs-string">&quot;prompt&quot;</span>: prompt,
        <span class="hljs-string">&quot;stream&quot;</span>: <span class="hljs-literal">False</span>
    })
    <span class="hljs-keyword">return</span> res.json()[<span class="hljs-string">&#x27;response&#x27;</span>]

<span class="hljs-built_in">print</span>(query(<span class="hljs-string">&quot;Поясни квантову заплутаність&quot;</span>))
</code></pre>
<p>Тепер у вас повноцінний локальний LLM, готовий до роботи з MCP та LangGraph.</p>
<p><strong>Що далі?</strong></p>
<p>У нас є вибір між хмарними та локальними моделями, і ми навчилися підключати обидві. Найцікавіше попереду — <strong>створення ІІ-агентів на LangGraph</strong>, які використовуватимуть обрану модель, пам'ять, інструменти та власну логіку.</p>
<p><strong>Переходимо до найсмачнішого — коду та практики!</strong></p>
<hr>
<p>Перш ніж перейти до практики, важливо підготувати робоче середовище. Я припускаю, що ви вже знайомі з основами Python, знаєте, що таке бібліотеки та залежності, і розумієте, навіщо використовувати віртуальне середовище.</p>
<p>Якщо все це для вас нове — рекомендую спочатку пройти короткий курс або посібник з основ Python, а потім повернутися до статті.</p>
<h4 id="крок-1-створення-віртуального-середовища">Крок 1: Створення віртуального середовища</h4>
<p>Створіть нове віртуальне середовище в папці проекту:</p>
<pre><code class="language-bash">python -m venv venv
<span class="hljs-built_in">source</span> venv/bin/activate  <span class="hljs-comment"># для Linux/macOS</span>
virtualenv\Scripts\activate   <span class="hljs-comment"># для Windows</span>
</code></pre>
<h4 id="крок-2-встановлення-залежностей">Крок 2: Встановлення залежностей</h4>
<p>Створіть файл <code>requirements.txt</code> та додайте до нього такі рядки:</p>
<pre><code>langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
</code></pre>
<blockquote>
<p>⚠️ <strong>Актуальні версії вказані станом на 21 липня 2025 року.</strong> З моменту публікації вони могли змінитися — <strong>перевіряйте актуальність перед встановленням.</strong></p>
</blockquote>
<p>Потім встановіть залежності:</p>
<pre><code class="language-bash">pip install -r requirements.txt```

<span class="hljs-comment">#### Крок 3: Конфігурація змінних середовища</span>

Створіть у корені проекту файл `.<span class="hljs-built_in">env</span>` та додайте до нього необхідні API-ключі:
</code></pre>
<p>OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc</p>
<pre><code>
**Призначення змінних:**

*   **OPENAI_API_KEY** — ключ для доступу до GPT-моделей від OpenAI;
*   **DEEPSEEK_API_KEY** — ключ для використання моделей Deepseek;
*   **OPENROUTER_API_KEY** — єдиний ключ для доступу до безлічі моделей через OpenRouter

---

Деякі інструменти MCP (наприклад, `brave-web-search`) вимагають ключ для роботи. Без нього вони просто не активуються.

**А якщо у вас немає API-ключів?**

Не проблема. Ви можете почати розробку з локальною моделлю (наприклад, через Ollama), не підключаючи жодного зовнішнього сервісу. У цьому випадку файл `.env` можна взагалі не створювати.

Готово! Тепер у нас є все необхідне для початку — ізольоване середовище, залежності, і, за потреби, доступ до хмарних нейромереж та інтеграцій MCP.

Далі — ми запустимо нашого LLM-агента різними способами.

### Простий запуск LLM-агентів через LangGraph: базова інтеграція

Почнемо з найпростішого: як «підключити мозок» до майбутнього агента. Ми розберемо базові способи запуску мовних моделей (LLM) за допомогою LangChain, щоб на наступному кроці перейти до інтеграції з LangGraph та побудови повноцінного ІІ-агента.

#### Імпорти
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
</code></pre>
<ul>
<li><code>os</code> та <code>load_dotenv()</code> — для завантаження змінних з файлу <code>.env</code>.</li>
<li><code>ChatOpenAI</code>, <code>ChatOllama</code>, <code>ChatDeepSeek</code> — обгортки для підключення мовних моделей через LangChain.</li>
</ul>
<blockquote>
<p>💡 Якщо ви використовуєте альтернативні підходи до роботи з конфігураціями (наприклад, Pydantic Settings), ви можете замінити <code>load_dotenv()</code> на свій звичний метод.</p>
</blockquote>
<h4 id="завантаження-змінних-середовища">Завантаження змінних середовища</h4>
<pre><code class="language-python">load_dotenv()
</code></pre>
<p>Це завантажить усі змінні з <code>.env</code>, включаючи ключі для доступу до API OpenAI, DeepSeek, OpenRouter та інших.</p>
<h4 id="прості-функції-для-отримання-llm">Прості функції для отримання LLM</h4>
<p><strong>OpenAI</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_openai_llm</span>():
    <span class="hljs-keyword">return</span> ChatOpenAI(model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span>, api_key=os.getenv(<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>))
</code></pre>
<p>Якщо змінна <code>OPENAI_API_KEY</code> правильно встановлена, LangChain підставить її автоматично — явне зазначення <code>api_key=...</code> тут є необов'язковим.</p>
<p><strong>DeepSeek</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_deepseek_llm</span>():
    <span class="hljs-comment"># ...</span>
</code></pre>
<p>Аналогічно, але використовуючи обгортку <code>ChatDeepSeek</code>.</p>
<p><strong>OpenRouter (та інші сумісні API)</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_openrouter_llm</span>(<span class="hljs-params">model=<span class="hljs-string">&quot;moonshotai/kimi-k2:free&quot;</span></span>):
    <span class="hljs-keyword">return</span> ChatOpenAI(
        model=model,
        api_key=os.getenv(<span class="hljs-string">&quot;OPENROUTER_API_KEY&quot;</span>),
        base_url=<span class="hljs-string">&quot;https://openrouter.ai/api/v1&quot;</span>,
        temperature=<span class="hljs-number">0</span>
    )
</code></pre>
<p><strong>Особливості:</strong></p>
<ul>
<li><code>ChatOpenAI</code> використовується, незважаючи на те, що модель не від OpenAI — тому що OpenRouter використовує той самий протокол.</li>
<li><code>base_url</code> є обов'язковим: він вказує на OpenRouter API.</li>
<li>Модель <code>moonshotai/kimi-k2:free</code> була обрана як один з найбільш збалансованих варіантів за якістю та швидкістю на момент написання статті.</li>
<li>API-ключ <code>OpenRouter</code> потрібно передавати явно — автоматична підстановка тут не працює.</li>
</ul>
<h4 id="міні-тест-перевірка-роботи-моделі">Міні-тест: перевірка роботи моделі</h4>
<pre><code class="language-python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:
    llm = get_openrouter_llm(model=<span class="hljs-string">&quot;moonshotai/kimi-k2:free&quot;</span>)
    response = llm.invoke(<span class="hljs-string">&quot;Хто ти?&quot;</span>)
    <span class="hljs-built_in">print</span>(response.content)
</code></pre>
<p><em>(Зображення з результатом виконання: <code>Я - ІІ-асистент, створений компанією Moonshot AI...</code>)</em></p>
<p>Якщо все налаштовано правильно, ви отримаєте осмислену відповідь від моделі. Вітаю — перший крок зроблено!</p>
<h3 id="але-це-ще-не-агент">Але це ще не агент</h3>
<p>На поточному етапі ми підключили LLM і зробили простий виклик. Це більше схоже на консольний чат-бот, ніж на ІІ-агента.</p>
<p><strong>Чому?</strong></p>
<ul>
<li>Ми пишемо <strong>синхронний, лінійний код</strong> без логіки стану або цілей.</li>
<li>Агент не приймає рішень, не запам'ятовує контекст і не використовує інструменти.</li>
<li>MCP та LangGraph поки що не задіяні.</li>
</ul>
<p><strong>Що далі?</strong></p>
<p>Далі ми реалізуємо <strong>повноцінного ІІ-агента</strong> за допомогою <strong>LangGraph</strong> — спочатку без MCP, щоб зосередитися на архітектурі, станах та логіці самого агента.</p>
<p>Занурюємося в справжню агентну механіку. Поїхали!</p>
<h3 id="агент-класифікації-вакансій-від-теорії-до-практики">Агент класифікації вакансій: від теорії до практики</h3>
<p>...концепції LangGraph на практиці та створити корисний інструмент для HR-платформ та бірж фрілансу.</p>
<h4 id="завдання-агента">Завдання агента</h4>
<p>Наш агент приймає на вхід текстовий опис вакансії або послуги та виконує трирівневу класифікацію:</p>
<ol>
<li><strong>Тип роботи</strong>: проектна робота або постійна вакансія</li>
<li><strong>Категорія професії</strong>: з 45+ заздалегідь визначених спеціальностей</li>
<li><strong>Тип пошуку</strong>: чи шукає людина роботу, чи шукає виконавця</li>
</ol>
<p>Результат повертається у структурованому JSON-форматі з оцінкою впевненості для кожної класифікації.</p>
<h4 id="-архітектура-агента-на-langgraph">📈 Архітектура агента на LangGraph</h4>
<p>Дотримуючись принципів LangGraph, створюємо <strong>граф станів</strong> з чотирьох вузлів:</p>
<ul>
<li>Вхідний опис</li>
<li>↓</li>
<li>Вузол класифікації типу роботи</li>
<li>↓</li>
<li>Вузол класифікації категорії</li>
<li>↓</li>
<li>Вузол визначення типу пошуку</li>
<li>↓</li>
<li>Вузол розрахунку впевненості</li>
<li>↓</li>
<li>JSON-результат</li>
</ul>
<p>Кожен вузол — це <strong>спеціалізована функція</strong>, яка:</p>
<ul>
<li>Отримує поточний стан агента</li>
<li>Виконує свою частину аналізу</li>
<li>Оновлює стан та передає його далі</li>
</ul>
<h4 id="управління-станом">Управління станом</h4>
<p>Визначаємо <strong>структуру пам'яті агента</strong> через <code>TypedDict</code>:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> TypedDict, <span class="hljs-type">Dict</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">State</span>(<span class="hljs-title class_ inherited__">TypedDict</span>):
    <span class="hljs-string">&quot;&quot;&quot;Стан агента для зберігання інформації про процес класифікації&quot;&quot;&quot;</span>
    description: <span class="hljs-built_in">str</span>
    job_type: <span class="hljs-built_in">str</span>
    category: <span class="hljs-built_in">str</span>
    search_type: <span class="hljs-built_in">str</span>
    confidence_scores: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">float</span>]
    processed: <span class="hljs-built_in">bool</span>
</code></pre>
<p>Це <strong>робоча пам'ять агента</strong> — все, що він пам'ятає та накопичує під час аналізу. Подібно до того, як людина-експерт тримає в голові контекст завдання під час аналізу документа.</p>
<p>Давайте розглянемо повний код, а після цього зосередимося на основних моментах.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> asyncio
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> enum <span class="hljs-keyword">import</span> Enum
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> TypedDict, <span class="hljs-type">Dict</span>, <span class="hljs-type">Any</span>, <span class="hljs-type">List</span>

<span class="hljs-keyword">from</span> langgraph.graph <span class="hljs-keyword">import</span> StateGraph, END
<span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate
<span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI
<span class="hljs-keyword">from</span> langchain.schema <span class="hljs-keyword">import</span> HumanMessage

<span class="hljs-comment"># Категорії професій</span>
CATEGORIES = [
    <span class="hljs-string">&quot;2D-аніматор&quot;</span>, <span class="hljs-string">&quot;3D-аніматор&quot;</span>, <span class="hljs-string">&quot;3D-моделлер&quot;</span>,
    <span class="hljs-string">&quot;Бізнес-аналітик&quot;</span>, <span class="hljs-string">&quot;Блокчейн-розробник&quot;</span>, ...
]

<span class="hljs-keyword">class</span> <span class="hljs-title class_">JobType</span>(<span class="hljs-title class_ inherited__">Enum</span>):
    PROJECT = <span class="hljs-string">&quot;проектна робота&quot;</span>
    PERMANENT = <span class="hljs-string">&quot;постійна робота&quot;</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">SearchType</span>(<span class="hljs-title class_ inherited__">Enum</span>):
    LOOKING_FOR_WORK = <span class="hljs-string">&quot;пошук роботи&quot;</span>
    LOOKING_FOR_PERFORMER = <span class="hljs-string">&quot;пошук виконавця&quot;</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">State</span>(<span class="hljs-title class_ inherited__">TypedDict</span>):
    <span class="hljs-string">&quot;&quot;&quot;Стан агента для зберігання інформації про процес класифікації&quot;&quot;&quot;</span>
    description: <span class="hljs-built_in">str</span>
    job_type: <span class="hljs-built_in">str</span>
    category: <span class="hljs-built_in">str</span>
    search_type: <span class="hljs-built_in">str</span>
    confidence_scores: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">float</span>]
    processed: <span class="hljs-built_in">bool</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">VacancyClassificationAgent</span>:
    <span class="hljs-string">&quot;&quot;&quot;Асинхронний агент для класифікації вакансій та послуг&quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;gpt-4o-mini&quot;</span>, temperature: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.1</span></span>):
        <span class="hljs-string">&quot;&quot;&quot;Ініціалізація агента&quot;&quot;&quot;</span>
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_create_workflow</span>(<span class="hljs-params">self</span>) -&gt; StateGraph:
        <span class="hljs-string">&quot;&quot;&quot;Створює робочий процес агента на основі LangGraph&quot;&quot;&quot;</span>
        workflow = StateGraph(State)
        
        <span class="hljs-comment"># Додаємо вузли в граф</span>
        workflow.add_node(<span class="hljs-string">&quot;job_type_classification&quot;</span>, self._classify_job_type)
        workflow.add_node(<span class="hljs-string">&quot;category_classification&quot;</span>, self._classify_category)
        workflow.add_node(<span class="hljs-string">&quot;search_type_classification&quot;</span>, self._classify_search_type)
        workflow.add_node(<span class="hljs-string">&quot;confidence_calculation&quot;</span>, self._calculate_confidence)
        
        <span class="hljs-comment"># Визначаємо послідовність виконання вузлів</span>
        workflow.set_entry_point(<span class="hljs-string">&quot;job_type_classification&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;job_type_classification&quot;</span>, <span class="hljs-string">&quot;category_classification&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;category_classification&quot;</span>, <span class="hljs-string">&quot;search_type_classification&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;search_type_classification&quot;</span>, <span class="hljs-string">&quot;confidence_calculation&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;confidence_calculation&quot;</span>, END)
        
        <span class="hljs-keyword">return</span> workflow.<span class="hljs-built_in">compile</span>()

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_job_type</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Вузол для визначення типу роботи: проектна або постійна&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (реалізація далі)</span>
        
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_category</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Вузол для визначення категорії професії&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (реалізація далі)</span>
        
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_search_type</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Вузол для визначення типу пошуку&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (реалізація далі)</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_calculate_confidence</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Вузол для розрахунку рівня впевненості в класифікації&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (реалізація далі)</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_find_closest_category</span>(<span class="hljs-params">self, predicted_category: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:
        <span class="hljs-string">&quot;&quot;&quot;Знаходить найближчу категорію зі списку доступних&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (реалізація далі)</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">classify</span>(<span class="hljs-params">self, description: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Основний метод для класифікації вакансії/послуги&quot;&quot;&quot;</span>
        initial_state = {
            <span class="hljs-string">&quot;description&quot;</span>: description,
            <span class="hljs-string">&quot;job_type&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,
            <span class="hljs-string">&quot;category&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,
            <span class="hljs-string">&quot;search_type&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,
            <span class="hljs-string">&quot;confidence_scores&quot;</span>: {},
            <span class="hljs-string">&quot;processed&quot;</span>: <span class="hljs-literal">False</span>
        }
        
        <span class="hljs-comment"># Запускаємо робочий процес</span>
        result = <span class="hljs-keyword">await</span> self.workflow.ainvoke(initial_state)
        
        <span class="hljs-comment"># Формуємо підсумкову відповідь у форматі JSON</span>
        classification_result = {
            <span class="hljs-string">&quot;job_type&quot;</span>: result[<span class="hljs-string">&quot;job_type&quot;</span>],
            <span class="hljs-string">&quot;category&quot;</span>: result[<span class="hljs-string">&quot;category&quot;</span>],
            <span class="hljs-string">&quot;search_type&quot;</span>: result[<span class="hljs-string">&quot;search_type&quot;</span>],
            <span class="hljs-string">&quot;confidence_scores&quot;</span>: result[<span class="hljs-string">&quot;confidence_scores&quot;</span>],
            <span class="hljs-string">&quot;success&quot;</span>: result[<span class="hljs-string">&quot;processed&quot;</span>]
        }
        <span class="hljs-keyword">return</span> classification_result

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    <span class="hljs-string">&quot;&quot;&quot;Демонстрація роботи агента&quot;&quot;&quot;</span>
    agent = VacancyClassificationAgent()
    
    test_cases = [
        <span class="hljs-string">&quot;Потрібен Python розробник для створення веб-додатку на Django. Постійна робота.&quot;</span>,
        <span class="hljs-string">&quot;Шукаю замовлення на створення логотипів та фірмового стилю. Працюю в Adobe Illustrator.&quot;</span>,
        <span class="hljs-string">&quot;Потрібен 3D-аніматор для короткострокового проекту створення рекламного ролика.&quot;</span>,
        <span class="hljs-string">&quot;Резюме: досвідчений маркетолог, шукаю віддалену роботу в сфері digital-маркетингу&quot;</span>,
        <span class="hljs-string">&quot;Шукаємо фронтенд-розробника React у нашу команду на постійній основі&quot;</span>
    ]
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;🤖 Демонстрація роботи агента класифікації вакансій\n&quot;</span>)
    <span class="hljs-keyword">for</span> i, description <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(test_cases, <span class="hljs-number">1</span>):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;--- Тест <span class="hljs-subst">{i}</span>: ---&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Опис: <span class="hljs-subst">{description}</span>&quot;</span>)
        <span class="hljs-keyword">try</span>:
            result = <span class="hljs-keyword">await</span> agent.classify(description)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Результат класифікації:&quot;</span>)
            <span class="hljs-built_in">print</span>(json.dumps(result, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">2</span>))
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;❌ Помилка: <span class="hljs-subst">{e}</span>&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:
    asyncio.run(main())

</code></pre>
<p><em>(...решта коду з реалізацією методів була представлена в статті...)</em></p>
<h3 id="ключові-переваги-архітектури">Ключові переваги архітектури</h3>
<ol>
<li><strong>Модульність</strong> — кожен вузол вирішує одне завдання, легко тестувати та покращувати окремо</li>
<li><strong>Розширюваність</strong> — нові функції додаються декларативно</li>
<li><strong>Прозорість</strong> — весь процес прийняття рішень документований та відстежуваний</li>
<li><strong>Продуктивність</strong> — асинхронна обробка множинних запитів</li>
<li><strong>Надійність</strong> — вбудована обробка помилок та відновлення</li>
</ol>
<h3 id="реальна-користь">Реальна користь</h3>
<p>Такий агент може використовуватися в:</p>
<ul>
<li><strong>HR-платформах</strong> для автоматичної категоризації резюме та вакансій</li>
<li><strong>Біржах фрілансу</strong> для покращення пошуку та рекомендацій</li>
<li><strong>Внутрішніх системах</strong> компаній для обробки заявок та проектів</li>
<li><strong>Аналітичних рішеннях</strong> для дослідження ринку праці</li>
</ul>
<h3 id="mcp-в-дії-створюємо-агента-з-файловою-системою-та-веб-пошуком">MCP в дії: створюємо агента з файловою системою та веб-пошуком</h3>
<p>Після того як ми розібралися з базовими принципами LangGraph та створили простого агента-класифікатора, давайте розширимо його можливості, підключивши до зовнішнього світу через MCP.</p>
<p>Зараз ми створимо повноцінного ІІ-помічника, який зможе:</p>
<ul>
<li>Працювати з файловою системою (читати, створювати, змінювати файли)</li>
<li>Шукати актуальну інформацію в інтернеті</li>
<li>Запам'ятовувати контекст діалогу</li>
<li>Обробляти помилки та відновлюватися після збоїв</li>
</ul>
<h4 id="від-теорії-до-реальних-інструментів">Від теорії до реальних інструментів</h4>
<p>Пам'ятаєте, як на початку статті ми говорили про те, що <strong>MCP — це міст між нейромережею та її оточенням</strong>? Зараз ви побачите це на практиці. Наш агент отримає доступ до <strong>реальних інструментів</strong>:</p>
<pre><code># Інструменти файлової системи
- read_file – читання файлів
- write_file – запис та створення файлів
- list_directory – перегляд вмісту папок
- create_directory – створення папок

# Інструменти веб-пошуку
- brave_web_search – пошук в інтернеті
- get_web_content – отримання вмісту сторінок
</code></pre>
<p>Це вже не «іграшковий» агент — це <strong>робочий інструмент</strong>, який може вирішувати реальні завдання.</p>
<h4 id="-архітектура-від-простого-до-складного">📈 Архітектура: від простого до складного</h4>
<p><strong>1. Конфігурація як основа стабільності</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass

<span class="hljs-meta">@dataclass</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">AgentConfig</span>:
    <span class="hljs-string">&quot;&quot;&quot;Спрощена конфігурація ІІ-агента&quot;&quot;&quot;</span>
    filesystem_path: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;/path/to/work/directory&quot;</span>
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>
    enable_web_search: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">validate</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-string">&quot;&quot;&quot;Валідація конфігурації&quot;&quot;&quot;</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(self.filesystem_path):
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Шлях не існує: <span class="hljs-subst">{self.filesystem_path}</span>&quot;</span>)
</code></pre>
<p><strong>Чому це важливо?</strong> На відміну від прикладу класифікації, тут агент взаємодіє із зовнішніми системами. Одна помилка в шляху до файлів або відсутній API-ключ — і весь агент перестає працювати. <strong>Валідація на старті</strong> економить години налагодження.</p>
<p><strong>2. Фабрика моделей: гнучкість вибору</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_model</span>(<span class="hljs-params">config: AgentConfig</span>):
    <span class="hljs-string">&quot;&quot;&quot;Створює модель відповідно до конфігурації&quot;&quot;&quot;</span>
    provider = config.model_provider.value
    <span class="hljs-keyword">if</span> provider == <span class="hljs-string">&quot;ollama&quot;</span>:
        <span class="hljs-keyword">return</span> ChatOllama(model=<span class="hljs-string">&quot;qwen2.5:32b&quot;</span>, base_url=<span class="hljs-string">&quot;http://localhost:11434&quot;</span>)
    <span class="hljs-keyword">elif</span> provider == <span class="hljs-string">&quot;openai&quot;</span>:
        <span class="hljs-keyword">return</span> ChatOpenAI(model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span>, api_key=os.getenv(<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>))
    <span class="hljs-comment"># ... інші провайдери</span>
</code></pre>
<p>Один код — багато моделей. Хочете безкоштовну локальну модель? Використовуйте Ollama. Потрібна максимальна точність? Перейдіть на GPT-4. Важлива швидкість? Спробуйте DeepSeek. Код залишається тим самим.</p>
<p><strong>3. MCP-інтеграція: підключення до реального світу</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_mcp_client</span>(<span class="hljs-params">self</span>):
    <span class="hljs-string">&quot;&quot;&quot;Ініціалізація MCP клієнта&quot;&quot;&quot;</span>
    mcp_config = {
        <span class="hljs-string">&quot;filesystem&quot;</span>: {
            <span class="hljs-string">&quot;command&quot;</span>: <span class="hljs-string">&quot;npx&quot;</span>,
            <span class="hljs-string">&quot;args&quot;</span>: [<span class="hljs-string">&quot;-y&quot;</span>, <span class="hljs-string">&quot;@modelcontextprotocol/server-filesystem&quot;</span>, self.filesystem_path],
            <span class="hljs-string">&quot;transport&quot;</span>: <span class="hljs-string">&quot;stdio&quot;</span>
        },
        <span class="hljs-string">&quot;brave-search&quot;</span>: {
            <span class="hljs-string">&quot;command&quot;</span>: <span class="hljs-string">&quot;npx&quot;</span>,
            <span class="hljs-string">&quot;args&quot;</span>: [<span class="hljs-string">&quot;-y&quot;</span>, <span class="hljs-string">&quot;@modelcontextprotocol/server-brave-search@latest&quot;</span>],
            <span class="hljs-string">&quot;transport&quot;</span>: <span class="hljs-string">&quot;stdio&quot;</span>,
            <span class="hljs-string">&quot;env&quot;</span>: {<span class="hljs-string">&quot;BRAVE_API_KEY&quot;</span>: os.getenv(<span class="hljs-string">&quot;BRAVE_API_KEY&quot;</span>)}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = <span class="hljs-keyword">await</span> self.mcp_client.get_tools()
</code></pre>
<p>Тут відбувається ключова робота MCP: ми підключаємо зовнішні MCP-сервери до агента, які надають набір інструментів та функцій. Агент, у свою чергу, отримує не просто окремі функції, а повне контекстне розуміння того, як працювати з файловою системою та Інтернетом.</p>
<h4 id="стійкість-до-помилок">Стійкість до помилок</h4>
<p>У реальному світі все ламається: мережа недоступна, файли заблоковані, API-ключі прострочені. Наш агент готовий до цього:</p>
<pre><code class="language-python"><span class="hljs-meta">@retry_on_failure(<span class="hljs-params">max_retries=<span class="hljs-number">2</span>, delay=<span class="hljs-number">1.0</span></span>)</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_message</span>(<span class="hljs-params">self, user_input: <span class="hljs-built_in">str</span>, thread_id: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;default&quot;</span></span>) -&gt; <span class="hljs-built_in">str</span>:
    <span class="hljs-comment"># ...</span>
</code></pre>
<p>Декоратор <code>@retry_on_failure</code> автоматично повторює операції при тимчасових збоях. Користувач навіть не помітить, що щось пішло не так.</p>
<h3 id="підсумки-від-теорії-до-практики-іі-агентів">Підсумки: від теорії до практики ІІ-агентів</h3>
<p>Сьогодні ми пройшли шлях від базових концепцій до створення працюючих ІІ-агентів. Давайте підсумуємо, що ми вивчили та чого досягли.</p>
<p><strong>Що ми освоїли</strong></p>
<p><strong>1. Фундаментальні концепції</strong></p>
<ul>
<li>Розібралися з відмінностями між чат-ботами та справжніми ІІ-агентами</li>
<li>Зрозуміли роль <strong>MCP (Model Context Protocol)</strong> як моста між моделлю та зовнішнім світом</li>
<li>Вивчили архітектуру <strong>LangGraph</strong> для побудови складної логіки агентів</li>
</ul>
<p><strong>2. Практичні навички</strong></p>
<ul>
<li>Налаштували робоче середовище з підтримкою хмарних та локальних моделей</li>
<li>Створили <strong>агента-класифікатора</strong> з асинхронною архітектурою та керуванням станами</li>
<li>Побудували <strong>MCP-агента</strong> з доступом до файлової системи та веб-пошуку</li>
</ul>
<p><strong>3. Архітектурні патерни</strong></p>
<ul>
<li>Освоїли модульну конфігурацію та фабрики моделей</li>
<li>Впровадили обробку помилок та <strong>механізми повторних спроб</strong> для готових до виробництва рішень</li>
</ul>
<h3 id="ключові-переваги-підходу">Ключові переваги підходу</h3>
<p><strong>LangGraph + MCP</strong> дають нам:</p>
<ul>
<li><strong>Прозорість</strong> — кожен крок агента документований та відстежуваний</li>
<li><strong>Розширюваність</strong> — нові функції додаються декларативно</li>
<li><strong>Надійність</strong> — вбудована обробка помилок та відновлення</li>
<li><strong>Гнучкість</strong> — підтримка багатьох моделей та провайдерів з коробки</li>
</ul>
<h3 id="висновок">Висновок</h3>
<p>ІІ-агенти — це не футуристична фантастика, а <strong>реальна технологія сьогодення</strong>. За допомогою LangGraph та MCP ми можемо створювати системи, які вирішують конкретні бізнес-завдання, автоматизують рутину та відкривають нові можливості.</p>
<p><strong>Головне — почати.</strong> Візьміть код з прикладів, адаптуйте під свої завдання, експериментуйте. Кожен проект — це новий досвід та крок до майстерності в галузі розробки ІІ.</p>
<p>Успіхів у ваших проектах!</p>
<hr>
<p><em>Теги: python, іі, mcp, langchain, іі-асистент, ollama, іі-агенти, local llm, langgraph, mcp-server</em>
<em>Хаби: Блог компанії Amvera, Обробка природної мови, Штучний інтелект, Python, Програмування</em></p>

            
            
        </body>
        </html>