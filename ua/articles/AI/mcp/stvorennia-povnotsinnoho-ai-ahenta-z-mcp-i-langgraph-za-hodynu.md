# Як навчити нейромережу працювати руками: створення повноцінного ШІ-агента з MCP та LangGraph за годину

Друзі, вітаю! Сподіваюся, встигли скучити.

Останні пару місяців я з головою поринув у дослідження інтеграції ШІ-агентів у власні Python-проекти. У процесі накопичилося чимало практичних знань та спостережень, якими просто гріх не поділитися. Тому сьогодні я повертаюся на Хабр — з новою темою, свіжим поглядом та з наміром писати частіше.

На порядку денному — LangGraph та MCP: інструменти, за допомогою яких можна створювати дійсно корисних ШІ-агентів.

Якщо раніше ми сперечалися про те, яка нейромережа краще відповідає російською мовою, то сьогодні поле битви змістилося в бік більш прикладних завдань: хто краще справляється з роллю ШІ-агента? Які фреймворки дійсно спрощують розробку? І як інтегрувати все це добро в реальний проект?

Але перш ніж зануритися в практику та код, давайте розберемося з базовими поняттями. Особливо з двома ключовими: **ШІ-агенти та MCP**. Без них розмова про LangGraph буде неповною.

### ШІ-агенти простими словами

ШІ-агенти — це не просто «прокачані» чат-боти. Вони являють собою більш складні, автономні сутності, які володіють двома найважливішими особливостями:

1.  **Вміння взаємодіяти та координуватися**

    Сучасні агенти здатні ділити завдання на підзадачі, викликати інших агентів, запитувати зовнішні дані, працювати в команді. Це вже не одиночний асистент, а розподілена система, де кожен компонент може робити свій внесок.

2.  **Доступ до зовнішніх ресурсів**

    ШІ-агент більше не обмежений рамками діалогу. Він може звертатися до баз даних, виконувати виклики до API, взаємодіяти з локальними файлами, векторними сховищами знань і навіть запускати команди в терміналі. Все це стало можливим завдяки появі MCP — нового рівня інтеграції між моделлю та середовищем.

---

Якщо говорити просто: **MCP — це міст між нейромережею та її оточенням**. Він дозволяє моделі «розуміти» контекст завдання, отримувати доступ до даних, виконувати виклики та формувати обґрунтовані дії, а не просто видавати текстові відповіді.

**Уявімо аналогію:**

*   У вас є **нейромережа** — вона вміє міркувати та генерувати тексти.
*   Є **дані та інструменти** — документи, API, бази знань, термінал, код.
*   І є **MCP** — це інтерфейс, який дозволяє моделі взаємодіяти з цими зовнішніми джерелами так, ніби вони були частиною її внутрішнього світу.

**Без MCP:**

Модель — це ізольований діалоговий двигун. Ви подаєте їй текст — вона відповідає. І все.

**З MCP:**

Модель стає повноцінним **виконавцем завдань**:

*   отримує доступ до структур даних та API;
*   викликає зовнішні функції;
*   орієнтується в поточному стані проекту або програми;
*   може запам'ятовувати, відстежувати та змінювати контекст у міру діалогу;
*   використовує розширення, такі як інструменти пошуку, код-раннери, базу векторних ембеддингів тощо.

У технічному сенсі **MCP — це протокол взаємодії між LLM та її оточенням**, де контекст подається у вигляді структурованих об'єктів (замість «сирого» тексту), а виклики оформлюються як інтерактивні операції (наприклад, function calling, tool usage або agent actions). Саме це і перетворює звичайну модель на **справжнього ШІ-агента**, здатного робити більше, ніж просто "поговорити".

### А тепер — до справи!

Тепер, коли ми розібралися з базовими поняттями, логічно поставити запитання: «Як все це реалізувати на практиці в Python?»

Ось тут і вступає в гру **LangGraph** — потужний фреймворк для побудови графів станів, поведінки агентів та ланцюжків мислення. Він дозволяє "прошивати" логіку взаємодії між агентами, інструментами та користувачем, створюючи живу архітектуру ШІ, що адаптується до завдань.

У наступних розділах ми подивимося, як:

*   будується агент з нуля;
*   створюються стани, переходи та події;
*   інтегруються функції та інструменти;
*   і як вся ця екосистема працює в реальному проекті.

### Трохи теорії: що таке LangGraph

Перш ніж приступити до практики, потрібно сказати пару слів про сам фреймворк.

**LangGraph** — це проект від команди **LangChain**, тих самих, хто першими запропонували концепцію «ланцюжків» (chains) взаємодії з LLM. Якщо раніше основний акцент робився на лінійні або умовно-розгалужені пайплайни (langchain.chains), то тепер розробники роблять ставку на **графову модель**, і саме LangGraph вони рекомендують як нове «ядро» для побудови складних ШІ-систем.

**LangGraph** — це фреймворк для побудови кінцевих автоматів та графів станів, у яких кожен **вузол** представляє собою частину логіки агента: виклик моделі, зовнішній інструмент, умова, введення користувача тощо.

### Як це працює: графи та вузли

Концептуально, LangGraph будується на наступних ідеях:

*   **Граф** — це структура, яка описує можливі шляхи виконання логіки. Можна думати про нього як про карту: з однієї точки можна перейти в іншу залежно від умов або результату виконання.
*   **Вузли** — це конкретні кроки всередині графа. Кожен вузол виконує якусь функцію: викликає модель, викликає зовнішній API, перевіряє умову або просто оновлює внутрішній стан.
*   **Переходи між вузлами** — це логіка маршрутизації: якщо результат попереднього кроку такий-то, то йдемо туди-то.
*   **Стан** — передається між вузлами та накопичує все, що потрібно: історію, проміжні висновки, введення користувача, результат роботи інструментів тощо.

Таким чином, ми отримуємо **гнучкий механізм керування логікою агента**, у якому можна описувати як прості, так і дуже складні сценарії: цикли, умови, паралельні дії, вкладені виклики та багато іншого.

### Чому це зручно?

LangGraph дозволяє будувати **прозору, відтворювану та розширювану логіку**:

*   легко налагоджувати;
*   легко візуалізувати;
*   легко масштабувати під нові завдання;
*   легко інтегрувати зовнішні інструменти та MCP-протоколи.

По суті, LangGraph — це **«мозок» агента**, де кожен крок задокументований, контрольований і може бути змінений без хаосу та «магії».

### Ну а тепер — досить теорії!

Можна ще довго розповідати про графи, стани, композицію логіки та переваги LangGraph над класичними пайплайнами. Але, як показує практика, краще один раз побачити в коді.

**Час перейти до практики.** Далі — приклад на Python: створимо простого, але корисного ШІ-агента на базі LangGraph, який використовуватиме зовнішні інструменти, пам'ять та прийматиме рішення сам.

### Підготовка: хмарні та локальні нейромережі

Для того щоб приступити до створення ШІ-агентів, нам в першу чергу потрібен **мозок** — мовна модель. Тут є два підходи:

*   **використовувати хмарні рішення**, де все готово «з коробки»;
*   або **підняти модель локально** — для повної автономії та конфіденційності.

Розглянемо обидва варіанти.

#### Хмарні сервіси: швидко та зручно

Найпростіший шлях — скористатися потужностями великих провайдерів: OpenAI, Anthropic, та використовувати...

### Де взяти ключі та токени:

*   **OpenAI** — ChatGPT та інші продукти;
*   **Anthropic** — Claude;
*   **OpenRouter.ai** — десятки моделей (один токен — безліч моделей через OpenAI-сумісний API);
*   **Amvera Cloud** — можливість підключити LLAMA з оплатою рублями та вбудованим проксіюванням до OpenAI та Anthropic.

Цей шлях зручний, особливо якщо ви:

*   не хочете налаштовувати інфраструктуру;
*   розробляєте з акцентом на швидкість;
*   працюєте з обмеженими ресурсами.

### Локальні моделі: повний контроль

Якщо вам важлива **приватність, робота без інтернету** або ви хочете будувати **повністю автономні агенти**, то має сенс розгорнути нейромережу локально.

**Основні переваги:** 

*   **Конфіденційність** — дані залишаються у вас;
*   **Робота без інтернету** — корисно в ізольованих мережах;
*   **Відсутність підписок та токенів** — безкоштовно після налаштування.

**Недоліки очевидні:** 

*   Вимоги до ресурсів (особливо до відеопам'яті);
*   Налаштування може зайняти час;
*   Деякі моделі складно розгорнути без досвіду.

Тим не менш, є інструменти, які роблять локальний запуск простішим. Один з найкращих на сьогодні — це **Ollama**.

### Розгортання локальної LLM через Ollama + Docker

Ми підготуємо локальний запуск моделі Qwen 2.5 (qwen2.5:32b) з використанням Docker-контейнера та системи Ollama. Це дозволить інтегрувати нейромережу з MCP та використовувати її у власних агентах на базі LangGraph.

Якщо обчислювальних ресурсів вашого комп'ютера або сервера виявиться недостатньо для роботи з цією версією моделі, ви завжди можете вибрати менш "ненажерливу" нейромережу — процес встановлення та запуску залишиться аналогічним.

**Швидка установка (короткий опис кроків)**

1.  **Встановіть Docker + Docker Compose**
2.  **Створіть структуру проекту:** 
```bash
mkdir qwen-local && cd qwen-local
```
3.  **Створіть `docker-compose.yml`**
(універсальний варіант, GPU визначається автоматично)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **Запустіть контейнер:** 
```bash
docker compose up -d
```

5.  **Завантажте модель:** 
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **Перевірте роботу через API:** 
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Привіт!", "stream": false}'
```
*(Зображення з результатом виконання команди curl)*

7.  **Інтеграція з Python:** 
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Поясни квантову заплутаність"))
```
Тепер у вас повноцінна локальна LLM, готова до роботи з MCP та LangGraph.

**Що далі?**

У нас є вибір між хмарними та локальними моделями, і ми навчилися підключати обидві. Найцікавіше попереду — **створення ШІ-агентів на LangGraph**, які використовують вибрану модель, пам'ять, інструменти та власну логіку.

**Переходимо до найсмачнішого — коду та практики!**

---

Перш ніж перейти до практики, важливо підготувати робоче середовище. Я припускаю, що ви вже знайомі з основами Python, знаєте, що таке бібліотеки та залежності, і розумієте, навіщо використовувати віртуальне середовище.

Якщо все це вам в новинку — рекомендую спочатку пройти короткий курс або гайд по Python-базі, а потім повертатися до статті.

#### Крок 1: Створення віртуального середовища

Створіть нове віртуальне середовище в папці проекту:
```bash
python -m venv venv
source venv/bin/activate  # для Linux/macOS
virtualenv\Scripts\activate   # для Windows
```

#### Крок 2: Встановлення залежностей

Створіть файл `requirements.txt` та додайте до нього наступні рядки:
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ⚠️ **Актуальні версії вказані на 21 липня 2025 року.** З моменту публікації вони могли змінитися — **перевіряйте актуальність перед встановленням.**

Потім встановіть залежності:
```bash
pip install -r requirements.txt```

#### Крок 3: Конфігурація змінних середовища

Створіть у корені проекту файл `.env` та додайте до нього потрібні API-ключі:
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Призначення змінних:**

*   **OPENAI_API_KEY** — ключ для доступу до GPT-моделей від OpenAI;
*   **DEEPSEEK_API_KEY** — ключ для використання моделей Deepseek;
*   **OPENROUTER_API_KEY** — єдиний ключ для доступу до безлічі моделей через OpenRouter

---
Деякі MCP-інструменти (наприклад, `brave-web-search`) вимагають ключ для роботи. Без нього вони просто не активуються.

**А якщо у вас немає API-ключів?**

Не проблема. Ви можете почати розробку з локальною моделлю (наприклад, через Ollama), не підключаючи жодного зовнішнього сервісу. У цьому випадку файл `.env` можна не створювати взагалі.

Готово! Тепер у нас є все необхідне для початку — ізольоване середовище, залежності, і, за необхідності, доступ до хмарних нейромереж та MCP-інтеграцій.

Далі - запустимо нашого LLM-агента різними способами.

### Простий запуск LLM-агентів через LangGraph: базова інтеграція

Почнемо з найпростішого: як «підключити мозок» до майбутнього агента. Ми розберемо базові способи запуску мовних моделей (LLM) за допомогою LangChain, щоб у наступному кроці перейти до інтеграції з LangGraph та побудови повноцінного ШІ-агента.

#### Імпорти
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` та `load_dotenv()` — для завантаження змінних з `.env`-файлу.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` — обгортки для підключення мовних моделей через LangChain.

> 💡 Якщо ви використовуєте альтернативні підходи до роботи з конфігураціями (наприклад, Pydantic Settings), можете замінити `load_dotenv()` на свій звичний спосіб.

#### Завантаження змінних середовища
```python
load_dotenv()
```
Це завантажить усі змінні з `.env`, включаючи ключі для доступу до API OpenAI, DeepSeek, OpenRouter та інших.

#### Прості функції для отримання LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
Якщо змінна `OPENAI_API_KEY` коректно задана, LangChain підставить її автоматично — явне вказання `api_key=...` тут є необов'язковим.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
Аналогічно, але використовуємо обгортку `ChatDeepSeek`.

**OpenRouter (та інші сумісні API)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Особливості:**

*   `ChatOpenAI` використовується, незважаючи на те, що модель не від OpenAI — тому що OpenRouter використовує той самий протокол.
*   `base_url` є обов'язковим: він вказує на OpenRouter API.
*   Модель `moonshotai/kimi-k2:free` була обрана як один з найбільш збалансованих варіантів за якістю та швидкістю на момент написання статті.
*   API-ключ `OpenRouter` потрібно передавати явно — автоматична підстановка тут не працює.

#### Міні-тест: перевірка роботи моделі
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Хто ти?")
    print(response.content)
```
*(Зображення з результатом виконання: `Я - ШІ-асистент, створений компанією Moonshot AI...`)*

Якщо все налаштовано правильно, ви отримаєте осмислену відповідь від моделі. Вітаю — перший крок зроблено!

### Але це ще не агент

На поточному етапі ми підключили LLM та зробили простий виклик. Це більше схоже на консольного чат-бота, ніж на ШІ-агента.

**Чому?**

*   Ми пишемо **синхронний, лінійний код** без логіки стану або цілей.
*   Агент не приймає рішень, не запам'ятовує контекст і не використовує інструменти.
*   MCP та LangGraph поки не задіяні.

**Що далі?**

Далі ми реалізуємо **повноцінного ШІ-агента** з використанням **LangGraph** — спочатку без MCP, щоб сфокусуватися на архітектурі, станах та логіці самого агента.

Поринаємо в справжню агентну механіку. Поїхали!

### Агент класифікації вакансій: від теорії до практики

...концепції LangGraph на практиці та створити корисний інструмент для HR-платформ та бірж фрілансу.

#### Завдання агента

Наш агент приймає на вхід текстовий опис вакансії або послуги та виконує трирівневу класифікацію:

1.  **Тип роботи**: проектна робота або постійна вакансія
2.  **Категорія професії**: з 45+ заздалегідь визначених спеціальностей
3.  **Тип пошуку**: чи шукає людина роботу або шукає виконавця

Результат повертається в структурованому JSON-форматі з оцінкою впевненості для кожної класифікації.

#### 📈 Архітектура агента на LangGraph

Дотримуючись принципів LangGraph, створюємо **граф станів** з чотирьох вузлів:

- Вхідний опис
- ↓
- Вузол класифікації типу роботи
- ↓
- Вузол класифікації категорії
- ↓
- Вузол визначення типу пошуку
- ↓
- Вузол розрахунку впевненості
- ↓
- JSON-результат

Кожен вузол — це **спеціалізована функція**, яка:

*   Отримує поточний стан агента
*   Виконує свою частину аналізу
*   Оновлює стан та передає його далі

#### Керування станом

Визначаємо **структуру пам'яті агента** через `TypedDict`:

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """Стан агента для зберігання інформації про процес класифікації"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

Це **робоча пам'ять агента** — все, що він пам'ятає та накопичує в процесі аналізу. Подібно до того, як людина-експерт тримає в голові контекст завдання під час аналізу документа.

Давайте розглянемо повний код, а після сконцентруємося на основних моментах.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Категорії професій
CATEGORIES = [
    "2D-аніматор", "3D-аніматор", "3D-моделлер",
    "Бізнес-аналітик", "Блокчейн-розробник", ...
]

class JobType(Enum):
    PROJECT = "проектна робота"
    PERMANENT = "постійна робота"

class SearchType(Enum):
    LOOKING_FOR_WORK = "пошук роботи"
    LOOKING_FOR_PERFORMER = "пошук виконавця"

class State(TypedDict):
    """Стан агента для зберігання інформації про процес класифікації"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Асинхронний агент для класифікації вакансій та послуг"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Ініціалізація агента"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Створює робочий процес агента на основі LangGraph"""
        workflow = StateGraph(State)
        
        # Додаємо вузли в граф
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # Визначаємо послідовність виконання вузлів
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """Вузол для визначення типу роботи: проектна або постійна"""
        # ... (implementation follows)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """Вузол для визначення категорії професії"""
        # ... (implementation follows)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """Вузол для визначення типу пошуку"""
        # ... (implementation follows)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """Вузол для розрахунку рівня впевненості в класифікації"""
        # ... (implementation follows)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Знаходить найбільш схожу категорію зі списку доступних"""
        # ... (implementation follows)

    async def classify(self, description: str) -> Dict[str, Any]:
        """Основний метод для класифікації вакансії/послуги"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # Запускаємо робочий процес
        result = await self.workflow.ainvoke(initial_state)
        
        # Формуємо підсумкову відповідь у форматі JSON
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Демонстрація роботи агента"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "Потрібен Python розробник для створення веб-додатку на Django. Постійна робота.",
        "Шукаю замовлення на створення логотипів та фірмового стилю. Працюю в Adobe Illustrator.",
        "Потрібен 3D-аніматор для короткострокового проекту створення рекламного ролика.",
        "Резюме: досвідчений маркетолог, шукаю віддалену роботу в сфері digital-маркетингу",
        "Шукаємо фронтенд-розробника React в нашу команду на постійну основу"
    ]
    
    print("🤖 Демонстрація роботи агента класифікації вакансій\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Тест {i}: ---")
        print(f"Опис: {description}")
        try:
            result = await agent.classify(description)
            print("Результат класифікації:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"❌ Помилка: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...решта коду з реалізацією методів була представлена в статті...)*

### Ключові переваги архітектури
1.  **Модульність** — кожен вузол вирішує одне завдання, легко тестувати та покращувати окремо
2.  **Розширюваність** — можна додавати нові вузли аналізу без зміни існуючих
3.  **Прозорість** — весь процес прийняття рішень документований та відстежуваний
4.  **Продуктивність** — асинхронна обробка множинних запитів
5.  **Надійність** — вбудовані механізми відкату та обробка помилок

### Реальна користь
Такий агент може використовуватися в:
*   **HR-платформах** для автоматичної категоризації резюме та вакансій
*   **Біржах фрілансу** для покращення пошуку та рекомендацій
*   **Внутрішніх системах** компаній для обробки заявок та проектів
*   **Аналітичних рішеннях** для дослідження ринку праці

### MCP в дії: створюємо агента з файловою системою та веб-пошуком
Після того як ми розібралися з базовими принципами LangGraph та створили простого агента-класифікатора, давайте розширимо його можливості, підключивши до зовнішнього світу через MCP.

Зараз ми створимо повноцінного ШІ-помічника, який зможе:
*   Працювати з файловою системою (читати, створювати, змінювати файли)
*   Шукати актуальну інформацію в інтернеті
*   Запам'ятовувати контекст діалогу
*   Обробляти помилки та відновлюватися після збоїв

#### Від теорії до реальних інструментів
Пам'ятаєте, як на початку статті ми говорили про те, що **MCP — це міст між нейромережею та її оточенням**? Зараз ви побачите це на практиці. Наш агент отримає доступ до **реальних інструментів**:
```
# Інструменти файлової системи
- read_file — читання файлів
- write_file — запис та створення файлів
- list_directory — перегляд вмісту папок
- create_directory — створення папок

# Інструменти веб-пошуку
- brave_web_search — пошук в інтернеті
- get_web_content — отримання вмісту сторінок
```
Це вже не «іграшковий» агент — це **робочий інструмент**, який може вирішувати реальні завдання.

#### 📈 Архітектура: від простого до складного

**1. Конфігурація як основа стабільності**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Спрощена конфігурація ШІ-агента"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Валідація конфігурації"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Шлях не існує: {self.filesystem_path}")
```
**Чому це важливо?** На відміну від прикладу з класифікацією, тут агент взаємодіє із зовнішніми системами. Одна помилка в шляху до файлів або відсутній API-ключ — і весь агент перестає працювати. **Валідація на старті** економить години налагодження.

**2. Фабрика моделей: гнучкість вибору**
```python
def create_model(config: AgentConfig):
    """Створює модель відповідно до конфігурації"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... інші провайдери
```
Один код — безліч моделей. Хочете безкоштовну локальну модель? Використовуйте Ollama. Потрібна максимальна точність? Переключіться на GPT-4. Важлива швидкість? Спробуйте DeepSeek. Код залишається тим самим.

**3. MCP-інтеграція: підключення до реального світу**
```python
async def _init_mcp_client(self):
    """Ініціалізація MCP клієнта"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
Тут відбувається ключова робота MCP: ми підключаємо до агента зовнішні MCP-сервери, які надають набір інструментів та функцій. Агент при цьому отримує не просто окремі функції, а повноцінне контекстне розуміння того, як працювати з файловою системою та інтернетом.

#### Стійкість до помилок
У реальному світі все ламається: мережа недоступна, файли заблоковані, API-ключі прострочені. Наш агент готовий до цього:
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
Декоратор `@retry_on_failure` автоматично повторює операції при тимчасових збоях. Користувач навіть не помітить, що щось пішло не так.

### Підсумки: від теорії до практики ШІ-агентів

Сьогодні ми пройшли шлях від базових концепцій до створення працюючих ШІ-агентів. Давайте підіб'ємо підсумки того, що ми вивчили та чого досягли.

**Що ми освоїли**

**1. Фундаментальні концепції**
*   Розібралися з відмінністю між чат-ботами та справжніми ШІ-агентами
*   Зрозуміли роль **MCP (Model Context Protocol)** як моста між моделлю та зовнішнім світом
*   Вивчили архітектуру **LangGraph** для побудови складної логіки агентів

**2. Практичні навички**
*   Налаштували робоче середовище з підтримкою хмарних та локальних моделей
*   Створили **агента-класифікатора** з асинхронною архітектурою та керуванням станами
*   Побудували **MCP-агента** з доступом до файлової системи та веб-пошуку

**3. Архітектурні патерни**
*   Освоїли модульну конфігурацію та фабрики моделей
*   Впровадили обробку помилок та **retry-механізми** для продакшн-готових рішень

### Ключові переваги підходу
**LangGraph + MCP** дають нам:
*   **Прозорість** — кожен крок агента документований та відстежуваний
*   **Розширюваність** — нові можливості додаються декларативно
*   **Надійність** — вбудована обробка помилок та відновлення
*   **Гнучкість** — підтримка безлічі моделей та провайдерів з коробки

### Висновок

ШІ-агенти — це не футуристична фантастика, а **реальна технологія сьогодення**. За допомогою LangGraph та MCP ми можемо створювати системи, які вирішують конкретні бізнес-завдання, автоматизують рутину та відкривають нові можливості.

**Головне — почати.** Візьміть код з прикладів, адаптуйте під свої завдання, експериментуйте. Кожен проект — це новий досвід та крок до майстерності в області ШІ-розробки.

Удачі у ваших проектах!

---
*Теги: python, ші, mcp, langchain, ші-асистент, ollama, ші-агенти, local llm, langgraph, mcp-server*
*Хаби: Блог компанії Amvera, Обробка природної мови, Штучний інтелект, Python, Програмування*
