<h1>Як навчити нейромережу працювати руками: створення повноцінного ШІ-агента з MCP та LangGraph за годину</h1>
<p>Друзі, вітаю! Сподіваюся, встигли скучити.</p>
<p>Останні пару місяців я з головою поринув у дослідження інтеграції ШІ-агентів у власні Python-проекти. У процесі накопичилося чимало практичних знань та спостережень, якими просто гріх не поділитися. Тому сьогодні я повертаюся на Хабр — з новою темою, свіжим поглядом та з наміром писати частіше.</p>
<p>На порядку денному — LangGraph та MCP: інструменти, за допомогою яких можна створювати дійсно корисних ШІ-агентів.</p>
<p>Якщо раніше ми сперечалися про те, яка нейромережа краще відповідає російською мовою, то сьогодні поле битви змістилося в бік більш прикладних завдань: хто краще справляється з роллю ШІ-агента? Які фреймворки дійсно спрощують розробку? І як інтегрувати все це добро в реальний проект?</p>
<p>Але перш ніж зануритися в практику та код, давайте розберемося з базовими поняттями. Особливо з двома ключовими: <strong>ШІ-агенти та MCP</strong>. Без них розмова про LangGraph буде неповною.</p>
<h3>ШІ-агенти простими словами</h3>
<p>ШІ-агенти — це не просто «прокачані» чат-боти. Вони являють собою більш складні, автономні сутності, які володіють двома найважливішими особливостями:</p>
<ol>
<li><strong>Вміння взаємодіяти та координуватися</strong></li>
</ol>
<p>Сучасні агенти здатні ділити завдання на підзавдання, викликати інших агентів, запитувати зовнішні дані, працювати в команді. Це вже не одиночний асистент, а розподілена система, де кожен компонент може вносити свій внесок.</p>
<ol start="2">
<li><strong>Доступ до зовнішніх ресурсів</strong></li>
</ol>
<p>ШІ-агент більше не обмежений рамками діалогу. Він може звертатися до баз даних, виконувати виклики до API, взаємодіяти з локальними файлами, векторними сховищами знань і навіть запускати команди в терміналі. Все це стало можливим завдяки появі MCP — нового рівня інтеграції між моделлю та середовищем.</p>
<hr>
<p>Якщо говорити просто: <strong>MCP — це міст між нейромережею та її оточенням</strong>. Він дозволяє моделі «розуміти» контекст завдання, отримувати доступ до даних, виконувати виклики та формувати обґрунтовані дії, а не просто видавати текстові відповіді.</p>
<p><strong>Уявімо аналогію:</strong></p>
<ul>
<li>У вас є <strong>нейромережа</strong> — вона вміє міркувати та генерувати тексти.</li>
<li>Є <strong>дані та інструменти</strong> — документи, API, бази знань, термінал, код.</li>
<li>І є <strong>MCP</strong> — це інтерфейс, який дозволяє моделі взаємодіяти з цими зовнішніми джерелами так, ніби вони були частиною її внутрішнього світу.</li>
</ul>
<p><strong>Без MCP:</strong></p>
<p>Модель — це ізольований діалоговий движок. Ви подаєте їй текст — вона відповідає. І все.</p>
<p><strong>З MCP:</strong></p>
<p>Модель стає повноцінним <strong>виконавцем завдань</strong>:</p>
<ul>
<li>отримує доступ до структур даних та API;</li>
<li>виконує зовнішні функції;</li>
<li>орієнтується в поточному стані проекту або програми;</li>
<li>може запам'ятовувати, відстежувати та змінювати контекст під час діалогу;</li>
<li>використовує розширення, такі як інструменти пошуку, код-раннери, базу векторних ембеддінгів тощо.</li>
</ul>
<p>У технічному сенсі <strong>MCP — це протокол взаємодії між LLM та її оточенням</strong>, де контекст подається у вигляді структурованих об'єктів (замість «сирого» тексту), а виклики оформлюються як інтерактивні операції (наприклад, function calling, tool usage або agent actions). Саме це і перетворює звичайну модель на <strong>справжнього ШІ-агента</strong>, здатного робити більше, ніж просто "поговорити".</p>
<h3>А тепер — до справи!</h3>
<p>Тепер, коли ми розібралися з базовими поняттями, логічно поставити запитання: «Як все це реалізувати на практиці в Python?»</p>
<p>Ось тут і вступає в гру <strong>LangGraph</strong> — потужний фреймворк для побудови графів станів, поведінки агентів та ланцюжків мислення. Він дозволяє "прошивати" логіку взаємодії між агентами, інструментами та користувачем, створюючи живу архітектуру ШІ, що адаптується до завдань.</p>
<p>У наступних розділах ми подивимося, як:</p>
<ul>
<li>будується агент з нуля;</li>
<li>створюються стани, переходи та події;</li>
<li>інтегруються функції та інструменти;</li>
<li>і як вся ця екосистема працює в реальному проекті.</li>
</ul>
<h3>Трохи теорії: що таке LangGraph</h3>
<p>Перш ніж приступити до практики, потрібно сказати пару слів про сам фреймворк.</p>
<p><strong>LangGraph</strong> — це проект від команди <strong>LangChain</strong>, тих самих, хто першими запропонували концепцію «ланцюжків» (chains) взаємодії з LLM. Якщо раніше основний акцент робився на лінійні або умовно-розгалужені пайплайни (langchain.chains), то тепер розробники роблять ставку на <strong>графову модель</strong>, і саме LangGraph вони рекомендують як нове «ядро» для побудови складних ШІ-систем.</p>
<p><strong>LangGraph</strong> — це фреймворк для побудови кінцевих автоматів та графів станів, у яких кожен <strong>вузол</strong> представляє собою частину логіки агента: виклик моделі, зовнішній інструмент, умова, введення користувача тощо.</p>
<h3>Як це працює: графи та вузли</h3>
<p>Концептуально, LangGraph будується на наступних ідеях:</p>
<ul>
<li><strong>Граф</strong> — це структура, яка описує можливі шляхи виконання логіки. Можна думати про нього як про карту: з однієї точки можна перейти в іншу залежно від умов або результату виконання.</li>
<li><strong>Вузли</strong> — це конкретні кроки всередині графа. Кожен вузол виконує якусь функцію: викликає модель, викликає зовнішній API, перевіряє умову або просто оновлює внутрішній стан.</li>
<li><strong>Переходи між вузлами</strong> — це логіка маршрутизації: якщо результат попереднього кроку такий-то, то йдемо туди-то.</li>
<li><strong>Стан</strong> — передається між вузлами та накопичує все, що потрібно: історію, проміжні висновки, введення користувача, результат роботи інструментів тощо.</li>
</ul>
<p>Таким чином, ми отримуємо <strong>гнучкий механізм керування логікою агента</strong>, в якому можна описувати як прості, так і дуже складні сценарії: цикли, умови, паралельні дії, вкладені виклики та багато іншого.</p>
<h3>Чому це зручно?</h3>
<p>LangGraph дозволяє будувати <strong>прозору, відтворювану та розширювану логіку</strong>:</p>
<ul>
<li>легко налагоджувати;</li>
<li>легко візуалізувати;</li>
<li>легко масштабувати під нові завдання;</li>
<li>легко інтегрувати зовнішні інструменти та протоколи MCP.</li>
</ul>
<p>По суті, LangGraph — це <strong>«мозок» агента</strong>, де кожен крок задокументований, контрольований і може бути змінений без хаосу та «магії».</p>
<h3>Ну а тепер — досить теорії!</h3>
<p>Можна ще довго розповідати про графи, стани, композицію логіки та переваги LangGraph над класичними пайплайнами. Але, як показує практика, краще один раз побачити в коді.</p>
<p><strong>Час перейти до практики.</strong> Далі — приклад на Python: створимо простого, але корисного ШІ-агента на базі LangGraph, який використовуватиме зовнішні інструменти, пам'ять та прийматиме рішення сам.</p>
<h3>Підготовка: хмарні та локальні нейромережі</h3>
<p>Для того щоб приступити до створення ШІ-агентів, нам в першу чергу потрібен <strong>мозок</strong> — мовна модель. Тут є два підходи:</p>
<ul>
<li><strong>використовувати хмарні рішення</strong>, де все готово «з коробки»;</li>
<li>або <strong>підняти модель локально</strong> — для повної автономії та конфіденційності.</li>
</ul>
<p>Розглянемо обидва варіанти.</p>
<h4>Хмарні сервіси: швидко та зручно</h4>
<p>Найпростіший шлях — скористатися потужностями великих провайдерів: OpenAI, Anthropic, та використовувати...</p>
<h3>Де взяти ключі та токени:</h3>
<ul>
<li><strong>OpenAI</strong> — ChatGPT та інші продукти;</li>
<li><strong>Anthropic</strong> — Claude;</li>
<li><strong>OpenRouter.ai</strong> — десятки моделей (один токен — безліч моделей через OpenAI-сумісний API);</li>
<li><strong>Amvera Cloud</strong> — можливість підключити LLAMA з оплатою рублями та вбудованим проксіюванням до OpenAI та Anthropic.</li>
</ul>
<p>Цей шлях зручний, особливо якщо ви:</p>
<ul>
<li>не хочете налаштовувати інфраструктуру;</li>
<li>розробляєте з акцентом на швидкість;</li>
<li>працюєте з обмеженими ресурсами.</li>
</ul>
<h4>Локальні моделі: повний контроль</h4>
<p>Якщо вам важлива <strong>приватність, робота без інтернету</strong> або ви хочете будувати <strong>повністю автономні агенти</strong>, то має сенс розгорнути нейромережу локально.</p>
<p><strong>Основні переваги:</strong></p>
<ul>
<li><strong>Конфіденційність</strong> — дані залишаються у вас;</li>
<li><strong>Робота без інтернету</strong> — корисно в ізольованих мережах;</li>
<li><strong>Відсутність підписок та токенів</strong> — безкоштовно після налаштування.</li>
</ul>
<p><strong>Недоліки очевидні:</strong></p>
<ul>
<li>Вимоги до ресурсів (особливо до відеопам'яті);</li>
<li>Налаштування може зайняти час;</li>
<li>Деякі моделі складно розгорнути без досвіду.</li>
</ul>
<p>Тим не менш, є інструменти, які роблять локальний запуск простішим. Один з найкращих на сьогодні — це <strong>Ollama</strong>.</p>
<h3>Розгортання локальної LLM через Ollama + Docker</h3>
<p>Ми підготуємо локальний запуск моделі Qwen 2.5 (qwen2.5:32b) з використанням Docker-контейнера та системи Ollama. Це дозволить інтегрувати нейромережу з MCP та використовувати її у власних агентах на базі LangGraph.</p>
<p>Якщо обчислювальних ресурсів вашого комп'ютера або сервера виявиться недостатньо для роботи з даною версією моделі, ви завжди можете вибрати менш "ненажерливу" нейромережу — процес встановлення та запуску залишиться аналогічним.</p>
<p><strong>Швидка установка (зведення кроків)</strong></p>
<ol>
<li><strong>Встановіть Docker + Docker Compose</strong></li>
<li><strong>Створіть структуру проекту:</strong>
<pre class="line-numbers"><code class="language-bash">mkdir qwen-local && cd qwen-local
</code></pre>
</li>
<li><strong>Створіть <code>docker-compose.yml</code></strong>
(універсальний варіант, GPU визначається автоматично)
<pre class="line-numbers"><code class="language-yaml">services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
</code></pre>
</li>
<li><strong>Запустіть контейнер:</strong>
<pre class="line-numbers"><code class="language-bash">docker compose up -d
</code></pre>
</li>
<li><strong>Завантажте модель:</strong>
<pre class="line-numbers"><code class="language-bash">docker exec -it ollama_qwen ollama pull qwen2.5:32b
</code></pre>
</li>
<li><strong>Перевірте роботу через API:</strong>
<pre class="line-numbers"><code class="language-bash">curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Привіт!", "stream": false}'
</code></pre>
<p>(Зображення з результатом виконання команди curl)</p>
</li>
<li><strong>Інтеграція з Python:</strong>
<pre class="line-numbers"><code class="language-python">import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={
        "model": "qwen2.5:32b",
        "prompt": prompt,
        "stream": False
    })
    return res.json()['response']

print(query("Поясни квантову заплутаність"))
</code></pre>
<p>Тепер у вас повноцінна локальна LLM, готова до роботи з MCP та LangGraph.</p>
</li>
</ol>
<p><strong>Що далі?</strong></p>
<p>У нас є вибір між хмарними та локальними моделями, і ми навчилися підключати обидві. Найцікавіше попереду — <strong>створення ШІ-агентів на LangGraph</strong>, які використовуватимуть обрану модель, пам'ять, інструменти та власну логіку.</p>
<p><strong>Переходимо до найсмачнішого — коду та практики!</strong></p>
<hr>
<p>Перш ніж перейти до практики, важливо підготувати робоче середовище. Я припускаю, що ви вже знайомі з основами Python, знаєте, що таке бібліотеки та залежності, і розумієте, навіщо використовувати віртуальне середовище.</p>
<p>Якщо все це вам в новинку — рекомендую спочатку пройти короткий курс або гайд з Python-бази, а потім повертатися до статті.</p>
<h4>Крок 1: Створення віртуального середовища</h4>
<p>Створіть нове віртуальне середовище в папці проекту:</p>
<pre class="line-numbers"><code class="language-bash">python -m venv venv
source venv/bin/activate  # для Linux/macOS
venc\Scripts\activate   # для Windows
</code></pre>
<h4>Крок 2: Встановлення залежностей</h4>
<p>Створіть файл <code>requirements.txt</code> та додайте до нього наступні рядки:</p>
<pre class="line-numbers"><code class="language-text">langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
</code></pre>
<blockquote>
<p>⚠️ <strong>Актуальні версії вказані станом на 21 липня 2025 року.</strong> З моменту публікації вони могли змінитися — <strong>перевіряйте актуальність перед встановленням.</strong></p>
</blockquote>
<p>Потім встановіть залежності:</p>
<pre class="line-numbers"><code class="language-bash">pip install -r requirements.txt</code></pre>
<h4>Крок 3: Конфігурація змінних середовища</h4>
<p>Створіть у корені проекту файл <code>.env</code> та додайте до нього потрібні API-ключі:</p>
<pre class="line-numbers"><code class="language-text">OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
</code></pre>
<p><strong>Призначення змінних:</strong></p>
<ul>
<li><strong>OPENAI_API_KEY</strong> — ключ для доступу до GPT-моделей від OpenAI;</li>
<li><strong>DEEPSEEK_API_KEY</strong> — ключ для використання моделей Deepseek;</li>
<li><strong>OPENROUTER_API_KEY</strong> — єдиний ключ для доступу до безлічі моделей через OpenRouter</li>
</ul>
<hr>
<p>Деякі MCP-інструменти (наприклад, <code>brave-web-search</code>) вимагають ключ для роботи. Без нього вони просто не активуються.</p>
<p><strong>А якщо у вас немає API-ключів?</strong></p>
<p>Не проблема. Ви можете почати розробку з локальною моделлю (наприклад, через Ollama), не підключаючи жодного зовнішнього сервісу. У цьому випадку файл <code>.env</code> можна не створювати зовсім.</p>
<p>Готово! Тепер у нас є все необхідне для початку — ізольоване середовище, залежності, і, за необхідності, доступ до хмарних нейромереж та MCP-інтеграцій.</p>
<p>Далі - запустимо нашого LLM-агента різними способами.</p>
<h3>Простий запуск LLM-агентів через LangGraph: базова інтеграція</h3>
<p>Почнемо з найпростішого: як «підключити мозок» до майбутнього агента. Ми розберемо базові способи запуску мовних моделей (LLM) за допомогою LangChain, щоб у наступному кроці перейти до інтеграції з LangGraph та побудови повноцінного ШІ-агента.</p>
<h4>Імпорти</h4>
<pre class="line-numbers"><code class="language-python">import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
</code></pre>
<ul>
<li><code>os</code> та <code>load_dotenv()</code> — для завантаження змінних з файлу <code>.env</code>.</li>
<li><code>ChatOpenAI</code>, <code>ChatOllama</code>, <code>ChatDeepSeek</code> — обгортки для підключення мовних моделей через LangChain.</li>
</ul>
<blockquote>
<p>💡 Якщо ви використовуєте альтернативні підходи до роботи з конфігураціями (наприклад, Pydantic Settings), можете замінити <code>load_dotenv()</code> на свій звичний спосіб.</p>
</blockquote>
<h4>Завантаження змінних середовища</h4>
<pre class="line-numbers"><code class="language-python">load_dotenv()
</code></pre>
<p>Це завантажить усі змінні з <code>.env</code>, включаючи ключі для доступу до API OpenAI, Deepseek, OpenRouter та інших.</p>
<h4>Прості функції для отримання LLM</h4>
<p><strong>OpenAI</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
</code></pre>
<p>Якщо змінна <code>OPENAI_API_KEY</code> коректно задана, LangChain підставить її автоматично — явне вказання <code>api_key=...</code> тут є необов'язковим.</p>
<p><strong>DeepSeek</strong></p>
<pre class="line-numbers"><code class="language-python">def get_deepseek_llm():
    # ...
</code></pre>
<p>Аналогічно, але ми використовуємо обгортку <code>ChatDeepSeek</code>.</p>
<p><strong>OpenRouter (та інші сумісні API)</strong></p>
<pre class="line-numbers"><code class="language-python">def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
</code></pre>
<p><strong>Особливості:</strong></p>
<ul>
<li><code>ChatOpenAI</code> використовується, незважаючи на те, що модель не від OpenAI — тому що OpenRouter використовує той самий протокол.</li>
<li><code>base_url</code> є обов'язковим: він вказує на API OpenRouter.</li>
<li>Модель <code>moonshotai/kimi-k2:free</code> була обрана як один з найбільш збалансованих варіантів за якістю та швидкістю на момент написання статті.</li>
<li>API-ключ <code>OpenRouter</code> потрібно передавати явно — автоматична підстановка тут не працює.</li>
</ul>
<h4>Міні-тест: перевірка роботи моделі</h4>
<pre class="line-numbers"><code class="language-python">if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Хто ти?")
    print(response.content)
</code></pre>
<p>(Зображення з результатом виконання: <code>Я - ШІ-асистент, створений компанією Moonshot AI...</code>)</p>
<p>Якщо все налаштовано правильно, ви отримаєте осмислену відповідь від моделі. Вітаю — перший крок зроблено!</p>
<h3>Але це ще не агент</h3>
<p>На поточному етапі ми підключили LLM та зробили простий виклик. Це більше схоже на консольний чат-бот, ніж на ШІ-агента.</p>
<p><strong>Чому?</strong></p>
<ul>
<li>Ми пишемо <strong>синхронний, лінійний код</strong> без логіки стану або цілей.</li>
<li>Агент не приймає рішень, не запам'ятовує контекст і не використовує інструменти.</li>
<li>MCP та LangGraph поки не задіяні.</li>
</ul>
<p><strong>Що далі?</strong></p>
<p>Далі ми реалізуємо <strong>повноцінного ШІ-агента</strong> з використанням <strong>LangGraph</strong> — спочатку без MCP, щоб зосередитися на архітектурі, станах та логіці самого агента.</p>
<p>Занурюємося в справжню агентну механіку. Поїхали!</p>
<h3>Агент класифікації вакансій: від теорії до практики</h3>
<p>...концепції LangGraph на практиці та створити корисний інструмент для HR-платформ та бірж фрілансу.</p>
<h4>Завдання агента</h4>
<p>Наш агент приймає на вхід текстовий опис вакансії або послуги та виконує трирівневу класифікацію:</p>
<ol>
<li><strong>Тип роботи</strong>: проектна робота або постійна вакансія</li>
<li><strong>Категорія професії</strong>: з 45+ заздалегідь визначених спеціальностей</li>
<li><strong>Тип пошуку</strong>: чи шукає людина роботу, чи шукає виконавця</li>
</ol>
<p>Результат повертається у структурованому JSON-форматі з оцінкою впевненості для кожної класифікації.</p>
<h4>📈 Архітектура агента на LangGraph</h4>
<p>Дотримуючись принципів LangGraph, створюємо <strong>граф станів</strong> з чотирьох вузлів:</p>
<ul>
<li>Вхідний опис</li>
<li>↓</li>
<li>Вузол класифікації типу роботи</li>
<li>↓</li>
<li>Вузол класифікації категорії</li>
<li>↓</li>
<li>Вузол визначення типу пошуку</li>
<li>↓</li>
<li>Вузол розрахунку впевненості</li>
<li>↓</li>
<li>JSON-результат</li>
</ul>
<p>Кожен вузол — це <strong>спеціалізована функція</strong>, яка:</p>
<ul>
<li>Отримує поточний стан агента</li>
<li>Виконує свою частину аналізу</li>
<li>Оновлює стан та передає його далі</li>
</ul>
<h4>Керування станом</h4>
<p>Визначаємо <strong>структуру пам'яті агента</strong> через <code>TypedDict</code>:</p>
<pre class="line-numbers"><code class="language-python">from typing import TypedDict, Dict

class State(TypedDict):
    """Стан агента для зберігання інформації про процес класифікації"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
</code></pre>
<p>Це <strong>робоча пам'ять агента</strong> — все, що він пам'ятає та накопичує в процесі аналізу. Подібно до того, як людина-експерт тримає в голові контекст завдання під час аналізу документа.</p>
<p>Давайте розглянемо повний код, а після зосередимося на основних моментах.</p>
<pre class="line-numbers"><code class="language-python">import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Категорії професій
CATEGORIES = [
    "2D-аніматор", "3D-аніматор", "3D-моделлер",
    "Бізнес-аналітик", "Блокчейн-розробник", ...
]

class JobType(Enum):
    PROJECT = "проектна робота"
    PERMANENT = "постійна робота"

class SearchType(Enum):
    LOOKING_FOR_WORK = "пошук роботи"
    LOOKING_FOR_PERFORMER = "пошук виконавця"

class State(TypedDict):
    """Стан агента для зберігання інформації про процес класифікації"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """Асинхронний агент для класифікації вакансій та послуг"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """Ініціалізація агента"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """Створює робочий процес агента на основі LangGraph"""
        workflow = StateGraph(State)

        # Додаємо вузли в граф
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)

        # Визначаємо послідовність виконання вузлів
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)

        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """Вузол для визначення типу роботи: проектна або постійна"""
        # ... (implementation follows)

    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """Вузол для визначення категорії професії"""
        # ... (implementation follows)

    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """Вузол для визначення типу пошуку"""
        # ... (implementation follows)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """Вузол для розрахунку рівня впевненості в класифікації"""
        # ... (implementation follows)

    def _find_closest_category(self, predicted_category: str) -> str:
        """Знаходить найближчу категорію зі списку доступних"""
        # ... (implementation follows)

    async def classify(self, description: str) -> Dict[str, Any]:
        """Основний метод для класифікації вакансії/послуги"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }

        # Запускаємо робочий процес
        result = await self.workflow.ainvoke(initial_state)

        # Формуємо підсумкову відповідь у форматі JSON
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """Демонстрація роботи агента"""
    agent = VacancyClassificationAgent()

    test_cases = [
        "Потрібен Python розробник для створення веб-додатку на Django. Постійна робота.",
        "Шукаю замовлення на створення логотипів та фірмового стилю. Працюю в Adobe Illustrator.",
        "Потрібен 3D-аніматор для короткострокового проекту створення рекламного ролика.",
        "Резюме: досвідчений маркетолог, шукаю віддалену роботу в сфері digital-маркетингу",
        "Шукаємо фронтенд-розробника React у нашу команду на постійній основі"
    ]

    print("🤖 Демонстрація роботи агента класифікації вакансій\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- Тест {i}: ---")
        print(f"Опис: {description}")
        try:
            result = await agent.classify(description)
            print("Результат класифікації:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"❌ Помилка: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

</code></pre>
<p>(...решта коду з реалізацією методів була представлена в статті...)</p>
<h3>Ключові переваги архітектури</h3>
<ol>
<li><strong>Модульність</strong> — кожен вузол вирішує одне завдання, легко тестувати та покращувати окремо</li>
<li><strong>Розширюваність</strong> — можна додавати нові вузли аналізу без зміни існуючих</li>
<li><strong>Прозорість</strong> — весь процес прийняття рішень задокументований та відстежуваний</li>
<li><strong>Продуктивність</strong> — асинхронна обробка множинних запитів</li>
<li><strong>Надійність</strong> — вбудовані механізми відкату та обробка помилок</li>
</ol>
<h3>Реальна користь</h3>
<p>Такий агент може використовуватися в:</p>
<ul>
<li><strong>HR-платформах</strong> для автоматичної категоризації резюме та вакансій</li>
<li><strong>Біржах фрілансу</strong> для покращення пошуку та рекомендацій</li>
<li><strong>Внутрішніх системах</strong> компаній для обробки заявок та проектів</li>
<li><strong>Аналітичних рішеннях</strong> для дослідження ринку праці</li>
</ul>
<h3>MCP в дії: створюємо агента з файловою системою та веб-пошуком</h3>
<p>Після того як ми розібралися з базовими принципами LangGraph та створили простого агента-класифікатора, давайте розширимо його можливості, підключивши до зовнішнього світу через MCP.</p>
<p>Зараз ми створимо повноцінного ШІ-помічника, який зможе:</p>
<ul>
<li>Працювати з файловою системою (читати, створювати, змінювати файли)</li>
<li>Шукати актуальну інформацію в інтернеті</li>
<li>Запам'ятовувати контекст діалогу</li>
<li>Обробляти помилки та відновлюватися після збоїв</li>
</ul>
<h4>Від теорії до реальних інструментів</h4>
<p>Пам'ятаєте, як на початку статті ми говорили про те, що <strong>MCP — це міст між нейромережею та її оточенням</strong>? Зараз ви побачите це на практиці. Наш агент отримає доступ до <strong>реальних інструментів</strong>:</p>
<pre class="line-numbers"><code class="language-text"># Інструменти файлової системи
- read_file — читання файлів
- write_file — запис та створення файлів
- list_directory — перегляд вмісту папок
- create_directory — створення папок

# Інструменти веб-пошуку
- brave_web_search — пошук в інтернеті
- get_web_content — отримання вмісту сторінок
</code></pre>
<p>Це вже не «іграшковий» агент — це <strong>робочий інструмент</strong>, який може вирішувати реальні завдання.</p>
<h4>📈 Архітектура: від простого до складного</h4>
<p><strong>1. Конфігурація як основа стабільності</strong></p>
<pre class="line-numbers"><code class="language-python">from dataclasses import dataclass

@dataclass
class AgentConfig:
    """Спрощена конфігурація ШІ-агента"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """Валідація конфігурації"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"Шлях не існує: {self.filesystem_path}")
</code></pre>
<p><strong>Чому це важливо?</strong> На відміну від прикладу з класифікацією, тут агент взаємодіє із зовнішніми системами. Одна помилка в шляху до файлів або відсутній API-ключ — і весь агент перестає працювати. <strong>Валідація на старті</strong> економить години налагодження.</p>
<p><strong>2. Фабрика моделей: гнучкість вибору</strong></p>
<pre class="line-numbers"><code class="language-python">def create_model(config: AgentConfig):
    """Створює модель згідно з конфігурацією"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... інші провайдери
</code></pre>
<p>Один код — безліч моделей. Хочете безкоштовну локальну модель? Використовуйте Ollama. Потрібна максимальна точність? Переключіться на GPT-4. Важлива швидкість? Спробуйте DeepSeek. Код залишається тим самим.</p>
<p><strong>3. MCP-інтеграція: підключення до реального світу</strong></p>
<pre class="line-numbers"><code class="language-python">async def _init_mcp_client(self):
    """Ініціалізація MCP клієнта"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
</code></pre>
<p>Тут відбувається ключова робота MCP: ми підключаємо до агента зовнішні MCP-сервери, які надають набір інструментів та функцій. Агент при цьому отримує не просто окремі функції, а повноцінне контекстне розуміння того, як працювати з файловою системою та інтернетом.</p>
<h4>Стійкість до помилок</h4>
<p>У реальному світі все ламається: мережа недоступна, файли заблоковані, API-ключі прострочені. Наш агент готовий до цього:</p>
<pre class="line-numbers"><code class="language-python">@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
</code></pre>
<p>Декоратор <code>@retry_on_failure</code> автоматично повторює операції при тимчасових збоях. Користувач навіть не помітить, що щось пішло не так.</p>
<h3>Підсумки: від теорії до практики ШІ-агентів</h3>
<p>Сьогодні ми пройшли шлях від базових концепцій до створення працюючих ШІ-агентів. Давайте підіб'ємо підсумки того, що ми вивчили та чого досягли.</p>
<p><strong>Що ми освоїли</strong></p>
<p><strong>1. Фундаментальні концепції</strong></p>
<ul>
<li>Розібралися з відмінністю між чат-ботами та справжніми ШІ-агентами</li>
<li>Зрозуміли роль <strong>MCP (Model Context Protocol)</strong> як мосту між моделлю та зовнішнім світом</li>
<li>Вивчили архітектуру <strong>LangGraph</strong> для побудови складної логіки агентів</li>
</ul>
<p><strong>2. Практичні навички</strong></p>
<ul>
<li>Налаштували робоче середовище з підтримкою хмарних та локальних моделей</li>
<li>Створили <strong>агента-класифікатора</strong> з асинхронною архітектурою та керуванням станами</li>
<li>Побудували <strong>MCP-агента</strong> з доступом до файлової системи та веб-пошуку</li>
</ul>
<p><strong>3. Архітектурні патерни</strong></p>
<ul>
<li>Освоїли модульну конфігурацію та фабрики моделей</li>
<li>Впровадили обробку помилок та <strong>механізми повторних спроб</strong> для готових до продакшену рішень</li>
</ul>
<h3>Ключові переваги підходу</h3>
<p><strong>LangGraph + MCP</strong> дають нам:</p>
<ul>
<li><strong>Прозорість</strong> — кожен крок агента задокументований та відстежуваний</li>
<li><strong>Розширюваність</strong> — нові можливості додаються декларативно</li>
<li><strong>Надійність</strong> — вбудована обробка помилок та відновлення</li>
<li><strong>Гнучкість</strong> — підтримка безлічі моделей та провайдерів з коробки</li>
</ul>
<h3>Висновок</h3>
<p>ШІ-агенти — це не футуристична фантастика, а <strong>реальна технологія сьогодення</strong>. За допомогою LangGraph та MCP ми можемо створювати системи, які вирішують конкретні бізнес-завдання, автоматизують рутину та відкривають нові можливості.</p>
<p><strong>Головне — почати.</strong> Візьміть код з прикладів, адаптуйте під свої завдання, експериментуйте. Кожен проект — це новий досвід та крок до майстерності в галузі ШІ-розробки.</p>
<p>Успіхів у ваших проектах!</p>
<hr>
<p><em>Теги: python, ші, mcp, langchain, ші-асистент, ollama, ші-агенти, local llm, langgraph, mcp-server</em><br>
<em>Хаби: Блог компанії Amvera, Natural Language Processing, Штучний інтелект, Python, Програмування</em><br>
<img src="https://habr.com/ru/companies/amvera/articles/929568/" alt="habr"></p>