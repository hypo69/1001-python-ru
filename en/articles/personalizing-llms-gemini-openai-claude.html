<h2>Cheatsheet: Personalizing LLMs: Prompts, Fine-tuning Models, Code Examples.</h2>
<p>In this article:</p>
<ol>
<li>How the "memory effect" is created in LLMs (a brief overview).</li>
<li>Why and when model fine-tuning is necessary.</li>
<li>When fine-tuning is not the best solution.</li>
<li>Data preparation.</li>
<li>Fine-tuning examples for <strong>OpenAI (GPT)</strong>, <strong>Google (Gemini)</strong>, and <strong>Anthropic (Claude)</strong> (differs).</li>
</ol>
<h3>1. How LLMs "Remember" and "Adapt": The Illusion of Context</h3>
<p>Before talking about fine-tuning, it is important to understand how LLMs manage to create a sense of personalization in the first place.
This is important so as not to rush into expensive fine-tuning if the task can be solved in simpler ways:</p>
<ul>
<li>Through the <strong>Context Window (Short-Term Memory):</strong> Within a single dialogue, you send the model not only a new question, but also <strong>all or part of the previous correspondence</strong>. The model processes all this text as a single "context". It is thanks to this that it "remembers" previous remarks and continues the thought. The limitation here is the length of the context window (the number of tokens).</li>
<li>Creating <strong>System Prompts:</strong> You can set the model's role, tone, and rules of conduct at the beginning of each dialogue. For example: "You are a Python expert, answer briefly."</li>
<li>Including several examples of desired behavior in the query <strong>Few-Shot Learning:</strong> (input/output pairs) allows the model to "learn" this pattern directly within the current query.</li>
<li><strong>State management on the application side:</strong> The most powerful way. The application (which accesses the API) can store information about the user (preferences, history, profile data) and dynamically add it to the prompt before sending it to the model.</li>
</ul>
<h3>2. </h3>
<p>Fine-tuning is the process of further training an already prepared base LLM on your own, specific dataset. This allows the model to:</p>
<ul>
<li><strong>Adapt style and tone:</strong> The model will speak "in your language" – whether it be a strict scientific, friendly marketing, or the slang of a specific community.</li>
<li><strong>Follow specific instructions and formats:</strong> If you need answers in a strictly defined JSON structure, or always with a specific set of fields.</li>
<li><strong>Understand domain-specific language:</strong> Training on your internal documentation or industry texts will help the model better handle the terminology of your niche.</li>
<li><strong>Improve performance on narrow tasks:</strong> For certain types of queries (e.g., classifying reviews, generating code in a specific framework), fine-tuning can provide more accurate and relevant answers than the base model.</li>
<li><strong>Reduce the length of prompts:</strong> If the model already "knows" the desired behavior thanks to tuning, you do not need to remind it of this in the prompt every time, which saves tokens and reduces latency.</li>
</ul>
<h3>3. </h3>
<p>Fine-tuning is a powerful, but not universal, tool. You should not use it if:</p>
<ul>
<li><strong>The model needs to access new knowledge:</strong> Fine-tuning changes the model's weights, but does not "load" new facts into it in real time. If your task is to answer questions based on a constantly changing knowledge base (company documents, latest news), it is better to use <strong>Retrieval Augmented Generation (RAG)</strong>. Here, the base model receives context from your database <em>at the time of the query</em>.</li>
<li><strong>A simple task is solved by prompt engineering:</strong> Always start with the most effective prompt engineering. If the task is solved with simple instructions and few-shot examples, fine-tuning is redundant and more costly.</li>
<li><strong>You do not have enough high-quality data:</strong> Bad data = bad tuned model.</li>
</ul>
<h3>4. Data Preparation. </h3>
<p>The quality and quantity of your data are critical. The model learns from your examples, so they must be accurate, diverse, and consistent.</p>
<ul>
<li><strong>Format:</strong> Most often JSON Lines (<code>.jsonl</code>) or CSV (<code>.csv</code>).</li>
<li><strong>Data structure:</strong> Depends on the task.
<ul>
<li><strong>Instruction Tuning (Instruction-Response):</strong> Suitable for generalized tasks like "question-answer", paraphrasing, summarization.
<pre class="line-numbers"><code class="language-json">{"input_text": "Paraphrase the sentence: 'AI technology is developing rapidly.'", "output_text": "Artificial intelligence is demonstrating rapid progress."}</code></pre>
<pre class="line-numbers"><code class="language-json">{"input_text": "Name the capital of France.", "output_text": "The capital of France is Paris."}</code></pre>
</li>
<li><strong>Chat Tuning:</strong> Ideal for training the model to conduct a dialogue in a specific role or style.
<pre class="line-numbers"><code class="language-json">{"messages": [{"author": "user", "content": "Hi! What would you recommend for dinner?"}, {"author": "model", "content": "Good evening! Today is a great day for Pasta Carbonara, or, if you prefer something light, a Caesar salad."}])}</code></pre>
<pre class="line-numbers"><code class="language-json">{"messages": [{"author": "user", "content": "Tell me about the new features in Python 3.12."}, {"author": "model", "content": "Python 3.12 introduced indented f-strings, a new syntax for universal generics, and improved handling of Unicode errors."}])}</code></pre>
</li>
</ul>
</li>
<li><strong>Quantity:</strong> A minimum of 100-200 high-quality examples, but the more the better (thousands for serious tasks).</li>
<li><strong>Quality:</strong> Accuracy, consistency of style, tone, format throughout the dataset.</li>
</ul>
<h3>5. Models and Platforms</h3>
<h4>5.1. OpenAI</h4>
<p>OpenAI provides a fairly simple and intuitive API for fine-tuning.</p>
<p><strong>Process:</strong></p>
<ol>
<li>Prepare data in JSONL format, where each line is an object with an array of <code>messages</code> (as in the Chat Completions API).</li>
<li>Upload the data file via the API.</li>
<li>Create a fine-tuning job, specifying the uploaded file and the base model.</li>
<li>Track progress.</li>
<li>Use the new, tuned model by its ID.</li>
</ol>
<p><strong>Data Example (<code>training_data.jsonl</code> file):</strong></p>
<pre class="line-numbers"><code class="language-json">{"messages": [{"role": "system", "content": "You are a friendly IT company customer support assistant."}, {"role": "user", "content": "My internet is not working."}, {"role": "assistant", "content": "Oh, I'm sorry for the inconvenience! Let's try to figure it out. How are you connected to the internet – via Wi-Fi or cable?"}]}</code></pre>
<pre class="line-numbers"><code class="language-json">{"messages": [{"role": "system", "content": "You are a friendly IT company customer support assistant."}, {"role": "user", "content": "How can I reset my password?"}, {"role": "assistant", "content": "Of course! To reset your password, please go to the login page and click 'Forgot password?' below the input field. Then follow the instructions on the screen."}])}</code></pre>
<p><strong>Python Code Example:</strong></p>
<p>First, install: <code>pip install openai</code></p>
<pre class="line-numbers"><code class="language-python">import openai
from openai import OpenAI
import os

# Set your OpenAI API key. It is recommended to use an environment variable.
# os.environ["OPENAI_API_KEY"] = "sk-..."
client = OpenAI()

# 1. Uploading the data file
try:
    file_response = client.files.create(
        file=open("training_data.jsonl", "rb"),
        purpose="fine-tune"
    )
    file_id = file_response.id
    print(f"File uploaded successfully. File ID: {file_id}")
except openai.APIStatusError as e:
    print(f"Error uploading file: {e.status_code} - {e.response}")
    exit()

# 2. Creating a fine-tuning job
try:
    ft_job_response = client.fine_tuning.jobs.create(
        training_file=file_id,
        model="gpt-3.5-turbo" # You can specify a specific version, e.g., "gpt-3.5-turbo-0125"
    )
    job_id = ft_job_response.id
    print(f"Fine-tuning job created. Job ID: {job_id}")
    print("Track the job status via the API or in the OpenAI Playground.")
except openai.APIStatusError as e:
    print(f"Error creating job: {e.status_code} - {e.response}")
    exit()

# Example of tracking the status and getting the model name (run after creating the job):
# # job_id = "ftjob-..." # Replace with your job ID
# # job_status = client.fine_tuning.jobs.retrieve(job_id)
# # print(f"Current job status: {job_status.status}")
# # if job_status.status == "succeeded":
# #     fine_tuned_model_name = job_status.fine_tuned_model
# #     print(f"Tuned model name: {fine_tuned_model_name}")

# 3. Using the tuned model (after it is ready)
# # Replace with the real name of your model, obtained after successful fine-tuning
# # fine_tuned_model_name = "ft:gpt-3.5-turbo-0125:my-org::abcd123"

# # if 'fine_tuned_model_name' in locals() and fine_tuned_model_name:
# #     try:
# #         response = client.chat.completions.create(
# #             model=fine_tuned_model_name,
# #             messages=[
# #                 {"role": "user", "content": "I have a problem with my login."}
# #             ]
# #         )
# #         print("\nTuned model response:")
# #         print(response.choices[0].message.content)
# #     except openai.APIStatusError as e:
# #         print(f"Error using the model: {e.status_code} - {e.response}")
</code></pre>
<h4>5.2. Anthropic</h4>
<p>Anthropic <strong>does not provide a public API for fine-tuning its Claude 3 models (Opus, Sonnet, Haiku) in the same sense that OpenAI or Google do.</strong></p>
<p>Anthropic is focused on creating very powerful base models that, they claim, work great with advanced prompt engineering and RAG patterns, minimizing the need for fine-tuning in most cases.
For large corporate clients or partners, there may be programs to create "custom" models or specialized integrations, but this is not a publicly available fine-tuning feature via the API.</p>
<p>If you are working with Claude 3, your main focus should be on:</p>
<ul>
<li><strong>Quality prompt engineering:</strong> Experiment with system instructions, few-shot examples, and clear query formatting. Claude is known for its ability to strictly follow instructions, especially in XML tags.</li>
<li><strong>RAG systems:</strong> Use external knowledge bases to provide the model with up-to-date context.</li>
</ul>
<h4>5.3. Google (Gemini)</h4>
<p>Google is actively developing fine-tuning capabilities through its <strong>Google Cloud Vertex AI</strong> platform.
This is a full-fledged ML platform that provides tools for data preparation, running training jobs, and deploying models.
Fine-tuning is available for the Gemini family of models.</p>
<p><strong>Process:</strong></p>
<ol>
<li>Prepare data (JSONL or CSV) in <code>input_text</code>/<code>output_text</code> format (for instruction tuning) or <code>messages</code> (for chat tuning).</li>
<li>Upload data to Google Cloud Storage (GCS).</li>
<li>Create and run a fine-tuning job via the Vertex AI Console or SDK.</li>
<li>Deploy the tuned model to an Endpoint.</li>
<li>Use the tuned model via this endpoint.</li>
</ol>
<p><strong>Data Example (<code>gemini_tuning_data.jsonl</code> file):</strong></p>
<pre class="line-numbers"><code class="language-json">{"input_text": "Summarize the main ideas of this book: 'The book is about a hero's journey, overcoming obstacles and finding himself.'", "output_text": "The main character of the book goes on a transformative journey, facing difficulties and gaining self-knowledge."}</code></pre>
<pre class="line-numbers"><code class="language-json">{"input_text": "Explain the principle of a thermonuclear reactor in simple terms.", "output_text": "A thermonuclear reactor tries to reproduce the process that occurs on the Sun: the fusion of light atomic nuclei at very high temperatures, releasing a huge amount of energy."}</code></pre>
<p><strong>Python Code Example (requires <code>google-cloud-aiplatform</code>):</strong></p>
<p>First, install: <code>pip install google-cloud-aiplatform</code> and <code>pip install google-cloud-storage</code></p>
<pre class="line-numbers"><code class="language-python">import os
from google.cloud import aiplatform
from google.cloud import storage

# --- Settings ---
# REPLACE with your values:
PROJECT_ID = "your-gcp-project-id"
REGION = "us-central1"               # Choose a region that supports Gemini and Vertex AI
BUCKET_NAME = "your-gcs-bucket-for-tuning" # The name of your GCS bucket (must be created beforehand)
DATA_FILE_LOCAL_PATH = "gemini_tuning_data.jsonl"
GCS_DATA_URI = f"gs://{DATA_FILE_LOCAL_PATH}"
TUNED_MODEL_DISPLAY_NAME = "my-tuned-gemini-model"
# --- End of settings ---

# Initialize Vertex AI
aiplatform.init(project=PROJECT_ID, location=REGION)

# 1. Create a data file (if it doesn't exist)
with open(DATA_FILE_LOCAL_PATH, "w", encoding="utf-8") as f:
    f.write('{"input_text": "Summarize the main ideas of this book: \'The book is about a hero\'s journey, overcoming obstacles and finding himself.\'", "output_text": "The main character of the book goes on a transformative journey, facing difficulties and gaining self-knowledge."}\n')
    f.write('{"input_text": "Explain the principle of a thermonuclear reactor in simple terms.", "output_text": "A thermonuclear reactor tries to reproduce the process that occurs on the Sun: the fusion of light atomic nuclei at very high temperatures, releasing a huge amount of energy."}\n')
print(f"Data file '{DATA_FILE_LOCAL_PATH}' created.")


# 2. Upload data to Google Cloud Storage
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """Uploads a file to the GCS bucket."""
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print(f"File '{source_file_name}' uploaded to 'gs://{bucket_name}/{destination_blob_name}'.")

try:
    upload_blob(BUCKET_NAME, DATA_FILE_LOCAL_PATH, DATA_FILE_LOCAL_PATH)
except Exception as e:
    print(f"Error uploading file to GCS. Make sure the bucket exists and you have permissions: {e}")
    exit()

# 3. Create and run a fine-tuning job
print(f"\nStarting fine-tuning of the model '{TUNED_MODEL_DISPLAY_NAME}'...")
try:
    # `tune_model` starts the job and returns the tuned model after completion
    tuned_model = aiplatform.Model.tune_model(
        model_display_name=TUNED_MODEL_DISPLAY_NAME,
        source_model_name="gemini-1.0-pro-001", # Base model Gemini Pro
        training_data_path=GCS_DATA_URI,
        tuning_method="SUPERVISED_TUNING",
        train_steps=100, # Number of training steps. The optimal value depends on the size of the data.
        # batch_size=16, # You can specify
        # learning_rate_multiplier=1.0 # You can specify
    )
    print(f"Model '{TUNED_MODEL_DISPLAY_NAME}' successfully tuned. Model ID: {tuned_model.name}")
    print("The fine-tuning process can take a significant amount of time.")
except Exception as e:
    print(f"Fine-tuning error. Check the logs in the Vertex AI Console: {e}")
    exit()

# 4. Deploy the tuned model (for use)
print(f"\nDeploying the tuned model '{TUNED_MODEL_DISPLAY_NAME}' to an endpoint...")
try:
    endpoint = tuned_model.deploy(
        machine_type="n1-standard-4", # Machine type for the endpoint. Choose a suitable one.
        min_replica_count=1,
        max_replica_count=1
    )
    print(f"Model deployed to endpoint: {endpoint.name}")
    print("Deployment can also take several minutes.")
except Exception as e:
    print(f"Error deploying the model: {e}")
    exit()

# 5. Use the tuned model
print("\nTesting the tuned model...")
prompt = "Tell me about your capabilities after training."
instances = [{"prompt": prompt}] # For Instruction Tuning. If Chat Tuning, then {"messages": [...]} 

try:
    response = endpoint.predict(instances=instances)
    print("\nTuned model response:")
    print(response.predictions[0])
except Exception as e:
    print(f"Error using the tuned model: {e}")

# After finishing work, do not forget to delete the endpoint and the model to avoid unnecessary costs:
# endpoint.delete()
# tuned_model.delete()
</code></pre>
<h3>6. General Recommendations</h3>
<ul>
<li><strong>Start small:</strong> Do not try to train a model on thousands of examples right away. Start with a small but high-quality dataset.</li>
<li><strong>Iterate:</strong> Fine-tuning is an iterative process. Train, evaluate, adjust the data or hyperparameters, repeat.</li>
<li><strong>Monitoring:</strong> Carefully monitor the training metrics (loss) and use a validation dataset to avoid overfitting.</li>
<li><strong>Evaluation:</strong> Always test the tuned model on data it has <em>never seen</em> during training to assess its generalization ability.</li>
<li><strong>Cost:</strong> Remember that fine-tuning and deploying endpoints are paid services. Take this into account in your budget.</li>
<li><strong>Documentation:</strong> Always consult the official documentation of the LLM provider. APIs and capabilities are constantly evolving.</li>
</ul>
