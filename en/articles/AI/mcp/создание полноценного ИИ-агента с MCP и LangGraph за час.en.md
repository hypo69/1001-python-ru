<!-- Translated to en -->
# How to teach a neural network to work with its hands: creating a full-fledged AI agent with MCP and LangGraph in an hour


Friends, greetings! I hope you've missed me.

Over the past couple of months, I've been deeply immersed in researching the integration of AI agents into my own Python projects. In the process, I've accumulated a lot of practical knowledge and observations that it would be a sin not to share. So today, I'm returning to Habr ‚Äî with a new topic, a fresh perspective, and with the intention of writing more often.

On the agenda are LangGraph and MCP: tools with which you can create truly useful AI agents.

If before we argued about which neural network answers best in Russian, today the battlefield has shifted towards more practical tasks: who performs better as an AI agent? Which frameworks truly simplify development? And how to integrate all this good stuff into a real project?

But before diving into practice and code, let's understand the basic concepts. Especially the two key ones: **AI agents and MCP**. Without them, the conversation about LangGraph will be incomplete.

### AI agents in simple terms

AI agents are not just "pumped-up" chatbots. They represent more complex, autonomous entities that possess two crucial features:

1.  **Ability to interact and coordinate**

    Modern agents can break down tasks into subtasks, call other agents, request external data, work in a team. This is no longer a single assistant, but a distributed system where each component can contribute.

2.  **Access to external resources**

    An AI agent is no longer limited by the confines of a dialogue. It can access databases, make API calls, interact with local files, vector knowledge bases, and even run commands in the terminal. All this became possible thanks to the emergence of MCP ‚Äî a new level of integration between the model and the environment.

---

Simply put: **MCP is a bridge between a neural network and its environment**. It allows the model to "understand" the context of the task, access data, make calls, and form reasoned actions, rather than just producing text responses.

**Let's imagine an analogy:**

*   You have a **neural network** ‚Äî it can reason and generate texts.
*   There is **data and tools** ‚Äî documents, APIs, knowledge bases, terminal, code.
*   And there is **MCP** ‚Äî it is an interface that allows the model to interact with these external sources as if they were part of its inner world.

**Without MCP:**

The model is an isolated dialogue engine. You give it text ‚Äî it responds. And that's it.

**With MCP:**

The model becomes a full-fledged **task executor**:

*   gains access to data structures and APIs;
*   calls external functions;
*   navigates the current state of the project or application;
*   can remember, track, and change context during a dialogue;
*   uses extensions such as search tools, code runners, vector embedding databases, etc.

In a technical sense, **MCP is a protocol for interaction between an LLM and its environment**, where context is provided as structured objects (instead of "raw" text), and calls are formatted as interactive operations (e.g., function calling, tool usage, or agent actions). This is what turns an ordinary model into a **real AI agent**, capable of doing more than just "talking."

### Now ‚Äî to business!

Now that we've covered the basic concepts, it's logical to ask: "How do we implement all this in practice in Python?"

This is where **LangGraph** comes into play ‚Äî a powerful framework for building state graphs, agent behavior, and thought chains. It allows you to "wire" the logic of interaction between agents, tools, and the user, creating a living AI architecture that adapts to tasks.

In the following sections, we will look at how to:

*   build an agent from scratch;
*   create states, transitions, and events;
*   integrate functions and tools;
*   and how this entire ecosystem works in a real project.

### A little theory: what is LangGraph

Before we get to practice, a few words about the framework itself.

**LangGraph** ‚Äî is a project from the **LangChain** team, the same ones who first proposed the concept of "chains" (chains) of interaction with LLMs. If before the main emphasis was on linear or conditionally branching pipelines (langchain.chains), then now developers are betting on a **graph model**, and LangGraph is what they recommend as the new "core" for building complex AI systems.

**LangGraph** ‚Äî is a framework for building finite state machines and state graphs, where each **node** represents a part of the agent's logic: a model call, an external tool, a condition, user input, etc.

### How it works: graphs and nodes

Conceptually, LangGraph is built on the following ideas:

*   **Graph** ‚Äî is a structure that describes possible paths for logic execution. You can think of it as a map: from one point, you can move to another depending on conditions or the result of execution.
*   **Nodes** ‚Äî are specific steps within the graph. Each node performs some function: calls a model, calls an external API, checks a condition, or simply updates the internal state.
*   **Transitions between nodes** ‚Äî is the routing logic: if the result of the previous step is such and such, then go there.
*   **State** ‚Äî is passed between nodes and accumulates everything needed: history, intermediate conclusions, user input, the result of tool operation, etc.

Thus, we get a **flexible mechanism for managing agent logic**, in which both simple and very complex scenarios can be described: loops, conditions, parallel actions, nested calls, and much more.

### Why is it convenient?

LangGraph allows you to build **transparent, reproducible, and extensible logic**:

*   easy to debug;
*   easy to visualize;
*   easy to scale for new tasks;
*   easy to integrate external tools and MCP protocols.

In essence, LangGraph is the **"brain" of the agent**, where each step is documented, controlled, and can be changed without chaos and "magic."

### But now ‚Äî enough theory!

We could talk for a long time about graphs, states, logic composition, and the advantages of LangGraph over classic pipelines. But, as practice shows, it's better to see it once in code.

**It's time to move on to practice.** Next ‚Äî a Python example: we will create a simple but useful AI agent based on LangGraph, which will use external tools, memory, and make decisions itself.

### Preparation: cloud and local neural networks

In order to start creating AI agents, we first need a **brain** ‚Äî a language model. There are two approaches here:

*   **use cloud solutions**, where everything is ready "out of the box";
*   or **raise the model locally** ‚Äî for complete autonomy and confidentiality.

Let's consider both options.

#### Cloud services: fast and convenient

The easiest way is to use the power of large providers: OpenAI, Anthropic, and use...

### Where to get keys and tokens:

*   **OpenAI** ‚Äî ChatGPT and other products;
*   **Anthropic** ‚Äî Claude;
*   **OpenRouter.ai** ‚Äî dozens of models (one token ‚Äî many models via OpenAI-compatible API);
*   **Amvera Cloud** ‚Äî the ability to connect LLAMA with ruble payment and built-in proxying to OpenAI and Anthropic.

This path is convenient, especially if you:

*   do not want to configure the infrastructure;
*   develop with an emphasis on speed;
*   work with limited resources.

### Local models: full control

If **privacy, offline work** is important to you, or you want to build **fully autonomous agents**, then it makes sense to deploy the neural network locally.

**Main advantages:**

*   **Confidentiality** ‚Äî data remains with you;
*   **Offline work** ‚Äî useful in isolated networks;
*   **No subscriptions and tokens** ‚Äî free after setup.

**Disadvantages are obvious:**

*   Resource requirements (especially video memory);
*   Setup can take time;
*   Some models are difficult to deploy without experience.

Nevertheless, there are tools that make local launch easier. One of the best today is **Ollama**.

### Deploying local LLM via Ollama + Docker

We will prepare a local launch of the Qwen 2.5 (qwen2.5:32b) model using a Docker container and the Ollama system. This will allow us to integrate the neural network with MCP and use it in our own agents based on LangGraph.

If the computing resources of your computer or server are insufficient to work with this version of the model, you can always choose a less "resource-hungry" neural network ‚Äî the installation and launch process will remain similar.

**Quick installation (summary of steps)**

1.  **Install Docker + Docker Compose**
2.  **Create project structure:**
```bash
mkdir qwen-local && cd qwen-local
```
3.  **Create `docker-compose.yml`**
(universal option, GPU is determined automatically)

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - /tmp:/tmp
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    restart: unless-stopped
```

4.  **Start container:**
```bash
docker compose up -d
```

5.  **Download model:**
```bash
docker exec -it ollama_qwen ollama pull qwen2.5:32b
```

6.  **Check operation via API:**
```bash
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:32b", "prompt": "Hello!", "stream": false}'
```
*(Image with curl command execution result)*

7.  **Integration with Python:**
```python
import requests

def query(prompt):
    res = requests.post("http://localhost:11434/api/generate", json={"model": "qwen2.5:32b", "prompt": prompt, "stream": False})
    return res.json()['response']

print(query("Explain quantum entanglement"))
```
Now you have a full-fledged local LLM, ready to work with MCP and LangGraph.

**What's next?**

We have a choice between cloud and local models, and we have learned how to connect both. The most interesting part is ahead ‚Äî **creating AI agents on LangGraph**, which use the selected model, memory, tools, and their own logic.

**Let's move on to the most delicious part ‚Äî code and practice!**

---

Before moving on to practice, it is important to prepare the working environment. I assume that you are already familiar with the basics of Python, know what libraries and dependencies are, and understand why to use a virtual environment.

If all this is new to you ‚Äî I recommend first taking a short course or guide on Python basics, and then returning to the article.

#### Step 1: Creating a virtual environment

Create a new virtual environment in the project folder:
```bash
python -m venv venv
source venv/bin/activate  # for Linux/macOS
venc\Scripts\activate   # for Windows
```

#### Step 2: Installing dependencies

Create a `requirements.txt` file and add the following lines to it:
```
langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
```

> ‚ö†Ô∏è **Current versions are indicated as of July 21, 2025.** Since publication, they may have changed ‚Äî **check for relevance before installation.**

Then install the dependencies:
```bash
pip install -r requirements.txt```

#### Step 3: Configuring environment variables

Create a `.env` file in the project root and add the necessary API keys to it:
```
OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc
```

**Purpose of variables:**

*   **OPENAI_API_KEY** ‚Äî key for accessing GPT models from OpenAI;
*   **DEEPSEEK_API_KEY** ‚Äî key for using Deepseek models;
*   **OPENROUTER_API_KEY** ‚Äî single key for accessing many models via OpenRouter

---

Some MCP tools (e.g., `brave-web-search`) require a key to work. Without it, they simply won't activate.

**What if you don't have API keys?**

No problem. You can start development with a local model (e.g., via Ollama), without connecting any external services. In this case, the `.env` file can be omitted entirely.

Done! Now we have everything we need to get started ‚Äî an isolated environment, dependencies, and, if necessary, access to cloud neural networks and MCP integrations.

Next - we will launch our LLM agent in different ways.

### Simple launch of LLM agents via LangGraph: basic integration

Let's start with the simplest: how to "connect the brain" to the future agent. We will analyze the basic ways to launch language models (LLM) with LangChain, so that in the next step we can move on to integration with LangGraph and building a full-fledged AI agent.

#### Imports
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
```
*   `os` and `load_dotenv()` ‚Äî for loading variables from the `.env` file.
*   `ChatOpenAI`, `ChatOllama`, `ChatDeepSeek` ‚Äî wrappers for connecting language models via LangChain.

> üí° If you use alternative approaches to working with configurations (e.g., Pydantic Settings), you can replace `load_dotenv()` with your usual method.

#### Loading environment variables
```python
load_dotenv()
```
This will load all variables from `.env`, including keys for accessing OpenAI, DeepSeek, OpenRouter and other APIs.

#### Simple functions for getting LLM

**OpenAI**
```python
def get_openai_llm():
    return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
```
If the `OPENAI_API_KEY` variable is correctly set, LangChain will substitute it automatically ‚Äî explicit specification of `api_key=...` is optional here.

**DeepSeek**
```python
def get_deepseek_llm():
    # ...
```
Similarly, but we use the `ChatDeepSeek` wrapper.

**OpenRouter (and other compatible APIs)**
```python
def get_openrouter_llm(model="moonshotai/kimi-k2:free"):
    return ChatOpenAI(
        model=model,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )
```
**Features:**

*   `ChatOpenAI` is used, even though the model is not from OpenAI ‚Äî because OpenRouter uses the same protocol.
*   `base_url` is required: it points to the OpenRouter API.
*   The `moonshotai/kimi-k2:free` model was chosen as one of the most balanced options in terms of quality and speed at the time of writing.
*   The `OpenRouter` API key must be passed explicitly ‚Äî automatic substitution does not work here.

#### Mini-test: checking model operation
```python
if __name__ == "__main__":
    llm = get_openrouter_llm(model="moonshotai/kimi-k2:free")
    response = llm.invoke("Who are you?")
    print(response.content)
```
*(–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: `–Ø - –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –∫–æ–º–ø–∞–Ω–∏–µ–π Moonshot AI...`)*

If everything is configured correctly, you will receive a meaningful response from the model. Congratulations ‚Äî the first step is done!

### But this is not yet an agent

At this stage, we have connected the LLM and made a simple call. This is more like a console chatbot than an AI agent.

**Why?**

*   We are writing **synchronous, linear code** without state logic or goals.
*   –ê–≥–µ–Ω—Ç –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–π, –Ω–µ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.
*   –ú–°–† –∏ LangGraph –ø–æ–∫–∞ –Ω–µ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω—ã.

**What's next?**

–î–∞–ª–µ–µ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º **–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ò–ò-–∞–≥–µ–Ω—Ç–∞** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **LangGraph** ‚Äî —Å–Ω–∞—á–∞–ª–∞ –±–µ–∑ –ú–°–†, —á—Ç–æ–±—ã —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –∏ –ª–æ–≥–∏–∫–µ —Å–∞–º–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.

–ü–æ–≥—Ä—É–∂–∞–µ–º—Å—è –≤ –Ω–∞—Å—Ç–æ—è—â—É—é –∞–≥–µ–Ω—Ç–Ω—É—é –º–µ—Ö–∞–Ω–∏–∫—É. –ü–æ–µ—Ö–∞–ª–∏!

### –ê–≥–µ–Ω—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–π: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ

...–∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ LangGraph –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∏ —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–µ–∑–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è HR-–ø–ª–∞—Ç—Ñ–æ—Ä–º –∏ –±–∏—Ä–∂ —Ñ—Ä–∏–ª–∞–Ω—Å–∞.

#### –ó–∞–¥–∞—á–∞ –∞–≥–µ–Ω—Ç–∞

–ù–∞—à –∞–≥–µ–Ω—Ç –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–ª–∏ —É—Å–ª—É–≥–∏ –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ç—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é:

1.  **–¢–∏–ø —Ä–∞–±–æ—Ç—ã**: –ø—Ä–æ–µ–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏–ª–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è –≤–∞–∫–∞–Ω—Å–∏—è
2.  **–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏**: –∏–∑ 45+ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–µ–π
3.  **–¢–∏–ø –ø–æ–∏—Å–∫–∞**: –∏—â–µ—Ç –ª–∏ —á–µ–ª–æ–≤–µ–∫ —Ä–∞–±–æ—Ç—É –∏–ª–∏ –∏—â–µ—Ç –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è

–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º JSON-—Ñ–æ—Ä–º–∞—Ç–µ —Å –æ—Ü–µ–Ω–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

#### üìà –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ LangGraph

–°–ª–µ–¥—É—è –ø—Ä–∏–Ω—Ü–∏–ø–∞–º LangGraph, —Å–æ–∑–¥–∞—ë–º **–≥—Ä–∞—Ñ —Å–æ—Å—Ç–æ—è–Ω–∏–π** –∏–∑ —á–µ—Ç—ã—Ä—ë—Ö —É–∑–ª–æ–≤:

- –í—Ö–æ–¥–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
- ‚Üì
- –£–∑–µ–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–∞ —Ä–∞–±–æ—Ç—ã
- ‚Üì
- –£–∑–µ–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
- ‚Üì
- –£–∑–µ–ª –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
- ‚Üì
- –£–∑–µ–ª —Ä–∞—Å—á—ë—Ç–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
- ‚Üì
- JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç

–ö–∞–∂–¥—ã–π —É–∑–µ–ª ‚Äî —ç—Ç–æ **—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è**, –∫–æ—Ç–æ—Ä–∞—è:

*   –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
*   –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤–æ—é —á–∞—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞
*   –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –ø–µ—Ä–µ–¥–∞—ë—Ç –µ–≥–æ –¥–∞–ª—å—à–µ

#### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º

–û–ø—Ä–µ–¥–µ–ª—è–µ–º **—Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–∞** —á–µ—Ä–µ–∑ `TypedDict`:

```python
from typing import TypedDict, Dict

class State(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool
```

–≠—Ç–æ **—Ä–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å –∞–≥–µ–Ω—Ç–∞** ‚Äî –≤—Å—ë, —á—Ç–æ –æ–Ω –ø–æ–º–Ω–∏—Ç –∏ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∞–Ω–∞–ª–∏–∑–∞. –ü–æ–¥–æ–±–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫-—ç–∫—Å–ø–µ—Ä—Ç –¥–µ—Ä–∂–∏—Ç –≤ —É–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

–î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø–æ–ª–Ω—ã–π –∫–æ–¥, –∞ –ø–æ—Å–ª–µ —Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ–º—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–∞—Ö.

```python
import asyncio
import json
from enum import Enum
from typing import TypedDict, Dict, Any, List

from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π
CATEGORIES = [
    "2D-–∞–Ω–∏–º–∞—Ç–æ—Ä", "3D-–∞–Ω–∏–º–∞—Ç–æ—Ä", "3D-–º–æ–¥–µ–ª–ª–µ—Ä",
    "–ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫", "–ë–ª–æ–∫—á–µ–π–Ω-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", ...
]

class JobType(Enum):
    PROJECT = "–ø—Ä–æ–µ–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞"
    PERMANENT = "–ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞"

class SearchType(Enum):
    LOOKING_FOR_WORK = "–ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç—ã"
    LOOKING_FOR_PERFORMER = "–ø–æ–∏—Å–∫ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è"

class State(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    description: str
    job_type: str
    category: str
    search_type: str
    confidence_scores: Dict[str, float]
    processed: bool

class VacancyClassificationAgent:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–π –∏ —É—Å–ª—É–≥"""

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞"""
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LangGraph"""
        workflow = StateGraph(State)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã –≤ –≥—Ä–∞—Ñ
        workflow.add_node("job_type_classification", self._classify_job_type)
        workflow.add_node("category_classification", self._classify_category)
        workflow.add_node("search_type_classification", self._classify_search_type)
        workflow.add_node("confidence_calculation", self._calculate_confidence)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —É–∑–ª–æ–≤
        workflow.set_entry_point("job_type_classification")
        workflow.add_edge("job_type_classification", "category_classification")
        workflow.add_edge("category_classification", "search_type_classification")
        workflow.add_edge("search_type_classification", "confidence_calculation")
        workflow.add_edge("confidence_calculation", END)
        
        return workflow.compile()

    async def _classify_job_type(self, state: State) -> Dict[str, Any]:
        """–£–∑–µ–ª –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ä–∞–±–æ—Ç—ã: –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –∏–ª–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è"""
        # ... (implementation follows)
        
    async def _classify_category(self, state: State) -> Dict[str, Any]:
        """–£–∑–µ–ª –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏"""
        # ... (implementation follows)
        
    async def _classify_search_type(self, state: State) -> Dict[str, Any]:
        """–£–∑–µ–ª –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞"""
        # ... (implementation follows)

    async def _calculate_confidence(self, state: State) -> Dict[str, Any]:
        """–£–∑–µ–ª –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —É—Ä–æ–≤–Ω—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        # ... (implementation follows)

    def _find_closest_category(self, predicted_category: str) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö"""
        # ... (implementation follows)

    async def classify(self, description: str) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏/—É—Å–ª—É–≥–∏"""
        initial_state = {
            "description": description,
            "job_type": "",
            "category": "",
            "search_type": "",
            "confidence_scores": {},
            "processed": False
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å
        result = await self.workflow.ainvoke(initial_state)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
        classification_result = {
            "job_type": result["job_type"],
            "category": result["category"],
            "search_type": result["search_type"],
            "confidence_scores": result["confidence_scores"],
            "success": result["processed"]
        }
        return classification_result

async def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞"""
    agent = VacancyClassificationAgent()
    
    test_cases = [
        "–¢—Ä–µ–±—É–µ—Ç—Å—è Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –Ω–∞ Django. –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞.",
        "–ò—â—É –∑–∞–∫–∞–∑—ã –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –ª–æ–≥–æ—Ç–∏–ø–æ–≤ –∏ —Ñ–∏—Ä–º–µ–Ω–Ω–æ–≥–æ —Å—Ç–∏–ª—è. –†–∞–±–æ—Ç–∞—é –≤ Adobe Illustrator.",
        "–ù—É–∂–µ–Ω 3D-–∞–Ω–∏–º–∞—Ç–æ—Ä –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ —Ä–æ–ª–∏–∫–∞.",
        "–†–µ–∑—é–º–µ: –æ–ø—ã—Ç–Ω—ã–π –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥, –∏—â—É —É–¥–∞–ª–µ–Ω–Ω—É—é —Ä–∞–±–æ—Ç—É –≤ —Å—Ñ–µ—Ä–µ digital-–º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞",
        "–ò—â–µ–º —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ React –≤ –Ω–∞—à—É –∫–æ–º–∞–Ω–¥—É –Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é –æ—Å–Ω–æ–≤–µ"
    ]
    
    print("ü§ñ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–π\n")
    for i, description in enumerate(test_cases, 1):
        print(f"--- –¢–µ—Å—Ç {i}: ---")
        print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {description}")
        try:
            result = await agent.classify(description)
            print("–†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("-" * 80)

if __name__ == "__main__":
    asyncio.run(main())

```
*(...–æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∫–æ–¥–∞ —Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –º–µ—Ç–æ–¥–æ–≤ –±—ã–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ...)*

### –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
1.  **–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å** ‚Äî –∫–∞–∂–¥—ã–π —É–∑–µ–ª —Ä–µ—à–∞–µ—Ç –æ–¥–Ω—É –∑–∞–¥–∞—á—É, –ª–µ–≥–∫–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —É–ª—É—á—à–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ
2.  **–†–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å** ‚Äî –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —É–∑–ª—ã –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
3.  **–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å** ‚Äî –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º
4.  **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** ‚Äî –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
5.  **–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å** ‚Äî fallback-–º–µ—Ö–∞–Ω–∏–∑–º—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

### –†–µ–∞–ª—å–Ω–∞—è –ø–æ–ª—å–∑–∞
–¢–∞–∫–æ–π –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤:
*   **HR-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö** –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–π
*   **–ë–∏—Ä–∂–∞—Ö —Ñ—Ä–∏–ª–∞–Ω—Å–∞** –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
*   **–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö** –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞—è–≤–æ–∫ –∏ –ø—Ä–æ–µ–∫—Ç–æ–≤
*   **–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏—è—Ö** –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä—ã–Ω–∫–∞ —Ç—Ä—É–¥–∞

### –ú–°–† –≤ –¥–µ–π—Å—Ç–≤–∏–∏: —Å–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞ —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∏ –≤–µ–±-–ø–æ–∏—Å–∫–æ–º
–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –º—ã —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å —Å –±–∞–∑–æ–≤—ã–º–∏ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º–∏ LangGraph –∏ —Å–æ–∑–¥–∞–ª–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–≥–µ–Ω—Ç–∞-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, –¥–∞–≤–∞–π—Ç–µ —Ä–∞—Å—à–∏—Ä–∏–º –µ–≥–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –ø–æ–¥–∫–ª—é—á–∏–≤ –∫ –≤–Ω–µ—à–Ω–∏–º ‡¶™‡¶∞‡¶Æ‡¶æ‡¶£ —á–µ—Ä–µ–∑ –ú–°–†.

–°–µ–π—á–∞—Å –º—ã —Å–æ–∑–¥–∞–¥–∏–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–º–æ–∂–µ—Ç:
*   –†–∞–±–æ—Ç–∞—Ç—å —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π (—á–∏—Ç–∞—Ç—å, —Å–æ–∑–¥–∞–≤–∞—Ç—å, –∏–∑–º–µ–Ω—è—Ç—å —Ñ–∞–π–ª—ã)
*   –ò—Å–∫–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
*   –ó–∞–ø–æ–º–∏–Ω–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
*   –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ—à–∏–±–∫–∏ –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –ø–æ—Å–ª–µ —Å–±–æ–µ–≤

#### –û—Ç —Ç–µ–æ—Ä–∏–∏ –∫ —Ä–µ–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º
–ü–æ–º–Ω–∏—Ç–µ, –∫–∞–∫ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç–∞—Ç—å–∏ –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –æ —Ç–æ–º, —á—Ç–æ **–ú–°–† ‚Äî —ç—Ç–æ –º–æ—Å—Ç –º–µ–∂–¥—É –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –∏ –µ—ë –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º**? –°–µ–π—á–∞—Å –≤—ã —É–≤–∏–¥–∏—Ç–µ —ç—Ç–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ. –ù–∞—à –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∏—Ç –¥–æ—Å—Ç—É–ø –∫ **—Ä–µ–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º**:
```
# –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
- read_file ‚Äî —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
- write_file ‚Äî –∑–∞–ø–∏—Å—å –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
- list_directory ‚Äî –ø—Ä–æ—Å–º–æ—Ç—Ä —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–ø–æ–∫
- create_directory ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫

# –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤–µ–±-–ø–æ–∏—Å–∫–∞
- brave_web_search ‚Äî –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
- get_web_content ‚Äî –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü
```
–≠—Ç–æ —É–∂–µ –Ω–µ ¬´–∏–≥—Ä—É—à–µ—á–Ω—ã–π¬ª –∞–≥–µ–Ω—Ç ‚Äî —ç—Ç–æ **—Ä–∞–±–æ—á–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç**, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏.

#### üìà –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –∫ —Å–ª–æ–∂–Ω–æ–º—É

**1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏**
```python
from dataclasses import dataclass

@dataclass
class AgentConfig:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ê–Ü-–∞–≥–µ–Ω—Ç–∞"""
    filesystem_path: str = "/path/to/work/directory"
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: bool = True
    enable_web_search: bool = True

    def validate(self) -> None:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if not os.path.exists(self.filesystem_path):
            raise ValueError(f"–ü—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.filesystem_path}")
```
**–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ?** –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–∏–º–µ—Ä–∞ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π, –∑–¥–µ—Å—å –∞–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏. –û–¥–Ω–∞ –æ—à–∏–±–∫–∞ –≤ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –ê–†–Ü-–∫–ª—é—á ‚Äî –∏ –≤–µ—Å—å –∞–≥–µ–Ω—Ç –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç —Ä–∞–±–æ—Ç–∞—Ç—å. **–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Å—Ç–∞—Ä—Ç–µ** —ç–∫–æ–Ω–æ–º–∏—Ç —á–∞—Å—ã –æ—Ç–ª–∞–¥–∫–∏.

**2. –§–∞–±—Ä–∏–∫–∞ –º–æ–¥–µ–ª–µ–π: –≥–∏–±–∫–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞**
```python
def create_model(config: AgentConfig):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    provider = config.model_provider.value
    if provider == "ollama":
        return ChatOllama(model="qwen2.5:32b", base_url="http://localhost:11434")
    elif provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    # ... –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
```
–û–¥–∏–Ω –∫–æ–¥ ‚Äî –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π. –•–æ—Ç–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å? –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama. –ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å? –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ GPT-4. –í–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å? –ü–æ–ø—Ä–æ–±—É–π—Ç–µ DeepSeek. –ö–æ–¥ –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–µ–º –∂–µ.

**3. –ú–°–†-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Ä–µ–∞–ª—å–Ω–æ–º—É –º–∏—Ä—É**
```python
async def _init_mcp_client(self):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ú–°–† –∫–ª–∏–µ–Ω—Ç–∞"""
    mcp_config = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", self.filesystem_path],
            "transport": "stdio"
        },
        "brave-search": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-brave-search@latest"],
            "transport": "stdio",
            "env": {"BRAVE_API_KEY": os.getenv("BRAVE_API_KEY")}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = await self.mcp_client.get_tools()
```
–ó–¥–µ—Å—å –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–ª—é—á–µ–≤–∞—è —Ä–∞–±–æ—Ç–∞ –ú–°–†: –º—ã –ø–æ–¥–∫–ª—é—á–∞–µ–º –∫ –∞–≥–µ–Ω—Ç—É –≤–Ω–µ—à–Ω–∏–µ –ú–°–†-—Å–µ—Ä–≤–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π. –ê–≥–µ–Ω—Ç –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–ª—É—á–∞–µ—Ç –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º.

#### –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –æ—à–∏–±–∫–∞–º
–í —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ –≤—Å—ë –ª–æ–º–∞–µ—Ç—Å—è: —Å–µ—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, —Ñ–∞–π–ª—ã –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã, –ê–†–Ü-–∫–ª—é—á–∏ –ø—Ä–æ—Å—Ä–æ—á–µ–Ω—ã. –ù–∞—à –∞–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —ç—Ç–æ–º—É:
```python
@retry_on_failure(max_retries=2, delay=1.0)
async def process_message(self, user_input: str, thread_id: str = "default") -> str:
    # ...
```
–î–µ–∫–æ—Ä–∞—Ç–æ—Ä `@retry_on_failure` –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–±–æ—è—Ö. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–∞–∂–µ –Ω–µ –∑–∞–º–µ—Ç–∏—Ç, —á—Ç–æ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫.

### –ò—Ç–æ–≥–∏: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤

–°–µ–≥–æ–¥–Ω—è –º—ã –ø—Ä–æ—à–ª–∏ –ø—É—Ç—å –æ—Ç –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤. –î–∞–≤–∞–π—Ç–µ –ø–æ–¥–≤–µ–¥—ë–º –∏—Ç–æ–≥–∏ —Ç–æ–≥–æ, —á—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏ –∏ —á–µ–≥–æ –¥–æ—Å—Ç–∏–≥–ª–∏.

**–ß—Ç–æ –º—ã –æ—Å–≤–æ–∏–ª–∏**

**1. –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏**
*   –†–∞–∑–æ–±—Ä–∞–ª–∏—Å—å —Å —Ä–∞–∑–ª–∏—á–∏–µ–º –º–µ–∂–¥—É —á–∞—Ç-–±–æ—Ç–∞–º–∏ –∏ –Ω–∞—Å—Ç–æ—è—â–∏–º–∏ –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏
*   –ü–æ–Ω—è–ª–∏ —Ä–æ–ª—å **–ú–°–† (Model Context Protocol)** –∫–∞–∫ –º–æ—Å—Ç–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ –≤–Ω–µ—à–Ω–∏–º –º–∏—Ä–æ–º
*   –ò–∑—É—á–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É **LangGraph** –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤

**2. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏**
*   –ù–∞—Å—Ç—Ä–æ–∏–ª–∏ —Ä–∞–±–æ—á–µ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–±–ª–∞—á–Ω—ã—Ö –∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
*   –°–æ–∑–¥–∞–ª–∏ **–∞–≥–µ–Ω—Ç–∞-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞** —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏
*   –ü–æ—Å—Ç—Ä–æ–∏–ª–∏ **–ú–°–†-–∞–≥–µ–Ω—Ç–∞** —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ –∏ –≤–µ–±-–ø–æ–∏—Å–∫—É

**3. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã**
*   –û—Å–≤–æ–∏–ª–∏ –º–æ–¥—É–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ —Ñ–∞–±—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π
*   –í–Ω–µ–¥—Ä–∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ **retry-–º–µ—Ö–∞–Ω–∏–∑–º—ã** –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω-–≥–æ—Ç–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π

### –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ–¥—Ö–æ–¥–∞
**LangGraph + –ú–°–†** –¥–∞—é—Ç –Ω–∞–º:
*   **–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å** ‚Äî –∫–∞–∂–¥—ã–π —à–∞–≥ –∞–≥–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º
*   **–†–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å** ‚Äî –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –¥–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω–æ
*   **–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å** ‚Äî –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
*   **–ì–∏–±–∫–æ—Å—Ç—å** ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –∏–∑ –∫–æ—Ä–æ–±–∫–∏

### –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ò–ò-–∞–≥–µ–Ω—Ç—ã ‚Äî —ç—Ç–æ –Ω–µ —Ñ—É—Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞, –∞ **—Ä–µ–∞–ª—å–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –¥–Ω—è**. –° –ø–æ–º–æ—â—å—é LangGraph –∏ MCP –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ—à–∞—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∏, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É—é—Ç —Ä—É—Ç–∏–Ω—É –∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.

**–ì–ª–∞–≤–Ω–æ–µ ‚Äî –Ω–∞—á–∞—Ç—å.** –í–æ–∑—å–º–∏—Ç–µ –∫–æ–¥ –∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤, –∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ —Å–≤–æ–∏ –∑–∞–¥–∞—á–∏, —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ. –ö–∞–∂–¥—ã–π –ø—Ä–æ–µ–∫—Ç ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –æ–ø—ã—Ç –∏ —à–∞–≥ –∫ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤—É –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.

–£–¥–∞—á–∏ –≤ –≤–∞—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö!

---
*–¢–µ–≥–∏: python, –∏–∏, mcp, langchain, –∏–∏-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, ollama, –∏–∏-–∞–≥–µ–Ω—Ç—ã, local llm, langgraph, mcp-server*
*–•–∞–±—ã: –ë–ª–æ–≥ –∫–æ–º–ø–∞–Ω–∏–∏ Amvera, Natural Language Processing, –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, Python, –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ*

![habr](https://habr.com/ru/companies/amvera/articles/929568/)