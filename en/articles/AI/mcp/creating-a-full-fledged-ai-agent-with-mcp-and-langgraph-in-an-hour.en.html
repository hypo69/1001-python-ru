<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>How to teach a neural network to work with its hands&colon; creating a full-fledged AI agent with MCP and LangGraph in an hour</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="how-to-teach-a-neural-network-to-work-with-its-hands-creating-a-full-fledged-ai-agent-with-mcp-and-langgraph-in-an-hour">How to teach a neural network to work with its hands: creating a full-fledged AI agent with MCP and LangGraph in an hour</h1>
<p>Friends, greetings! I hope you've missed me.</p>
<p>For the past couple of months, I've been deeply immersed in researching the integration of AI agents into my own Python projects. In the process, I've accumulated a lot of practical knowledge and observations that it would be a sin not to share. So today I'm returning to Habr - with a new topic, a fresh perspective, and with the intention of writing more often.</p>
<p>On the agenda are LangGraph and MCP: tools with which you can create truly useful AI agents.</p>
<p>If before we argued about which neural network answers better in Russian, today the battlefield has shifted towards more applied tasks: who better copes with the role of an AI agent? Which frameworks truly simplify development? And how to integrate all this good stuff into a real project?</p>
<p>But before diving into practice and code, let's understand the basic concepts. Especially two key ones: <strong>AI agents and MCP</strong>. Without them, the conversation about LangGraph will be incomplete.</p>
<h3 id="ai-agents-in-simple-terms">AI agents in simple terms</h3>
<p>AI agents are not just &quot;pumped-up&quot; chatbots. They are more complex, autonomous entities that possess two crucial features:</p>
<ol>
<li>
<p><strong>Ability to interact and coordinate</strong></p>
<p>Modern agents can break down tasks into subtasks, call other agents, request external data, and work in a team. This is no longer a solitary assistant, but a distributed system where each component can contribute.</p>
</li>
<li>
<p><strong>Access to external resources</strong></p>
<p>An AI agent is no longer limited by the confines of a dialogue. It can access databases, make API calls, interact with local files, vector knowledge bases, and even run commands in the terminal. All this became possible thanks to the emergence of MCP - a new level of integration between the model and the environment.</p>
</li>
</ol>
<hr>
<p>If to put it simply: <strong>MCP is a bridge between a neural network and its environment</strong>. It allows the model to &quot;understand&quot; the context of the task, access data, make calls, and form reasoned actions, rather than just issuing text responses.</p>
<p><strong>Let's imagine an analogy:</strong></p>
<ul>
<li>You have a <strong>neural network</strong> - it can reason and generate texts.</li>
<li>There are <strong>data and tools</strong> - documents, APIs, knowledge bases, terminal, code.</li>
<li>And there is <strong>MCP</strong> - it is an interface that allows the model to interact with these external sources as if they were part of its internal world.</li>
</ul>
<p><strong>Without MCP:</strong></p>
<p>Model is an isolated dialogue engine. You feed it text - it responds. And that's it.</p>
<p><strong>With MCP:</strong></p>
<p>Model becomes a full-fledged <strong>task executor</strong>:</p>
<ul>
<li>gains access to data structures and APIs;</li>
<li>calls external functions;</li>
<li>navigates the current state of the project or application;</li>
<li>can remember, track, and change context as the dialogue progresses;</li>
<li>uses extensions such as search tools, code runners, vector embedding databases, etc.</li>
</ul>
<p>In a technical sense, <strong>MCP is a protocol for interaction between LLM and its environment</strong>, where context is provided in the form of structured objects (instead of &quot;raw&quot; text), and calls are formalized as interactive operations (e.g., function calling, tool usage, or agent actions). This is what turns an ordinary model into a <strong>real AI agent</strong>, capable of doing more than just &quot;talking&quot;.</p>
<h3 id="and-now---to-business">And now - to business!</h3>
<p>Now that we've covered the basic concepts, it's logical to ask: &quot;How do we implement all this in Python in practice?&quot;</p>
<p>This is where <strong>LangGraph</strong> comes into play - a powerful framework for building state graphs, agent behavior, and thought chains. It allows you to &quot;stitch together&quot; the interaction logic between agents, tools, and the user, creating a living AI architecture that adapts to tasks.</p>
<p>In the following sections, we'll look at how to:</p>
<ul>
<li>build an agent from scratch;</li>
<li>create states, transitions, and events;</li>
<li>integrate functions and tools;</li>
<li>and how this entire ecosystem works in a real project.</li>
</ul>
<h3 id="a-little-theory-what-is-langgraph">A little theory: what is LangGraph</h3>
<p>Before we dive into practice, a few words about the framework itself.</p>
<p><strong>LangGraph</strong> is a project from the <strong>LangChain</strong> team, the very ones who first proposed the concept of &quot;chains&quot; for interacting with LLMs. If previously the main focus was on linear or conditionally branching pipelines (langchain.chains), now the developers are betting on a <strong>graph model</strong>, and LangGraph is what they recommend as the new &quot;core&quot; for building complex AI systems.</p>
<p><strong>LangGraph</strong> is a framework for building finite state machines and state graphs, where each <strong>node</strong> represents a part of the agent's logic: a model call, an external tool, a condition, user input, etc.</p>
<h3 id="how-it-works-graphs-and-nodes">How it works: graphs and nodes</h3>
<p>Conceptually, LangGraph is built on the following ideas:</p>
<ul>
<li><strong>Graph</strong> - is a structure that describes the possible paths of logic execution. You can think of it as a map: from one point you can move to another depending on conditions or the execution result.</li>
<li><strong>Nodes</strong> - are specific steps within the graph. Each node performs some function: calls a model, calls an external API, checks a condition, or simply updates the internal state.</li>
<li><strong>Transitions between nodes</strong> - is the routing logic: if the result of the previous step is such and such, then go there.</li>
<li><strong>State</strong> - is passed between nodes and accumulates everything needed: history, intermediate conclusions, user input, tool operation results, etc.</li>
</ul>
<p>Thus, we get a <strong>flexible mechanism for controlling agent logic</strong>, in which both simple and very complex scenarios can be described: loops, conditions, parallel actions, nested calls, and much more.</p>
<h3 id="why-is-it-convenient">Why is it convenient?</h3>
<p>LangGraph allows you to build <strong>transparent, reproducible, and extensible logic</strong>:</p>
<ul>
<li>easy to debug;</li>
<li>easy to visualize;</li>
<li>easy to scale for new tasks;</li>
<li>easy to integrate external tools and MCP protocols.</li>
</ul>
<p>In essence, LangGraph is the <strong>&quot;brain&quot; of the agent</strong>, where each step is documented, controllable, and can be changed without chaos and &quot;magic.&quot;</p>
<h3 id="well-thats-enough-theory">Well, that's enough theory!</h3>
<p>We could talk for a long time about graphs, states, logic composition, and the advantages of LangGraph over classic pipelines. But, as practice shows, it's better to see it once in code.</p>
<p><strong>It's time to move on to practice.</strong> Next - a Python example: we'll create a simple but useful AI agent based on LangGraph that will use external tools, memory, and make decisions itself.</p>
<h3 id="preparation-cloud-and-local-neural-networks">Preparation: cloud and local neural networks</h3>
<p>To start creating AI agents, we first need a <strong>brain</strong> - a language model. Here there are two approaches:</p>
<ul>
<li><strong>use cloud solutions</strong>, where everything is ready &quot;out of the box&quot;;</li>
<li>or <strong>raise the model locally</strong> - for complete autonomy and confidentiality.</li>
</ul>
<p>Let's consider both options.</p>
<h4 id="cloud-services-fast-and-convenient">Cloud services: fast and convenient</h4>
<p>The simplest way is to use the power of large providers: OpenAI, Anthropic, and use...</p>
<h3 id="where-to-get-keys-and-tokens">Where to get keys and tokens:</h3>
<ul>
<li><strong>OpenAI</strong> - ChatGPT and other products;</li>
<li><strong>Anthropic</strong> - Claude;</li>
<li><strong><a href="http://OpenRouter.ai">OpenRouter.ai</a></strong> - dozens of models (one token - many models via OpenAI-compatible API);</li>
<li><strong>Amvera Cloud</strong> - the ability to connect LLAMA with ruble payment and built-in proxying to OpenAI and Anthropic.</li>
</ul>
<p>This path is convenient, especially if you:</p>
<ul>
<li>don't want to configure infrastructure;</li>
<li>develop with a focus on speed;</li>
<li>work with limited resources.</li>
</ul>
<h3 id="local-models-full-control">Local models: full control</h3>
<p>If <strong>privacy, offline work</strong> are important to you, or you want to build <strong>fully autonomous agents</strong>, then it makes sense to deploy the neural network locally.</p>
<p><strong>Main advantages:</strong></p>
<ul>
<li><strong>Confidentiality</strong> - data remains with you;</li>
<li><strong>Offline work</strong> - useful in isolated networks;</li>
<li><strong>No subscriptions and tokens</strong> - free after setup.</li>
</ul>
<p><strong>Disadvantages are obvious:</strong></p>
<ul>
<li>Resource requirements (especially for video memory);</li>
<li>Setup can take time;</li>
<li>Some models are difficult to deploy without experience.</li>
</ul>
<p>Nevertheless, there are tools that make local launch easier. One of the best today is <strong>Ollama</strong>.</p>
<h3 id="deploying-local-llm-via-ollama--docker">Deploying local LLM via Ollama + Docker</h3>
<p>We will prepare a local launch of the Qwen 2.5 model (qwen2.5:32b) using a Docker container and the Ollama system. This will allow integrating the neural network with MCP and using it in your own LangGraph-based agents.</p>
<p>If the computing resources of your computer or server are insufficient for working with this version of the model, you can always choose a less &quot;resource-hungry&quot; neural network - the installation and launch process will remain similar.</p>
<p><strong>Quick installation (summary of steps)</strong></p>
<ol>
<li><strong>Install Docker + Docker Compose</strong></li>
<li><strong>Create project structure:</strong></li>
</ol>
<pre><code class="language-bash"><span class="hljs-built_in">mkdir</span> qwen-local &amp;&amp; <span class="hljs-built_in">cd</span> qwen-local
</code></pre>
<ol start="3">
<li><strong>Create <code>docker-compose.yml</code></strong>
(universal option, GPU is detected automatically)</li>
</ol>
<pre><code class="language-yaml"><span class="hljs-attr">services:</span>
  <span class="hljs-attr">ollama:</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">ollama/ollama:latest</span>
    <span class="hljs-attr">container_name:</span> <span class="hljs-string">ollama_qwen</span>
    <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;11434:11434&quot;</span>
    <span class="hljs-attr">volumes:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">./ollama_data:/root/.ollama</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">/tmp:/tmp</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">OLLAMA_HOST=0.0.0.0</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">OLLAMA_ORIGINS=*</span>
    <span class="hljs-attr">restart:</span> <span class="hljs-string">unless-stopped</span>
</code></pre>
<ol start="4">
<li><strong>Start the container:</strong></li>
</ol>
<pre><code class="language-bash">docker compose up -d
</code></pre>
<ol start="5">
<li><strong>Download the model:</strong></li>
</ol>
<pre><code class="language-bash">docker <span class="hljs-built_in">exec</span> -it ollama_qwen ollama pull qwen2.5:32b
</code></pre>
<ol start="6">
<li><strong>Check operation via API:</strong></li>
</ol>
<pre><code class="language-bash">curl http://localhost:11434/api/generate \
  -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \
  -d <span class="hljs-string">&#x27;{&quot;model&quot;: &quot;qwen2.5:32b&quot;, &quot;prompt&quot;: &quot;Hello!&quot;, &quot;stream&quot;: false}&#x27;</span>
</code></pre>
<p><em>(Image with the result of the curl command execution)</em></p>
<ol start="7">
<li><strong>Integration with Python:</strong></li>
</ol>
<pre><code class="language-python"><span class="hljs-keyword">import</span> requests

<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">prompt</span>):
    res = requests.post(<span class="hljs-string">&quot;http://localhost:11434/api/generate&quot;</span>, json={
        <span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;qwen2.5:32b&quot;</span>,
        <span class="hljs-string">&quot;prompt&quot;</span>: prompt,
        <span class="hljs-string">&quot;stream&quot;</span>: <span class="hljs-literal">False</span>
    })
    <span class="hljs-keyword">return</span> res.json()[<span class="hljs-string">&#x27;response&#x27;</span>]

<span class="hljs-built_in">print</span>(query(<span class="hljs-string">&quot;Explain quantum entanglement&quot;</span>))
</code></pre>
<p>Now you have a full-fledged local LLM, ready to work with MCP and LangGraph.</p>
<p><strong>What's next?</strong></p>
<p>We have a choice between cloud and local models, and we have learned how to connect both. The most interesting part is ahead - <strong>creating AI agents on LangGraph</strong>, which use the selected model, memory, tools, and their own logic.</p>
<p><strong>Let's move on to the most delicious part - code and practice!</strong></p>
<hr>
<p>Before moving on to practice, it is important to prepare the working environment. I assume that you are already familiar with the basics of Python, know what libraries and dependencies are, and understand why to use a virtual environment.</p>
<p>If all this is new to you - I recommend first taking a short course or guide on Python basics, and then returning to the article.</p>
<h4 id="step-1-creating-a-virtual-environment">Step 1: Creating a virtual environment</h4>
<p>Create a new virtual environment in the project folder:</p>
<pre><code class="language-bash">python -m venv venv
<span class="hljs-built_in">source</span> venv/bin/activate  <span class="hljs-comment"># for Linux/macOS</span>
venv\Scripts\activate   <span class="hljs-comment"># for Windows</span>
</code></pre>
<h4 id="step-2-installing-dependencies">Step 2: Installing dependencies</h4>
<p>Create a <code>requirements.txt</code> file and add the following lines to it:</p>
<pre><code>langchain==0.3.26
langchain-core==0.3.69
langchain-deepseek==0.1.3
langchain-mcp-adapters==0.1.9
langchain-ollama==0.3.5
langchain-openai==0.3.28
langgraph==0.5.3
langgraph-checkpoint==2.1.1
langgraph-prebuilt==0.5.2
langgraph-sdk==0.1.73
langsmith==0.4.8
mcp==1.12.0
ollama==0.5.1
openai==1.97.0
</code></pre>
<blockquote>
<p>‚ö†Ô∏è <strong>Current versions are indicated as of July 21, 2025.</strong> They may have changed since publication - <strong>check for relevance before installation.</strong></p>
</blockquote>
<p>Then install the dependencies:</p>
<pre><code class="language-bash">pip install -r requirements.txt```

<span class="hljs-comment">#### Step 3: Configuring environment variables</span>

Create a `.<span class="hljs-built_in">env</span>` file <span class="hljs-keyword">in</span> the project root and add the necessary API keys to it:
</code></pre>
<p>OPENAI_API_KEY=sk-proj-1234
DEEPSEEK_API_KEY=sk-123
OPENROUTER_API_KEY=sk-or-v1-123
BRAVE_API_KEY=BSAj123K1bvBGpH1344tLwc</p>
<pre><code>
**Purpose of variables:**

*   **OPENAI_API_KEY** - key for accessing GPT models from OpenAI;
*   **DEEPSEEK_API_KEY** - key for using Deepseek models;
*   **OPENROUTER_API_KEY** - single key for accessing many models via OpenRouter

---
Some MCP tools (e.g., `brave-web-search`) require a key to work. Without it, they simply won't activate.

**What if you don't have API keys?**

No problem. You can start development with a local model (e.g., via Ollama), without connecting any external services. In this case, you don't need to create the `.env` file at all.

Done! Now we have everything we need to get started - an isolated environment, dependencies, and, if necessary, access to cloud neural networks and MCP integrations.

Next - we will launch our LLM agent in different ways.

### Simple launch of LLM agents via LangGraph: basic integration

Let's start with the simplest: how to &quot;connect the brain&quot; to a future agent. We will analyze the basic ways to launch language models (LLM) using LangChain, so that in the next step we can move on to integration with LangGraph and building a full-fledged AI agent.

#### Imports
```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_deepseek import ChatDeepSeek
</code></pre>
<ul>
<li><code>os</code> and <code>load_dotenv()</code> - for loading variables from the <code>.env</code> file.</li>
<li><code>ChatOpenAI</code>, <code>ChatOllama</code>, <code>ChatDeepSeek</code> - wrappers for connecting language models via LangChain.</li>
</ul>
<blockquote>
<p>üí° If you use alternative approaches to working with configurations (e.g., Pydantic Settings), you can replace <code>load_dotenv()</code> with your usual method.</p>
</blockquote>
<h4 id="loading-environment-variables">Loading environment variables</h4>
<pre><code class="language-python">load_dotenv()
</code></pre>
<p>This will load all variables from <code>.env</code>, including keys for accessing OpenAI, DeepSeek, OpenRouter, and other APIs.</p>
<h4 id="simple-functions-for-getting-llm">Simple functions for getting LLM</h4>
<p><strong>OpenAI</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_openai_llm</span>():
    <span class="hljs-keyword">return</span> ChatOpenAI(model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span>, api_key=os.getenv(<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>))
</code></pre>
<p>If the <code>OPENAI_API_KEY</code> variable is correctly set, LangChain will substitute it automatically - explicit <code>api_key=...</code> here is optional.</p>
<p><strong>DeepSeek</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_deepseek_llm</span>():
    <span class="hljs-comment"># ...</span>
</code></pre>
<p>Similarly, but using the <code>ChatDeepSeek</code> wrapper.</p>
<p><strong>OpenRouter (and other compatible APIs)</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_openrouter_llm</span>(<span class="hljs-params">model=<span class="hljs-string">&quot;moonshotai/kimi-k2:free&quot;</span></span>):
    <span class="hljs-keyword">return</span> ChatOpenAI(
        model=model,
        api_key=os.getenv(<span class="hljs-string">&quot;OPENROUTER_API_KEY&quot;</span>),
        base_url=<span class="hljs-string">&quot;https://openrouter.ai/api/v1&quot;</span>,
        temperature=<span class="hljs-number">0</span>
    )
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li><code>ChatOpenAI</code> is used, despite the model not being from OpenAI - because OpenRouter uses the same protocol.</li>
<li><code>base_url</code> is mandatory: it points to the OpenRouter API.</li>
<li>The <code>moonshotai/kimi-k2:free</code> model was chosen as one of the most balanced options in terms of quality and speed at the time of writing.</li>
<li>The <code>OpenRouter</code> API key must be passed explicitly - automatic substitution does not work here.</li>
</ul>
<h4 id="mini-test-checking-model-operation">Mini-test: checking model operation</h4>
<pre><code class="language-python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:
    llm = get_openrouter_llm(model=<span class="hljs-string">&quot;moonshotai/kimi-k2:free&quot;</span>)
    response = llm.invoke(<span class="hljs-string">&quot;Who are you?&quot;</span>)
    <span class="hljs-built_in">print</span>(response.content)
</code></pre>
<p><em>(Image with the execution result: <code>I am an AI assistant created by Moonshot AI...</code>)</em></p>
<p>If everything is configured correctly, you will receive a meaningful response from the model. Congratulations - the first step is done!</p>
<h3 id="but-this-is-not-an-agent-yet">But this is not an agent yet</h3>
<p>At the current stage, we have connected LLM and made a simple call. This is more like a console chatbot than an AI agent.</p>
<p><strong>Why?</strong></p>
<ul>
<li>We write <strong>synchronous, linear code</strong> without state logic or goals.</li>
<li>The agent does not make decisions, does not remember context, and does not use tools.</li>
<li>MCP and LangGraph are not yet involved.</li>
</ul>
<p><strong>What's next?</strong></p>
<p>Next, we will implement a <strong>full-fledged AI agent</strong> using <strong>LangGraph</strong> - first without MCP, to focus on the agent's architecture, states, and logic.</p>
<p>Let's dive into real agent mechanics. Let's go!</p>
<h3 id="job-classification-agent-from-theory-to-practice">Job classification agent: from theory to practice</h3>
<p>...concepts of LangGraph in practice and create a useful tool for HR platforms and freelance exchanges.</p>
<h4 id="agent-task">Agent task</h4>
<p>Our agent takes a text description of a vacancy or service as input and performs a three-level classification:</p>
<ol>
<li><strong>Job type</strong>: project work or permanent vacancy</li>
<li><strong>Profession category</strong>: from 45+ predefined specialties</li>
<li><strong>Search type</strong>: whether a person is looking for a job or looking for a performer</li>
</ol>
<p>The result is returned in a structured JSON format with a confidence score for each classification.</p>
<h4 id="-agent-architecture-on-langgraph">üìà Agent architecture on LangGraph</h4>
<p>Following the principles of LangGraph, we create a <strong>state graph</strong> of four nodes:</p>
<ul>
<li>Input description</li>
<li>‚Üì</li>
<li>Job type classification node</li>
<li>‚Üì</li>
<li>Category classification node</li>
<li>‚Üì</li>
<li>Search type determination node</li>
<li>‚Üì</li>
<li>Confidence calculation node</li>
<li>‚Üì</li>
<li>JSON result</li>
</ul>
<p>Each node is a <strong>specialized function</strong> that:</p>
<ul>
<li>Receives the current agent state</li>
<li>Performs its part of the analysis</li>
<li>Updates the state and passes it on</li>
</ul>
<h4 id="state-management">State management</h4>
<p>We define the <strong>agent's memory structure</strong> via <code>TypedDict</code>:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> TypedDict, <span class="hljs-type">Dict</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">State</span>(<span class="hljs-title class_ inherited__">TypedDict</span>):
    <span class="hljs-string">&quot;&quot;&quot;Agent state for storing information about the classification process&quot;&quot;&quot;</span>
    description: <span class="hljs-built_in">str</span>
    job_type: <span class="hljs-built_in">str</span>
    category: <span class="hljs-built_in">str</span>
    search_type: <span class="hljs-built_in">str</span>
    confidence_scores: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">float</span>]
    processed: <span class="hljs-built_in">bool</span>
</code></pre>
<p>This is the <strong>agent's working memory</strong> - everything it remembers and accumulates during the analysis. Similar to how a human expert keeps the task context in mind when analyzing a document.</p>
<p>Let's look at the full code, and then focus on the main points.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> asyncio
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> enum <span class="hljs-keyword">import</span> Enum
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> TypedDict, <span class="hljs-type">Dict</span>, <span class="hljs-type">Any</span>, <span class="hljs-type">List</span>

<span class="hljs-keyword">from</span> langgraph.graph <span class="hljs-keyword">import</span> StateGraph, END
<span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate
<span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI
<span class="hljs-keyword">from</span> langchain.schema <span class="hljs-keyword">import</span> HumanMessage

<span class="hljs-comment"># Categories of professions</span>
CATEGORIES = [
    <span class="hljs-string">&quot;2D animator&quot;</span>, <span class="hljs-string">&quot;3D animator&quot;</span>, <span class="hljs-string">&quot;3D modeler&quot;</span>,
    <span class="hljs-string">&quot;Business analyst&quot;</span>, <span class="hljs-string">&quot;Blockchain developer&quot;</span>, ...
]

<span class="hljs-keyword">class</span> <span class="hljs-title class_">JobType</span>(<span class="hljs-title class_ inherited__">Enum</span>):
    PROJECT = <span class="hljs-string">&quot;project work&quot;</span>
    PERMANENT = <span class="hljs-string">&quot;permanent work&quot;</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">SearchType</span>(<span class="hljs-title class_ inherited__">Enum</span>):
    LOOKING_FOR_WORK = <span class="hljs-string">&quot;looking for work&quot;</span>
    LOOKING_FOR_PERFORMER = <span class="hljs-string">&quot;looking for performer&quot;</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">State</span>(<span class="hljs-title class_ inherited__">TypedDict</span>):
    <span class="hljs-string">&quot;&quot;&quot;Agent state for storing information about the classification process&quot;&quot;&quot;</span>
    description: <span class="hljs-built_in">str</span>
    job_type: <span class="hljs-built_in">str</span>
    category: <span class="hljs-built_in">str</span>
    search_type: <span class="hljs-built_in">str</span>
    confidence_scores: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">float</span>]
    processed: <span class="hljs-built_in">bool</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">VacancyClassificationAgent</span>:
    <span class="hljs-string">&quot;&quot;&quot;Asynchronous agent for classifying vacancies and services&quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;gpt-4o-mini&quot;</span>, temperature: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.1</span></span>):
        <span class="hljs-string">&quot;&quot;&quot;Agent initialization&quot;&quot;&quot;</span>
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.workflow = self._create_workflow()

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_create_workflow</span>(<span class="hljs-params">self</span>) -&gt; StateGraph:
        <span class="hljs-string">&quot;&quot;&quot;Creates the agent&#x27;s workflow based on LangGraph&quot;&quot;&quot;</span>
        workflow = StateGraph(State)
        
        <span class="hljs-comment"># Add nodes to the graph</span>
        workflow.add_node(<span class="hljs-string">&quot;job_type_classification&quot;</span>, self._classify_job_type)
        workflow.add_node(<span class="hljs-string">&quot;category_classification&quot;</span>, self._classify_category)
        workflow.add_node(<span class="hljs-string">&quot;search_type_classification&quot;</span>, self._classify_search_type)
        workflow.add_node(<span class="hljs-string">&quot;confidence_calculation&quot;</span>, self._calculate_confidence)
        
        <span class="hljs-comment"># Define the sequence of node execution</span>
        workflow.set_entry_point(<span class="hljs-string">&quot;job_type_classification&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;job_type_classification&quot;</span>, <span class="hljs-string">&quot;category_classification&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;category_classification&quot;</span>, <span class="hljs-string">&quot;search_type_classification&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;search_type_classification&quot;</span>, <span class="hljs-string">&quot;confidence_calculation&quot;</span>)
        workflow.add_edge(<span class="hljs-string">&quot;confidence_calculation&quot;</span>, END)
        
        <span class="hljs-keyword">return</span> workflow.<span class="hljs-built_in">compile</span>()

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_job_type</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Node for determining job type: project or permanent&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (implementation follows)</span>
        
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_category</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Node for determining profession category&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (implementation follows)</span>
        
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_classify_search_type</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Node for determining search type&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (implementation follows)</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_calculate_confidence</span>(<span class="hljs-params">self, state: State</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Node for calculating confidence level in classification&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (implementation follows)</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_find_closest_category</span>(<span class="hljs-params">self, predicted_category: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:
        <span class="hljs-string">&quot;&quot;&quot;Finds the closest category from the list of available ones&quot;&quot;&quot;</span>
        <span class="hljs-comment"># ... (implementation follows)</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">classify</span>(<span class="hljs-params">self, description: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
        <span class="hljs-string">&quot;&quot;&quot;Main method for classifying vacancy/service&quot;&quot;&quot;</span>
        initial_state = {
            <span class="hljs-string">&quot;description&quot;</span>: description,
            <span class="hljs-string">&quot;job_type&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,
            <span class="hljs-string">&quot;category&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,
            <span class="hljs-string">&quot;search_type&quot;</span>: <span class="hljs-string">&quot;&quot;</span>,
            <span class="hljs-string">&quot;confidence_scores&quot;</span>: {},
            <span class="hljs-string">&quot;processed&quot;</span>: <span class="hljs-literal">False</span>
        }
        
        <span class="hljs-comment"># Run the workflow</span>
        result = <span class="hljs-keyword">await</span> self.workflow.ainvoke(initial_state)
        
        <span class="hljs-comment"># Form the final JSON response</span>
        classification_result = {
            <span class="hljs-string">&quot;job_type&quot;</span>: result[<span class="hljs-string">&quot;job_type&quot;</span>],
            <span class="hljs-string">&quot;category&quot;</span>: result[<span class="hljs-string">&quot;category&quot;</span>],
            <span class="hljs-string">&quot;search_type&quot;</span>: result[<span class="hljs-string">&quot;search_type&quot;</span>],
            <span class="hljs-string">&quot;confidence_scores&quot;</span>: result[<span class="hljs-string">&quot;confidence_scores&quot;</span>],
            <span class="hljs-string">&quot;success&quot;</span>: result[<span class="hljs-string">&quot;processed&quot;</span>]
        }
        <span class="hljs-keyword">return</span> classification_result

<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    <span class="hljs-string">&quot;&quot;&quot;Demonstration of agent operation&quot;&quot;&quot;</span>
    agent = VacancyClassificationAgent()
    
    test_cases = [
        <span class="hljs-string">&quot;Requires Python developer to create a web application in Django. Permanent job.&quot;</span>,
        <span class="hljs-string">&quot;Looking for orders to create logos and corporate identity. I work in Adobe Illustrator.&quot;</span>,
        <span class="hljs-string">&quot;Need a 3D animator for a short-term project to create a commercial.&quot;</span>,
        <span class="hljs-string">&quot;Resume: experienced marketer, looking for remote work in digital marketing&quot;</span>,
        <span class="hljs-string">&quot;Looking for a React frontend developer for our team on a permanent basis&quot;</span>
    ]
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ü§ñ Demonstration of vacancy classification agent operation\n&quot;</span>)
    <span class="hljs-keyword">for</span> i, description <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(test_cases, <span class="hljs-number">1</span>):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;--- Test <span class="hljs-subst">{i}</span>: ---&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Description: <span class="hljs-subst">{description}</span>&quot;</span>)
        <span class="hljs-keyword">try</span>:
            result = <span class="hljs-keyword">await</span> agent.classify(description)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Classification result:&quot;</span>)
            <span class="hljs-built_in">print</span>(json.dumps(result, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">2</span>))
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;‚ùå Error: <span class="hljs-subst">{e}</span>&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:
    asyncio.run(main())

</code></pre>
<p><em>(...the rest of the code with method implementations was presented in the article...)</em></p>
<h3 id="key-advantages-of-the-architecture">Key advantages of the architecture</h3>
<ol>
<li><strong>Modularity</strong> - each node solves one problem, easy to test and improve separately</li>
<li><strong>Extensibility</strong> - new features are added declaratively</li>
<li><strong>Transparency</strong> - the entire decision-making process is documented and traceable</li>
<li><strong>Performance</strong> - asynchronous processing of multiple requests</li>
<li><strong>Reliability</strong> - built-in error handling and recovery</li>
</ol>
<h3 id="real-benefits">Real benefits</h3>
<p>Such an agent can be used in:</p>
<ul>
<li><strong>HR platforms</strong> for automatic categorization of resumes and vacancies</li>
<li><strong>Freelance exchanges</strong> for improving search and recommendations</li>
<li><strong>Internal systems</strong> of companies for processing applications and projects</li>
<li><strong>Analytical solutions</strong> for labor market research</li>
</ul>
<h3 id="mcp-in-action-creating-an-agent-with-a-file-system-and-web-search">MCP in action: creating an agent with a file system and web search</h3>
<p>After we have dealt with the basic principles of LangGraph and created a simple classifier agent, let's expand its capabilities by connecting it to the outside world via MCP.</p>
<p>Now we will create a full-fledged AI assistant that can:</p>
<ul>
<li>Work with the file system (read, create, modify files)</li>
<li>Search for relevant information on the Internet</li>
<li>Remember the dialogue context</li>
<li>Handle errors and recover from failures</li>
</ul>
<h4 id="from-theory-to-real-tools">From theory to real tools</h4>
<p>Remember how at the beginning of the article we talked about <strong>MCP being a bridge between a neural network and its environment</strong>? Now you will see this in practice. Our agent will gain access to <strong>real tools</strong>:</p>
<pre><code># File system tools
- read_file - reading files
- write_file - writing and creating files
- list_directory - viewing folder contents
- create_directory - creating folders

# Web search tools
- brave_web_search - searching the Internet
- get_web_content - getting page content
</code></pre>
<p>This is no longer a &quot;toy&quot; agent - it is a <strong>working tool</strong> that can solve real problems.</p>
<h4 id="-architecture-from-simple-to-complex">üìà Architecture: from simple to complex</h4>
<p><strong>1. Configuration as the basis of stability</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass

<span class="hljs-meta">@dataclass</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">AgentConfig</span>:
    <span class="hljs-string">&quot;&quot;&quot;Simplified AI agent configuration&quot;&quot;&quot;</span>
    filesystem_path: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;/path/to/work/directory&quot;</span>
    model_provider: ModelProvider = ModelProvider.OLLAMA
    use_memory: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>
    enable_web_search: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">validate</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-string">&quot;&quot;&quot;Configuration validation&quot;&quot;&quot;</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(self.filesystem_path):
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Path does not exist: <span class="hljs-subst">{self.filesystem_path}</span>&quot;</span>)
</code></pre>
<p><strong>Why is this important?</strong> Unlike the classification example, here the agent interacts with external systems. One error in the file path or a missing API key - and the entire agent stops working. <strong>Validation at startup</strong> saves hours of debugging.</p>
<p><strong>2. Model factory: flexibility of choice</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_model</span>(<span class="hljs-params">config: AgentConfig</span>):
    <span class="hljs-string">&quot;&quot;&quot;Creates a model according to the configuration&quot;&quot;&quot;</span>
    provider = config.model_provider.value
    <span class="hljs-keyword">if</span> provider == <span class="hljs-string">&quot;ollama&quot;</span>:
        <span class="hljs-keyword">return</span> ChatOllama(model=<span class="hljs-string">&quot;qwen2.5:32b&quot;</span>, base_url=<span class="hljs-string">&quot;http://localhost:11434&quot;</span>)
    <span class="hljs-keyword">elif</span> provider == <span class="hljs-string">&quot;openai&quot;</span>:
        <span class="hljs-keyword">return</span> ChatOpenAI(model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span>, api_key=os.getenv(<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>))
    <span class="hljs-comment"># ... other providers</span>
</code></pre>
<p>One code - many models. Want a free local model? Use Ollama. Need maximum accuracy? Switch to GPT-4. Is speed important? Try DeepSeek. The code remains the same.</p>
<p><strong>3. MCP integration: connecting to the real world</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_mcp_client</span>(<span class="hljs-params">self</span>):
    <span class="hljs-string">&quot;&quot;&quot;MCP client initialization&quot;&quot;&quot;</span>
    mcp_config = {
        <span class="hljs-string">&quot;filesystem&quot;</span>: {
            <span class="hljs-string">&quot;command&quot;</span>: <span class="hljs-string">&quot;npx&quot;</span>,
            <span class="hljs-string">&quot;args&quot;</span>: [<span class="hljs-string">&quot;-y&quot;</span>, <span class="hljs-string">&quot;@modelcontextprotocol/server-filesystem&quot;</span>, self.filesystem_path],
            <span class="hljs-string">&quot;transport&quot;</span>: <span class="hljs-string">&quot;stdio&quot;</span>
        },
        <span class="hljs-string">&quot;brave-search&quot;</span>: {
            <span class="hljs-string">&quot;command&quot;</span>: <span class="hljs-string">&quot;npx&quot;</span>,
            <span class="hljs-string">&quot;args&quot;</span>: [<span class="hljs-string">&quot;-y&quot;</span>, <span class="hljs-string">&quot;@modelcontextprotocol/server-brave-search@latest&quot;</span>],
            <span class="hljs-string">&quot;transport&quot;</span>: <span class="hljs-string">&quot;stdio&quot;</span>,
            <span class="hljs-string">&quot;env&quot;</span>: {<span class="hljs-string">&quot;BRAVE_API_KEY&quot;</span>: os.getenv(<span class="hljs-string">&quot;BRAVE_API_KEY&quot;</span>)}
        }
    }
    self.mcp_client = MultiServerMCPClient(mcp_config)
    self.tools = <span class="hljs-keyword">await</span> self.mcp_client.get_tools()
</code></pre>
<p>Here the key work of MCP takes place: we connect external MCP servers to the agent, which provide a set of tools and functions. The agent, in turn, receives not just individual functions, but a full contextual understanding of how to work with the file system and the Internet.</p>
<h4 id="error-resilience">Error resilience</h4>
<p>In the real world, everything breaks: the network is unavailable, files are locked, API keys are expired. Our agent is ready for this:</p>
<pre><code class="language-python"><span class="hljs-meta">@retry_on_failure(<span class="hljs-params">max_retries=<span class="hljs-number">2</span>, delay=<span class="hljs-number">1.0</span></span>)</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_message</span>(<span class="hljs-params">self, user_input: <span class="hljs-built_in">str</span>, thread_id: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;default&quot;</span></span>) -&gt; <span class="hljs-built_in">str</span>:
    <span class="hljs-comment"># ...</span>
</code></pre>
<p>The <code>@retry_on_failure</code> decorator automatically retries operations on temporary failures. The user won't even notice that something went wrong.</p>
<h3 id="summary-from-theory-to-practice-of-ai-agents">Summary: from theory to practice of AI agents</h3>
<p>Today we have come a long way from basic concepts to creating working AI agents. Let's summarize what we have learned and achieved.</p>
<p><strong>What we have mastered</strong></p>
<p><strong>1. Fundamental concepts</strong></p>
<ul>
<li>Understood the difference between chatbots and real AI agents</li>
<li>Understood the role of <strong>MCP (Model Context Protocol)</strong> as a bridge between the model and the outside world</li>
<li>Studied the architecture of <strong>LangGraph</strong> for building complex agent logic</li>
</ul>
<p><strong>2. Practical skills</strong></p>
<ul>
<li>Configured a working environment with support for cloud and local models</li>
<li>Created a <strong>classifier agent</strong> with an asynchronous architecture and state management</li>
<li>Built an <strong>MCP agent</strong> with access to the file system and web search</li>
</ul>
<p><strong>3. Architectural patterns</strong></p>
<ul>
<li>Mastered modular configuration and model factories</li>
<li>Implemented error handling and <strong>retry mechanisms</strong> for production-ready solutions</li>
</ul>
<h3 id="key-advantages-of-the-approach">Key advantages of the approach</h3>
<p><strong>LangGraph + MCP</strong> give us:</p>
<ul>
<li><strong>Transparency</strong> - every step of the agent is documented and traceable</li>
<li><strong>Extensibility</strong> - new features are added declaratively</li>
<li><strong>Reliability</strong> - built-in error handling and recovery</li>
<li><strong>Flexibility</strong> - support for multiple models and providers out of the box</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>AI agents are not futuristic fiction, but <strong>real technology of today</strong>. With LangGraph and MCP, we can create systems that solve specific business problems, automate routines, and open up new possibilities.</p>
<p><strong>The main thing is to start.</strong> Take the code from the examples, adapt it to your tasks, experiment. Each project is a new experience and a step towards mastery in the field of AI development.</p>
<p>Good luck with your projects!</p>
<hr>
<p><em>Tags: python, ai, mcp, langchain, ai-assistant, ollama, ai-agents, local llm, langgraph, mcp-server</em>
<em>Hubs: Amvera company blog, Natural Language Processing, Artificial Intelligence, Python, Programming</em></p>

            
            
        </body>
        </html>